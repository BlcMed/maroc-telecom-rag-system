{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies and env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# to load open ai key\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.readers.file.base:> [SimpleDirectoryReader] Total files added: 3\n",
      "> [SimpleDirectoryReader] Total files added: 3\n",
      "DEBUG:fsspec.local:open file: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf\n",
      "open file: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf\n",
      "DEBUG:fsspec.local:open file: /home/cuphead/Projects/llama-index/data/maroctelecom.txt\n",
      "open file: /home/cuphead/Projects/llama-index/data/maroctelecom.txt\n",
      "DEBUG:fsspec.local:open file: /home/cuphead/Projects/llama-index/data/story.txt\n",
      "open file: /home/cuphead/Projects/llama-index/data/story.txt\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: The Eight Steps of Data Analysis: A Graphical F...\n",
      "> Adding chunk: The Eight Steps of Data Analysis: A Graphical F...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 2\n",
      "Psychologica...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 2\n",
      "Psychologica...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 3\n",
      "to arrive at...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 3\n",
      "to arrive at...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 4\n",
      "•p-valuespro...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 4\n",
      "•p-valuespro...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 5\n",
      "The previous...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 5\n",
      "The previous...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 6\n",
      "signiﬁcance ...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 6\n",
      "signiﬁcance ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 7\n",
      "non-linear p...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 7\n",
      "non-linear p...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 8\n",
      "246\n",
      "Neutral ...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 8\n",
      "246\n",
      "Neutral ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 9\n",
      "they seek to...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 9\n",
      "they seek to...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 10\n",
      "Table 1\n",
      "A S...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 10\n",
      "Table 1\n",
      "A S...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 11\n",
      "decision cr...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 11\n",
      "decision cr...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 12\n",
      "that occurs...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 12\n",
      "that occurs...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 13\n",
      "that allow ...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 13\n",
      "that allow ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 14\n",
      "categorical...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 14\n",
      "categorical...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 15\n",
      "The advanta...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 15\n",
      "The advanta...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 16\n",
      "to compute ...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 16\n",
      "to compute ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 17\n",
      "statistical...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 17\n",
      "statistical...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 18\n",
      "7. Make a D...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 18\n",
      "7. Make a D...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 19\n",
      "4.A supplem...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 19\n",
      "4.A supplem...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 20\n",
      "Self-report...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 20\n",
      "Self-report...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 21\n",
      "020406080\n",
      "0...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 21\n",
      "020406080\n",
      "0...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 22\n",
      "01020\n",
      "poorf...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 22\n",
      "01020\n",
      "poorf...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 23\n",
      "took me an ...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 23\n",
      "took me an ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 24\n",
      "fair good v...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 24\n",
      "fair good v...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 25\n",
      "0510152025\n",
      "...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 25\n",
      "0510152025\n",
      "...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 26\n",
      "one’s healt...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 26\n",
      "one’s healt...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 27\n",
      "fair good v...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 27\n",
      "fair good v...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 28\n",
      "http://www....\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 28\n",
      "http://www....\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 29\n",
      "non-linear ...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 29\n",
      "non-linear ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 30\n",
      "•Reports th...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 30\n",
      "•Reports th...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 31\n",
      "optimal way...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 31\n",
      "optimal way...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 32\n",
      "References\n",
      "...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 32\n",
      "References\n",
      "...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 33\n",
      "Cumming, G....\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 33\n",
      "Cumming, G....\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 34\n",
      "Kutner, M. ...\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 34\n",
      "Kutner, M. ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 35\n",
      "Shadish, W....\n",
      "> Adding chunk: THE EIGHT STEPS OF DATA ANALYSIS 35\n",
      "Shadish, W....\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Maroc Telecom\n",
      "Company type\tPublic\n",
      "Traded as\n",
      "\tEu...\n",
      "> Adding chunk: Maroc Telecom\n",
      "Company type\tPublic\n",
      "Traded as\n",
      "\tEu...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Activities\n",
      "Land lines\n",
      "\n",
      "It consists of the provi...\n",
      "> Adding chunk: Activities\n",
      "Land lines\n",
      "\n",
      "It consists of the provi...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Once upon a time, in a town nestled between two...\n",
      "> Adding chunk: Once upon a time, in a town nestled between two...\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/home/cuphead/Projects/llama-index/venv/lib/python3.10/site-packages/certifi/cacert.pem'\n",
      "load_verify_locations cafile='/home/cuphead/Projects/llama-index/venv/lib/python3.10/site-packages/certifi/cacert.pem'\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f20fbc1cb80>, 'json_data': {'input': ['page_label: 1 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  The Eight Steps of Data Analysis: A Graphical Framework to Promote Sound Statistical Analysis Dustin Fife1 1Rowan University Abstract Data analysis is a risky endeavor, particularly among those unaware of its dangers. In the words of Cook and Campbell (1976; see also Shadish, Cook, & Campbell, 2002), “Statistical Conclusions Validity” threatens all research subjected to the dark arts of statistical magic. Although traditional statistics classes may advise against certain practices (e.g., multiple comparisons, small sample sizes, violating normality), they may fail to cover others (e.g., outlier detection and violating linearity). More common, perhaps, is that researchers may fail to remember them. In this paper, rather than rehashing old warnings and diatribes against this practice or that, I instead advocate a general statistical analysis strategy. This graphically-based eight step strategy promises to resolve the majority of statistical traps researchers may fall in without having to remember large lists of problematic statistical practices. These steps will assist in preventing both false positives and negatives and yield critical insights about the data that would have otherwise been missed. I conclude with an applied example that shows how the eight steps reveal interesting insights that would not be detected with standard statistical practices. Keywords: statistical assumptions, NHST, conﬁrmatory data analysis, graph- ical data analysis, ﬁshing, p-hacking The ﬁeld of psychology has been forced to participate in methodological introspection, of sorts. This introspection began late in the 20th century as methodologists vehemently protested the knee-jerk focus on p-values Null Hypothesis Signiﬁcance Testing (NHST) encourages (Cohen, 1994; Harlow, Mulaik, & Steiger, 2016; Jones, 1952). The American I wish to thank those who assisted in reviewing this manuscript, including Tom Dinzeo, Polly Tremoulet, Jeﬀrey Greeson, Yoav Zeevi, Christine Simmons, and Joseph Rodgers, as well as the anonymous reviewers and editor. Correspondence concerning this article should be addressed to Dustin Fife, 201 Mullica Hill Road Glassboro, NJ 08028. E-mail: ﬁfe.dustin@gmail.com', 'page_label: 2 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 2 Psychological Association (APA) suggested several alternatives, including a stronger focus on estimation (i.e., identifying the strength of the eﬀect, as well as the size/direction of the parameters of interest; Wilkinson & Task Force on Statistical Inference, 1999), rather than the probability of the data in the face of no eﬀect (a strange hypothesis indeed!). This forced introspection not only continues today, but the replication crisis has heightened its necessity. A recent attempt to replicate 100 diﬀerent studies (from three top journals in psychology) resulted in poor replication metrics (Open Science Collaboration, 2015); estimates of eﬀect sizes were half as strong in the replications than in the original studies and 61% of the attempted replications failed to produce statistical signiﬁcance. (For an alternative perspective on the replication crisis, see Shrout & Rodgers, 2018 and Maxwell, Lau, & Howard, 2015). Some have suggested this replication crisis was caused (at least partially) by researchers’ over-reliance on NHST (and other statistical practices; Cumming, 2014; Pashler & Wagenmakers, 2012). In this paper, however, I will not rant and rave about NHST; Others have already highlighted its problems (Cohen, 1994; Cumming, Fidler, & Thomason, 2001; Schmidt, 1996; Traﬁmow, 2017). Rather, my purpose is primarily to bridge the gap between known best practices in data analysis and actual statistical application. In so doing, I hope to provide a step-by-step strategy targeted at applied researchers for developing a deeper understanding of one’s data. A Potential Misunderstanding Arguably, what spawned the ﬁrst methodological “crisis” in the 1990s was Jacob Cohen’s thorough and acerbic critique of signiﬁcance testing (Cohen, 1994). Near the conclusion of his article, he said, “don’t look for a magic alternative to NHST, some other objective mechanical ritual to replace it. It doesn’t exist” (p. 1001). I wonder if this statement is too easy to misunderstand. I believe what Cohen was trying to suggest was there is no onealternative to NHST; rather statistical analysis requires a rather large toolbox, where each tool is adapted to the circumstances under which it is most appropriate. The tool might be, for example, Bayes factor, conﬁdence intervals, eﬀect sizes, single-subjects designs, preregistration, and/or graphical data analysis. I fear that when some read Cohen’s words, they interpret it to mean “NHST is wrong and there is nothing else researchers can use.” This is a rather discouraging interpretation, yet this interpretation is easy to make. In the past, methodologists have been quick to critique, but much slower to oﬀer alternative recommendations. What exacerbates the problem is the fact that few agree about what ought to replace NHST (e.g., Cumming, 2014; Kruschke & Liddell, 2018). Again, the reason clear alternatives have failed to emerge is likely because the alternative is, more than likely, a toolbox rather than a speciﬁc tool. I am inclined to think few methodologists believe anyonealternative is ideally suited to all circumstances, and, as such, have shied away from oﬀering step-by-step instructions for best-practices because it feels too mechanical. On the other hand, applied researchers generally need some sort of structure. What I will introduce provides the structure without the mechanics. My framework is designed to train a researcher to look for cues in data that the toolbox needs to be adjusted. While NHST seems to allow one to follow a sequence of steps and arrive at an unambiguous conclusion, my approach allows one to follow a sequence of steps designed to gather evidence in order', 'page_label: 3 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 3 to arrive at a conclusion that may be ambiguous. NHST permits one to “turn oﬀ” their brain. My approach re-activates it. Although the approach and many of the graphical tools I present are new, most of the suggestions are not; for decades statisticians have advocated for non-mechanistic judgments, assessing model assumptions, graphical data analysis, and sensitivity analyses.1Despite this fact, practices have changed little over the years. Researchers rarely evaluate statistical assumptions (Hoekstra, Kiers, & Johnson, 2012) or plot their data (Healy & Moody, 2014; Levine, 2018), and researchers still rely heavily on signiﬁcance testing (Counsell & Harlow, 2017; Coyne, 2016). Perhaps part of the problem is that these suggestions are relatively de-centralized. In this paper, I oﬀer a centralized set of guidelines that have been cobbled from various sources that synthesize decades of suggestions into one uniﬁed step-by-step framework. This framework will not only protect against false conclusions, but will also free researcher’s minds from rigid NHST thinking that is endemic in psychology. In the following sections, I ﬁrst review several reasons NHST practices are pervasive in psychology, then discuss potential causes for the replication crisis. Next, I review how the eight steps of data analysis encourages a greater focus on estimation and “listening” to one’s data. Finally, I conclude with an example where I show how the eight steps deepened understanding of data. Should Psychology Abandon p-values? For decades, some methodologists have suggested signiﬁcance tests ought to have no place in psychological journals (Cohen, 1994; Harlow et al., 2016; Schmidt, 1996; Valentine, Aloe, & Lau, 2015). Yet there seems to be little evidence researchers aren’t using p-values to make decisions, nor does there seem to be much of a visible shift in statistical practices (Counsell & Harlow, 2017; Coyne, 2016; Cumming et al., 2007). Despite passionate and cogent arguments against NHST, several obstacles remain and willremain, no matter how red-faced methodologists get. These include: •Social pressures . The entire ﬁeld of psychology understands p-value speak and a researcher may decide not to venture outside NHST practices for fear of having an otherwise publishable paper relegated to the ﬁle drawer. •Habit. For many researchers, they have been doing NHST for decades. For these people, shifting away from such rote practices is counter-intuitive (and diﬃcult). •Learning . Abandoning p-values in favor of some other statistical practice may require considerable time and eﬀort that most researchers do not have. •p-values reduce ambiguity . Withoutp-values, it would open the ﬁeld to disagree- ment about what constitutes a scientiﬁcally signiﬁcant ﬁnding. A rigid cutoﬀ of 0.05 acts as an operational deﬁnition for a relationship that ought to be noticed (and published). 1Anecdotally, I’ve shared this manuscript with both statisticians and applied researchers. The statisticians tend to say, “Of course that’s how data analysis ought to be done. We’ve been advocating for that for years!” On the other hand, applied researchers say, “I never thought to do data analysis this way.”', 'page_label: 4 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 4 •p-valuesprovidea“translationalmechanism”fromtheorytodata . Asargued by Cortina and Landis (2011), NHST bridges theoretical language into data analysis, and back again into theoretical language. This translation, they argue, is ill-deﬁned, at best, and non-existent, at worst, in other statistical frameworks. The guidelines I introduce help side-step (and sometimes address head-on) all of these concerns. No researcher needs to learn additional statistical techniques (except when the eight steps reveal that traditional methods fail to model the data), or remove p-values from their method section . Rather, I advocate a simple shift in focus that will add richness to one’s statistical practices. Doing so will, over time, shift the culture away from p-values and toward a greater focus on estimation and attending to messages our data have long been trying to tell us. Potential Causes of Replication Crisis To diagnose the cause of the replication crisis, it is advantageous to think of the current predicament as a systematic, discipline-wide habit of committing Type I errors. Some authors (e.g.; Rothman, 2010) have suggested abandoning statistical NHST will ﬁx all Type I (and Type II) errors. This may be technically correct, but eliminating NHST will not necessarily alter the number of false positives/negatives when researchers utilize other metrics for decision-making. Regardless of whether p-values or some other metric are used, the sources of false positives (and false negatives) are the same. Shadish, Cook, and Campbell (2002), noted two characteristics of data in particular that may inﬂate Type I error rates (and/or false positives): (1) Violated assumptions of statistical tests (e.g., normality, homogeneity, linearity, independence), and (2) Fishing/multiple testing. Various authors have commented on both of these issues and I will brieﬂy review each in turn. Violated Assumptions of Statistical Tests Linear models (e.g., regression, ANOVA, t-tests, structural equation models) are the most common models in psychology. For these models to behave appropriately (i.e., for p-values to actually reﬂect the probability of a Type I error under the null), the data must meet four key assumptions: independence2, normality, homoskedasticity, and linearity.3 When decisions of signiﬁcance are determined based on p-values, some of these assumptions are more critical than others (e.g., if normality is violated, the probability of committing a Type I error under a statistical signiﬁcance decision criteria, given the null, tend to stay close to 0.05; while violations of independence will lead to signiﬁcant departures from 0.05). When violated, Type I error rates may remain fairly close 0.05, or may deviate substantially in either direction. Likewise, when violated, Type II errors may also be inﬂated. 2Independence is a serious assumption that, when violated, will result in substantial bias in estimat- ing standard errors. However, independence is more of a design issue than a characteristic of the data. Consequently, I will not address how to evaluate independence. 3Linearity is actually not an assumption of ANOVAs/ t-tests. Or, rather, linearity is an assumption, though it is guaranteed to be met with categorical predictors.', 'page_label: 5 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 5 The previous paragraph is how most textbooks write of statistical assumptions. I tend to think of them diﬀerently, particularly since I rarely use p-values for decision-making. If these assumptions are violated, it means the researcher has simply chosen the wrong statistical model, a condition easily ﬁxed by choosing another. Yes, p-values will not remain at 0.05 if assumptions are violated, but it also means the researcher has simply chosen a model that is not appropriate. It would be like choosing to compute the mean on highly skewed data; one can do it but the information gleaned may be misleading. If the wrong model is chosen, one might have a false positive or negative. The sensitivity of linear models to these assumptions have been well documented (e.g., Maxwell & Delaney, 2004; Osborne, 2013) as well as the consequences of violating these assumptions (Micceri, 1989). Yet rarely do researchers mention whether they checked for the appropriateness of linear models (Hoekstra et al., 2012). And because most researchers do not provide the datasets used for analysis4, it is unknown the degree to which psychological research has been corrupted by violated assumptions. Thesolutiontotheproblem, asImentionindetaillater, isagreaterfocusonestimation and graphical data analysis. Graphics allow the researcher to determine at a glance whether assumptions have been violated. Although violated assumptions may inﬂate false positives, I suspect a far more common cause is multiple testing. I will address this issue in the following section. Fishing/Multiple Testing/ p-hacking Most researchers are likely familiar with the problem of multiple testing: when there are four groups, for example, it would be unwise to perform a t-test comparing each and every group (group 1 vs. group 2, group 2 vs group 3, etc.). Likewise, it would be unwise to compute dozens of Cohen’s dvalues and only interpret those that are larger than 0.5. Though the probability of one test being signiﬁcant under the null is 0.05 (or the probability of one large dis small), the probability of rejecting one among several is much higher (much like the probability of rolling at least one six over the course of 10 rolls is much higher than the probability of getting it on the ﬁrst roll). Indeed, this problem is suﬃciently well-known that if any researcher were to submit a paper and report they performed 107 t-tests, the paper would likely be rejected. Yet multiple testing likely happens all the time in psychology, but in more nuanced ways (Simmons, Nelson, & Simonsohn, 2011). Suppose, for example, a researcher collected 10 covariates that could potentially muck up the relationship between the IV and the DV. Said researcher might include a covariate, then run the analysis. If the treatment eﬀect is non-signiﬁcant, the researcher may decide another covariate is more appropriate to use as a control. This practice may continue until one of the covariates yields statistical signiﬁcance on the treatment eﬀect. This is another form of multiple testing. Likewise, if the researcher measured several dependent variables, then performed statistical analysis on each DV until 4This statement is based on extensive (and frustrating) personal experience.', 'page_label: 6 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 6 signiﬁcance was achieved, this too constitutes multiple testing, yet it is not so explicitly and universally condemned as running multiple t-tests. The term that encapsulates the new ways in which some researchers participate in multiple testing is called “ p-hacking,” and p-hacking includes not only the practices I have mentioned (measuring multiple DVs and covariates and running several models until signiﬁcance is obtained), but also several others, including adding more observations until signiﬁcance is reached, and dropping an experimental condition. With a discipline so focused onp-values, it is simple to see why so many researchers exercise “researcher degrees of freedom” (knowingly or unknowingly) to achieve the “gold standard” of p<0.05. Various authors (e.g., Nelson, Simmons, & Simonsohn, 2018; Wagenmakers, Wetzels, Borsboom, Maas, & Kievit, 2012) suggest researchers voluntarily report their planned statistical analysis a priori (e.g., through the “as predicted” platform: https://aspredicted. org/), then be explicit in the paper about any modiﬁcations to the original plan. This is a great idea, when possible. However, it is extremely rare that researchers are prepared to make such detailed plans in advance; Also preregistration won’t address the statistical issues mentioned previously5(i.e, violating of statistical assumptions) or invite researchers to consider the uncertainty associated with statistical analysis. Few researchers are ready to propose such risky hypotheses; few analyses go exactly as planned and preregistration best works when modiﬁcations to the original plan are unlikely. In these situations, preregistration is well-equipped to prevent p-hacking. Indeed, that is exactly what it was designed to do. It was not designed, however, to deepen one’s understanding of data. One could, presumably, preregister a hypothesis, test the hypothesis, and replicate it, and still be misled if that person is not attending to the messages the data are trying to voice. The eight steps of data analysis I propose, on the other hand, were designed to give voice to data. These steps were not designed to prevent p-hacking. In other words, neither the eight steps nor preregistration alone will ﬁx the replication crisis. Together, however, they will shift the focus away from simply publishing a (potentially spurious) ﬁnding toward building sound scientiﬁc and mathematical models of reality. Type II Errors and the File-Drawer Problem The replication crisis highlights the fact that psychology may be inundated with false positives. Unfortunately, it is more diﬃcult to estimate the prevalence of false negatives. Researchers may spend months collecting data only to have a p-value not reach statistical signiﬁcance. Some may abandon the project, while others might participate in p-hacking until signiﬁcance is achieved. The framework I propose will alleviate the problem of both false positives and negatives. For example, a pattern may not reach statistical signiﬁcance for several reasons that would be detected under this framework, including outliers that pull means to be more similar, strong 5Preregistration will certainly invite authors to consider how they will handle violations of assumptions (Wicherts et al., 2016), but preregistration alone will not tell researchers how to detect and handle such violations. The eight steps will guide researchers making these decisions. As such, the two (preregistration and the eight steps) work hand in hand.', 'page_label: 7 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 7 non-linear patterns that are poorly represented by a straight line, and violated statistical assumptions that render traditional tests overly conservative. In either case, whether we publish spurious ﬁndings or abandon non-signiﬁcant results, spending more time with our data via the eight steps of data analysis will aid in shifting our focus to what the data are actually trying to tell us. Guiding Principles of Data Analysis Before I explain the eight steps, let me ﬁrst introduce three guiding principles of data analysis: 1.Plot raw data whenever possible . One can easily be deceived when a graphic displays only summaries of the data (e.g., means as dots and standard errors as lines). For example, suppose Figure 1 came from an experiment where subjects were randomly assigned to watch a neutral video or a violent video. Further, suppose the subjects were subsequently measured on aggression. If one were to simply interpret the left plot, they might believe the type of video had a large eﬀect on aggression. Yet when we overlay the “jittered”6raw datapoints (right panel), we see that the aggression scores are bimodal in the violent group. Perhaps that bimodality is caused by gender (e.g., males report higher aggression after watching the video, while females do not). This would be an important discovery that would be masked if one simply graphed the summaries rather than the raw data. Furthermore, the right panel shows the mean for the violent video group is quite misleading; the mean falls at a place where data are quite sparse. 2.Utilize sensitivity analyses whenever needed. Often times our data throw us curve balls that require making an ad hoc decision. For example, our data may require a transformation to render the residuals more normal, an outlier may require deleting, or a missing value may require imputing. One would hope the conclusions gleaned from data remain largely unaﬀected by whatever decision we make. The only way to determine whether our results are sensitive to our decisions is to run the analysis both ways. Others have called this a “multiverse” analysis (Steegen, Tuerlinckx, Gelman, & Vanpaemel, 2016). For example, suppose a researcher decides an outlier ought to be deleted. The researcher should then run the analysis both with and without the outlier deleted and determine the degree to which the results change. The researcher might also investigate other strategies for dealing with the outlier (e.g., treating it as missing then imputing that value). Under this situation, I strongly recommend the researcher report the results of all three analyses (at least in a supplemental section) and comment on whether the results are sensitive to the decision made. Note that this sensitivity analysis is comparing two models that test the samehypoth- esis, rather than two models that test diﬀerent hypotheses (i.e., a model comparison; 6Jittering means to add random noise to a categorical variable (in this case, Neutral and Violent, which may be coded as 1 and 2). Jittering categorical values prevents overlap of datapoints and makes it easier to see the distribution of the datapoints. See Fife (2019a) for examples of how to produce jittered plots in the point-and-click software called Jamovi.', 'page_label: 8 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 8 246 Neutral ViolentAggression 0246 Neutral ViolentAggression Figure 1. Two plots of the same data. In the left panel, only means and standard deviations are reported. In the right panel, the raw data points overlay the standard deviation bars. When possible, graphical displays of raw data are always preferred over graphical summaries. Rodgers, 2010). Model comparisons are excellent tools for teasing out competing explanations of the data, but these sorts of model comparisons are outside the scope of this paper. 3.Explicitly state where analyses fall on the exploratory/conﬁrmatory continuum. Ex- ploratory data analysis (EDA) is the process whereby researchers analyze data without preconceived notions of what patterns they might ﬁnd. While conﬁrmatory data analysis (CDA) has been likened to placing a hypothesis on trial, EDA is like detective- work; data are searched for interesting patterns that might be worth pursuing for future investigation (including preregistered replications). Researchers might engage in either activity, or anything in between, such as “rough” CDA (Fife & Rodgers, 2019). Unfortunately, researchers have blamed EDA for the replication crisis . This is unfortunate and comes from misunderstanding the role, purpose, and tools associated with EDA. Prior to the replication crisis, researchers were notdoing EDA. Rather, they had exploratory intentions that utilized conﬁrmatory tools. Additionally, results discovered through exploration were presented as if they were conﬁrmatory, which is a critical violation of one of EDA’s most important rules: users of EDA must be explicit about which results were obtained through exploration versus conﬁrmation. For a more thorough and complete treatment of EDA, CDA, and everything in between, see Fife and Rodgers (2019). These eight steps are designed to assist researchers participating in conﬁrmatory or “rough” conﬁrmatory research (Fife & Rodgers, 2019), or even exploratory data analysis. In conﬁrmatory studies, researchers have a preconceived hypothesis for which', 'page_label: 9 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 9 they seek to evaluate the evidence. However, along the way of evaluating the evidence, data analysis plans often do not pan out as intended; interesting patterns present themselves which a researcher may want to investigate, unexpected complications arise that necessitate modifying the data analysis plan. In alignment with the rules and ethics of EDA, it is necessary that researchers be explicit about which analyses were uncovered through exploratory analysis, and which were obtained through conﬁrmatory analysis. One might choose to blend the two into one paper, provided the researcher is again explicit about which results were conﬁrmatory and which were exploratory. Returning to Figure 1, for example, the researcher may have decided before collecting data that the video would make participants more aggressive, yet did not anticipate the bimodality of scores in the treatment group. In the report, the researcher might say, “Upon graphing the results, a bimodal distribution was discovered among the participants in the treatment group. These results were unanticipated a priori so we decided to explore this relationship further...” As mentioned previously, the eight steps can be used to guide conﬁrmatory, rough conﬁrmatory, or even exploratory data analysis. In the following section I will outline the eight steps and hope to illustrate how they might be used to direct the analyst’s focus in such a way that improves understanding of data. The Eight Steps to Data Analysis In this section I will describe each of the eight steps. For simplicity, I have included a table (Table 1) that shows the eight steps and the function each step serves. For each of the steps that follow, I will address why I recommend performing said operation and what weaknesses it aims to overcome. When evaluating the recommendations in Table 1, the reader might be inclined to say, “Why thesesteps instead of others?” For example, a Bayesian might insist that Bayes factors should accompany all analyses. Another might complain there’s no mention of missing data or correcting for unreliability. Still others might say these steps fail to include preregistration. To this question, I would oﬀer a few reasons why I am advocating for these steps rather than others. First, these eights steps are not about tools/techniques, but about the approach one takes toward data analysis. This approach is designed to direct the analyst’s attention toward the evidence in favor (or against) the chosen hypothesis. Additionally, these steps are fairly universal, regardless of what type of data one chooses to analyze. Techniques, on the other hand, are situation-speciﬁc. Bayes factors are excellent for deciding between two hypotheses, but not so excellent when one is concerned about estimation. Increasing reliability is necessary when one has unreliable measures, but unnecessary when one has no measurement error. Preregistration is critical for strong conﬁrmatory studies, but unnecessary when one is doing exploratory research. The eight steps, on the other hand, are always appropriate and are critical in helping the analyst decide which tools are appropriate for a given situation. For example, Bayes factors could be used for making a', 'page_label: 10 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 10 Table 1 A Summary of the eight Steps of Data Analysis Step Purpose 1. State the theoretical hypothesis Helps to minimize “ﬁshing” for statistical signiﬁ- cance Provides a translational map from theory to data Allows users to specify their own decision criteria Invites researchers to consider preregistering hy- potheses 2. Assess psychometric properties of variablesInvites researchers to think about the impact of measurement 3. Plot univariate distributions Helps identify outliers Helps identify issues with non-normality Assists in identifying coding errors 4. Plot a graphic to match the Directs focus toward the size of eﬀects theoretical hypothesis Helps identify potential problems with non- linearity/heteroskedasticity Improves cognitive encoding of results Highlights uncertainty 5. Study residuals Helps identify problems with normality (e.g., through histogram of residuals) Helps identify problems with non- linearity/homoskedasticity (e.g., through a residual dependence or SL plot) 6. Interpret parameter esti- mates/eﬀect sizesEncourages the researcher to focus on estimation before signiﬁcance Put graphical information into concrete numbers 7. Set a decision criteria (if appro- priate)Assists in making a decision about signiﬁcance 8. Replicate on a new dataset Encourages cumulative and reproducible science', 'page_label: 11 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 11 decision criteria that is also pre-registered (Step 1). Likewise, unreliability and missing data will reveal themselves in Steps 2/3. A second reason I advocate for these steps is because they are relatively uncontroversial. There’s a great deal of disagreement about the merits of Bayesian estimation (Simonsohn, 2014), conﬁdence intervals (Morey, Hoekstra, Rouder, Lee, & Wagenmakers, 2016), preregis- tration (Szollosi et al., 2019), etc. Too often, I think methodologists are quick to argue over diﬀerences between approaches and give less air time to similarities. No methodologists that I know of would argue that graphics are bad, evaluating model assumptions is problematic, or interpreting parameter estimates is dangerous. Rather, they argue this is obvious, yet people are rarely doing these things; to me, that seems to indicate that maybe we ought to spend less time arguing and more time going back to the basics. Third, I believe these steps are beneﬁcial because they emphasize graphics. The visual processing system has enormous bandwidth and is able to encode large amounts of information with little eﬀort (Bonneau et al., 2014). Graphics have a way of simultaneously highlighting uncertainty while also revealing problems with statistical models. Additionally, humans are really good at detecting visual patterns. If it is something we are good at, should we not be leveraging that when doing scientiﬁc research? Finally, I advocate for these eight steps because I ﬁnd them useful. I am not so dogmatic as to think these eight steps are theeight steps of data analysis. One might conjure a diﬀerent set of eight, or ten, or twenty. Each step seems to provide a diﬀerent “view” of the research question, each contributing a unique piece of evidence for the ﬁnal evaluation. When I use them, and when I teach my students to use them, we gain a lot of information about the data. Additionally, when I have, through laziness, bypassed a step, I have been guilty of making short-sighted conclusions. 1. State the Theoretical Hypothesis and (Optionally) State a Decision Criteria The ﬁrst step to data analysis ought to be to state the theoretical hypothesis. Ideally, this would take place long before the researcher actually collects data (i.e., the hypothesis is preregistered). Doing so in advance will assist in preventing researchers from “bending” their original hypothesis to ﬁt the actual analysis performed (e.g., “Oh yeah. I, uh, meant to include that as a covariate originally. I just forgot”). This step is entirely voluntary and preregistration strengthens the weight of the evidence in favor of the hypothesis. (It is, after all, much less impressive to “predict” an eﬀect you already discovered than to predict an eﬀect before you actually collect data). If the analyst does decide a decision is needed, I recommend three strategies that will increase the utility of their decision criteria: (1) mapping hypotheses to speciﬁc statistical parameters, (2) stating strong hypotheses, and (3) developing decision criteria for clinical “signiﬁcance.” Mapping hypotheses to parameters. In an ideal world, one’s statistical hypothe- sis is closely tied to the theoretical hypothesis, such that support for the statistical hypothesis provides support for the theoretical hypothesis. However, there is always some simpliﬁcation', 'page_label: 12 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 12 that occurs when one translates from theory to statistical inference. This is inevitable, yet it is critical to ensure that the two (theory and model) are as closely aligned as possible. To do so, the theoretically-derived hypotheses ought to be tied to speciﬁc parameters in a model. Too often researchers write well-crafted introduction sections, providing strong theoretical rationale for their chosen verbal hypothesis, yet, there is a disconnect between the well-crafted introduction and the results section; the hypothesis points to a particular parameter (e.g., the interaction term in a model or the main eﬀect of a predictor after controlling for another), yet the results report gobs of results and corresponding tests of signiﬁcance. On other occasions, the statistical hypothesis is very weakly tied to the theoret- ical hypothesis. Not only does this dilute the message (because the parameter of interest is buried between other tests), but this constitutes ﬁshing. Each reported p-value is the result of a tested hypothesis and, as such, each reported p-value ought to be clearly supported by strong theoretical rationale. If the introduction section only develops arguments for testing one parameter, then only one parameter ought to be interpreted (though all parameters ought to be reported). Granted, some analyses require entering other parameters in the model. For example, if the researcher’s hypothesis concerns an interaction eﬀect between two independent variables, the main eﬀects must be included in the model as well. However, these main eﬀects need not be tested (or rather, interpreted since software packages tend to report signiﬁcance for all parameters) for signiﬁcance because, again, the researcher’s hypothesis is not concerned with these parameters. Stating Strong Hypotheses. Years ago, Meehl (1967) criticized the use of zero as a tested hypothesis. That’s a rather low bar to pass. Instead, he advocated for “strong” hypotheses, where researchers specify numeric values for the hypothesized parameter. For example, rather than testing whether a correlation is diﬀerent from zero, a researcher can test whether the correlation is diﬀerent from +0.4. This amounts to reversing the role of the null and the alternative and can lead to some logistic problems (e.g., researchers might be inclined to collect small samples so they don’t have power to reject their cherished hypotheses). With some modiﬁcation, we might instead hypothesize the parameter of interest falls within a particular range (e.g., from r= 0.2 to 0.4), or at least that its direction is positive (or negative). Better yet, researchers might use the values of the parameters themselves to set their own decision criteria, which I will discuss next. For a more detailed treatment about various approaches for developing precise hypotheses see Edwards and Berry (2010). Developing decision criteria. As mentioned previously, one of the purported advantages of NHST is that it provides a bridge from theory to conclusion via a p-value (Cortina & Landis, 2011). However, not all decisions require one to make a decision. Clearly, certain situations call for such judgments (e.g., Does this ﬁnding have scientiﬁc merit? Should this treatment be used? Are side-eﬀects of medication small enough to merit implementation?) and alternative frameworks have no clear route from theory to judgment. Unfortunately, using a universal criteria ( p< 0.05) has, in a way, “hijacked” decision making from the scientiﬁc community. A p-value is a function of both the sample size and the eﬀect size. In certain domains (e.g., neuroscience) a large Nis simply not feasible, and yet the culture of NHST does not permit ﬂexibility in considering other decision criteria', 'page_label: 13 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 13 that allow for more lenient p-values. On the other hand, alternatives to NHST (e.g., eﬀect sizes) have been criticized because they do not provide simple rules for deciding whether a ﬁnding has (or has not) scientiﬁc merit (Cortina & Landis, 2011). I advocate, instead, the decision criteria be left in the hands of the researcher. Re- searchers may choose, if they wish, that a signiﬁcant ﬁnding is one that reaches p< 0.05. Other researchers, on the other hand, may decide a clinically signiﬁcant ﬁnding is one where d> 0.83, or a mean diﬀerence between treatment and control group is greater than 10 points, or expenditures are reduced by $10,000. In short, any metric may serve as the basis for making decisions; one is not limited to p-values. One might criticize this approach by asking what is to stop researchers from setting more lenient criteria than p< 0.05. Researchers have a vested interest in a paper reaching “signiﬁcance” (however it is deﬁned) and if they can lower the threshold for reaching signiﬁcance to even less than what is currently acceptable, they will abuse that. They might, for example, state in advance that any correlation greater than 0.0000001 is clinically signiﬁcant. Fortunately, researchers eventually have to defend their decision criteria when submit- ting their paper for review. If they set a low bar for their decision criteria at preregistration, they will have to answer to a skeptical community of reviewers at a later date. I suspect this knowledge will severely limit the degree to which researchers seek to abuse the practice of setting their own decision criteria. Rather, I suspect researchers will actually impose more stringent criteria on their hypotheses. In addition, by placing these sorts of decisions back in the hands of the researcher and the scientiﬁc community, the signiﬁcance of results will not have to be qualiﬁed as “statistically but not clinically signiﬁcant.” 2. Assess psychometric properties. Manyhavesuggestedthe“replicationcrisis”isaresultofattemptingtomakeconclusive answers on noisy data (Loken & Gelman, 2017). The obvious solution to the noise is simply to increase the sample size. In some situations, however, this is not feasible, nor is it always the most practical approach. Gelman (2018) noted that doubling the reliability of a test will yield equivalent gains in precision as quadrupling one’s sample size. In other words, we might get more “bang for our buck” by spending a bit more time with measurement. By assessing the psychometric properties of our measures, it will invite deeper thinking on measurement issues and how they might aﬀect data analysis. If our measures fail psychometrically, no amount of sophisticated modeling will yield any insights that have scientiﬁc merit. For more details about assessing psychometrics, see Furr (2014). 3. Plot the Univariate Distributions The third step of data analysis is to plot the univariate distributions of the variables of interest. For quantitative variables, histograms are good candidates. (Quantile-quantile plots may also be beneﬁcial, but they are less common and thus less interpretable). For', 'page_label: 14 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 14 categorical variables, bar charts are appropriate. A visual of the distribution will inform the researcher of several potential pitfalls in the coming analysis, such as: •Incorrectly coded values (e.g., with a second wave of data collection, the researcher accidentally changes the treatment group designation from “Treatment” to “TRT,” then later aggregates the two data waves and accidentally treats those labeled as “Treatment” and “TRT” as separate groups) •Improperly coded missing values (e.g., a -999 is treated as a value, rather than a missing variable) •Non-normality (e.g., if there are excessive zeroes in the sample) •Outliers •Role reversals (e.g., if a bar chart shows many more women than men ﬁreﬁghters, the labels were likely ﬂipped) Beginning with such plots may prevent embarrassing retractions later. For example, Hofmann, Fang, and Brager (2015) wrote an article that suggested Oxytocin reduced psychiatric symptoms, but later had to retract the article. When entering the eﬀect sizes for a meta analysis program, they assumed all eﬀect sizes were positive. Because the program they used required specifying which eﬀects were negative (and because they improperly assumed they were all positive), the aggregated eﬀect size was inﬂated. This could be avoided by simply plotting the univariate distribution of eﬀect sizes. (Read more at Chawla, 2016). At this point it may not be necessary to address the outliers and/or non-normality of the data. Remember the assumption of linear models (e.g., regression, ANOVA, t-tests) are that the residuals of the model are normally distributed. The outcome variable itself need not be normal (though it usually helps). Often including one’s predictors in a model will render the residuals normal, even if the variable itself was not normal. Likewise, an outlier in univariate space may not be an outlier in multivariate space. However, plotting the univariate distributions in advance will inform the researcher of potential problems that may occur later in the analysis. For a video playlist on evaluating univariate distributions, see https://yt.vu/p/PL8F480DgtpW-T_ySqIurOMIaChNlOr3Ka. 4. Plot a Graphic to Match the Theoretical Hypothesis Once the univariate distributions are plotted (and any coding errors handled), the next step is to plot a graphic to match the theoretical hypothesis. If one were to perform simple linear regression, for example, a scatterplot would help the reader visualize the results. For ANOVAs/ t-test, violin plots or bee swarm plots would be appropriate. Although some of the strategies used (and types of plots) may be new, all of them are easy to produce in either R or even point and click software, such as Jamovi (project, 2019) or JASP (JASP Team, 2019).7For a tutorial on determining which graphic is most appropriate and for instructions on creating each of these types of plots, see Fife (2019a), as well as a video playlist at https://yt.vu/p/PL8F480DgtpW8WFhHFRzos7iUK2r-MhmKw. 7SPSS can perform most of these plots, but unfortunately (as far as I know) does not allow the user to overlay the raw datapoints over boxplots, mean plots, multi-way dot plots, etc.', 'page_label: 15 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 15 The advantage of utilizing goodgraphics is that they make it nearly impossible to deceive one’s readers (or one’s self, for that matter, especially if raw data are displayed). Sound graphics are essential for identifying two problems in particular: (1) outliers, and (2) non-linearity. If one or more outliers are present, it is possible they are driving statistical signiﬁcance. If so, this will be easy to determine from a graphic. Likewise, if the researcher attempts to ﬁt a straight line to data that are clearly non-linear, usually a graphic will show the error of one’s ways. If this is the case, one need not hang their head in defeat. It simply means the researcher has chosen the wrong model and must select another (such as polynomial regression, a transformed dependent variable, non-parametric procedures, or generalized linear models). To better detect departures from linearity, I recommend adding loess lines to a graphic; the package Flexplot (Fife, 2019b), which is available in R, the point-and-click platform Jamovi, and will shortly be released in JASP, defaults to show loess lines in scatterplots. Loess lines are non-parametric curves that are allowed to “bend” with the data. They can assist in detecting non-linearities that a standard model (which forces a straight line) would not detect. Recall our original goal: we are trying not to make a false positive/negative. At this point, it is impossible to make a false conclusion because we haven’t made a conclusion at all. We have not yet even computed an estimate, let alone made a decision about statistical signiﬁcance. In addition, the researcher may decide that computing the p-value on a dataset is not necessary. If the visual analysis shows a whopping eﬀect, who really cares about whether it is statistically signiﬁcant? In the words of Joseph Berkson, these sorts of images pass the “intraocular trauma test” (Berkson, 1942). (Though the converse is also true. I once had a graduate student produce scatterplots and ﬁnd the predicted relationship was non-existent. She immediately quit the analysis and concluded, “If there is an eﬀect, it’s so small I don’t even care about it.” I promptly gave her an A on her assignment). In short, plotting the data before computing the analysis will prevent researchers from deceiving themselves, render tests of statistical signiﬁcance less necessary, and force the reader to think in terms of the size of the eﬀect, rather than its existence. Unfortunately, not all statistical analyses lend themselves nicely to graphical display. Structural equation models, factor analysis models, and hierarchical linear models, for example, are more diﬃcult to map onto a single plot. These may require multiple plots and, unfortunately, the fragmented nature of these graphics may detach the visuals slightly from the actual analysis. Future research ought to attempt to bridge that gap and ﬁnd intuitive, graphical representations of these more complex models. 5. Study the Residuals After plotting the data, the researcher may not know if the chosen analysis (e.g., linear regression) is appropriate. The data may violate the assumption of linearity, normality of residuals, or homoskedasticity. Yet in order to properly diagnose the problem, we ought', 'page_label: 16 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 16 to compute the residuals (and thus have to actually perform the analysis). Unfortunately, before extracting the residuals, the model has to actually be ﬁt to the data. I would advise the reader to close one’s eyes (metaphorically or otherwise) until after the residuals have been studied before studying the results of a statistic. With residuals in hand, the researcher is now ready to properly assess the assumptions of the model. It is common (at least in the bio-medical literature) to perform statistical tests of normality or homoskedasticity, or independence. I would advise against it. Like other tests of signiﬁcance, these tests of assumptions are sensitive to sample size. With a small enough N, even large departures from statistical assumptions are not detected, while with largeN, even trivial diﬀerences are ﬂagged. These statistical tests tell us whether our distributions depart from what is expected. They do not tell us whether they are diﬀerent enough to muck up our analysis. The latter is better done through visual interpretation of results (as well as through a sensitivity analysis). Histograms will inform the researcher whether the normality assumption has been approximately met. Residual dependence plots assist in determining whether linear- ity and homoskedasticity have been met. Either of these plots will assist in ﬂagging outliers. For instructions in how to diagnose problems using these plots, see Kutner, Nachtsheim, Neter, and Li (2004), as well as the following YouTube playlist: https: //yt.vu/p/PL8F480DgtpW8v-h-7s9Ih826Qi7aa3rBS If the visual inspection of the residuals signals problems, one may have to iterate through steps 2-4 until the assumptions have been met, each time making a modiﬁcation to the model (such as transforming the DV, removing outliers, utilizing weighted least squares, or using generalized linear models). Furthermore, problems at this early stage demonstrate the researcher is not yet ready for conﬁrmatory data analysis. Again, there is nothing wrong with migrating to rough conﬁrmatory or exploratory data analysis and if a researcher ﬁnds the intended model is not appropriate, the researcher ought to explicitly state their analysis has turned from conﬁrmatory to something else (Fife & Rodgers, 2019). Regardless, the researcher may proceed to Step 6 once the assumptions have been met. 6. Interpret Eﬀect Sizes/Parameter Estimates At this point the researcher has far more information about the data than what is typically reported in psychological journals; the researcher knows outliers are not driving the analysis, knows the model chosen is appropriate, and has a visual that illustrates the strength of the relationship between the variables of interest. After Step 5, the researcher ought to be conﬁdent the model chosen is appropriate (i.e., the assumptions of the model have been met). Once again, I emphasize that it is impossible to commit a Type I (or Type II) error because statistical signiﬁcance has not yet been evaluated. Likewise, no conclusions have been made, so it’s impossible to make a false positive. Rather, I recommend the researcher study and interpret eﬀect sizes and parameter estimates. We all have been cautioned against making mountains out of molehills, or emphasizing statistical signiﬁcance at the expense of practical signiﬁcance. This is why the APA recommended researchers report eﬀect sizes in addition to statistical signiﬁcance. I would argue practical signiﬁcance is far more important than', 'page_label: 17 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 17 statistical signiﬁcance. Studying the eﬀect size (and parameter estimates) before statistical signiﬁcance is a conscious choice aimed at reminding the researcher of this preference for estimation rather than signiﬁcance. Most statistical packages oﬀer readily available estimates of eﬀect sizes, including f2, part and partial correlations, r2, and Cohen’s d. To determine which measure of eﬀect is appropriate, I recommend the concise and eﬀective article by Cohen (1992). For software dedicated to eﬀect size calculations and graphical data analysis, see Fife (2019b), as well as a tutorial on estimates at https://yt.vu/p/PL8F480DgtpW8jshu9vTyCf4HqSHYx05Fw. Where possible, eﬀect sizes in the original (unstandardized) metric should be inter- preted (Baguley, 2009; Bond, Wiitala, & Richard, 2003; Tukey, 1986). In a regression, the parameters of interest are the slopes (and occasionally the intercept). For ANOVAs/ t-test, the parameters of interest are the mean diﬀerences between groups. For structural equation modeling, the parameters of interest are the path coeﬃcients. For logistic regression and other generalized linear models, the researchers may have to perform mental gymnastics as they attempt to interpret things in terms of log odds (or in terms of odds ratios). Studying these parameters adds another layer of depth at which the researcher can make sense of the data. Not only will it inform the researcher about the direction of the eﬀect (e.g., males scored higher in aggression than females, anxiety is positively predictive of depression, performance is inversely related to mood), but also oﬀers a mathematical equation that maps predictors onto outcomes. For example, suppose a researcher performs a regression that assesses weight loss from experimental condition, controlling for motivation. Further suppose the regression equation is as follows: weight change = 1.2−0.8×motivation −4.5×treatment −1.2×motivation ×treatment This regression equation would be an interesting result indeed. This suggests the following: •Those in the control group who have no motivation will actually gain an average of 1.2 pounds (because control group is the reference group) •Every time an individual increases their motivation by a point, they can expect to lose 0.8 pounds •The treatment group averages 4.5 pounds more weight loss than the control group •The relationship between motivation and weight loss is stronger for the treatment group than for the control group, such that for the treatment group, for every point increase in motivation, they lose an additional 2 pounds (i.e., 1.2+0.8) Granted, much of this information could be gleaned from a graphic, but the estimates put the visual interpretation into concrete mathematical terms that are interesting in their own right. Furthermore, the eﬀect sizes and parameter estimates reduce ambiguity inherent in visual interpretation. To further reduce ambiguity (and marry the ideas of signiﬁcance testing with estimation), a researcher should pair conﬁdence (or credible) intervals with these estimates. Doing so will further reduce ambiguity, while explicitly recognizing the degree of uncertainty.', 'page_label: 18 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 18 7. Make a Decision (If Applicable) Recall we have plotted univariate distributions to ﬂag potential data recording errors, assess normality, and identify potential outliers. We have also created a visual representation of our analysis that shows both the size of the eﬀects and the direction. We have also thoroughly assessed whether our model is appropriate through residual analysis, estimated eﬀect sizes, and interpreted parameter estimates. In short, we have much more thoroughly familiarized ourselves with our own data (Tukey, 1986). If, at this stage, the reader feels it rather pointless to assess statistical signiﬁcance, I have successfully made my point. This second to last step is entirely optional and ideally makes it clear that our data have long been trying to tell us much more than we have allowed them. Simply computing statistical signiﬁcance without doing the previous steps is akin to eating a single sprinkle oﬀ a large birthday cake. With so much richness remaining, it is a shame that we limit ourselves to a single test that is largely uninformative. Earlier, I advocated that researchers instead set their own decision criteria. At this point, making a decision of signiﬁcance is easy; one has already pre-speciﬁed what is clinically signiﬁcant and now they simply compute the numbers and identify whether signiﬁcance was reached. 8. Replicate With New Data The decision made in the previous step is always provisional. Few single studies have the power (statistically or otherwise) to make conclusive statements about the truthfulness of a hypothesis. Rather, these ﬁndings are tentative and ought to invite closer scrutiny and replication. The discovery of the Higgs Boson, for example, was not considered settled until after hundreds of trillions of replications. The estimates obtained for the parameter of interest may serve as a prior in a Bayesian analysis, or could to be aggregated into the next study (and others like it) via meta-analysis. Such cumulative methods will invite a greater sense of humility about one’s own role on the scientiﬁc process and consequently invite deeper attention to development of theory. Reporting Results I recommend every researcher perform the eight steps when doing data analysis. However, it may not be necessary to report every step in a journal article. Not only would this increase the length of most articles (a trivial problem as journals become more digitized), but it may detract from the purpose of the article (to evaluate the original hypothesis). However, at minimum, I strongly recommend a researcher’s ﬁnal report contain: 1. One or more graphical depictions of the analysis of interest (Step 4). 2.A comment on how the researcher determined the appropriateness of statistical as- sumptions. 3. Parameter and eﬀect size estimates, with conﬁdence or credible intervals.', 'page_label: 19 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 19 4.A supplemental section containing all graphics and sensitivity analyses ora link to a website where these can be viewed. In other words, I am not advocating for a complete revamping and replacing of how statistics are reported in journal articles. Rather, I suggest we add these few pieces of information so the richness of our data becomes more visible. Example In the section that follows, I decided notto re-analyze existing datasets of previously published papers for two reasons. First, it can be diﬃcult to ﬁnd studies where researchers have actually uploaded their data for public scrutiny (although the Open Science Framework is making data far more accessible). Second, I would hate to pick on researchers conscientious enough to actually oﬀer their dataset by highlighting their data analysis mistakes. I want to encourage openness, and becoming a public data vigilante would be counterproductive. Consequently, I will analyze a publicly available dataset, the National Survey of Drug Use and Health (2014) and oﬀer my own hypothesis that, when analyzed using traditional NHST methods, yields misleading results. 1. State the Theoretical Hypothesis of Interest and (Optionally) Set a Decision Criteria. Suppose I am a drug counselor that has had only marginal success in assisting heroin addicts overcome their addictions; those who use heroin experience more psychological distress, which in turn motivates them to escape their distress via heroin. Now let us suppose I believe promoting healthy behaviors (e.g., exercise, nutritious eating) will reduce psychological distress and help break that negative feedback loop. Ideally, one would perform an experiment, but perhaps as a preliminary study, I decide to use an existing dataset to perform an observational analysis to assess the potential eﬃcacy of promoting healthy behaviors in a full experiment. However, I may consider controlling for mental illness. One may be experiencing psychological distress because of their mental illness, which may make them more likely to escape such distress through drug use. Stated diﬀerently: Among heroin users, those who report having more healthy behaviors will report less psychological distress, after controlling for mental illness Further suppose that this is the ﬁrst attempt the researcher has made at evaluating this hypothesis. In other words, the researcher is in “rough” conﬁrmatory mode, at best, and may even be leaning toward exploratory research. Given that, it makes little sense to set a decision criteria since any attempt to do so will be somewhat arbitrary. For illustrative purposes, I will test this hypothesis using standard NHST methodology using an ANCOVA model. Based on these data, I could conclude:', 'page_label: 20 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 20 Self-reported health rating was signiﬁcantly associated with psychological distress, after controlling for mental illness, F(4,189) = 10.84,MSE = 28.86,p<.001, ˆη2 G=.187. As I will show, the above statement is both misleading as well as incomplete. 2. Psychometrics The NSDUH utilizes the Kessler-6 distress scale to measure psychological distress (Kessler et al., 2003). This is comprised of six items that asks the degree to which participants experienced the following symptoms within the last thirty days: nervousness, hopelessness, restlessness, depression, feeling everything required eﬀort, and worthlessness. Reliability estimates suggested that within this sample, the measure was quite consistent ( α=0.94). Because of the high reliability estimates, I simply summed the scores to create my distress scale. As for the other two variables, mental illness was derived using a logistic regression model that utilized various indicators of mental health (e.g., suicidal thoughts, suicidal attempts, major depression, psychological impairment). Health is a one-item, self-reported question that asks them to rate their overall health. Unfortunately, I cannot assess the reliability of either health or mental illness, since both were assessed using only one item. 3. Plot Univariate Distributions I plotted the univariate distributions of the three variables of interest: probability of mental illness, health rating, and psychological distress. These distributions are shown in Figure 2. The plots reveal potential issues with the data. The probability of mental illness is far from normally distributed. The mode of the distribution is near zero (i.e., the data are zero-inﬂated). Note that linear models make no assumptions of normality for the independent variables (or the dependent variables for that matter, rather the assumption is about the residuals of the dependent variable). However, in my experience, if both the IV and the DV are skewed, the assumption of linearity will almost certainly be violated. Given that the distress variable is also skewed, this could certainly be problematic for linear models (such as an ANCOVA). At this point I am primed to look for serious issues with normality, linearity, and likely heteroskedasticity. As I mentioned earlier, if these assumptions are violated, it simply means we have chosen the wrong model to ﬁt the data. 4. Plot a Graphic to Match the Analysis of Interest Recall that the ﬁctitious researcher has decided to perform an ANCOVA. An AN- COVA essentially performs a standard linear regression between the covariate and the DV, extracts the residuals, then performs an ANOVA on the residuals (though this is all done', 'page_label: 21 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 21 020406080 0.00 0.25 0.50 0.75 Probability of Mental Illnesscount poorfairgoodvery goodexcellent 0204060 countHealth Rating 0102030 0 10 20 Psychological Distresscount Figure 2 . Univariate distributions of psychological distress, health rating, and probability of mental illness. simultaneously with an ANCOVA). An “added variable plot” would be an appropriate graphic to match the ANCOVA, where the grouping variable is plotted against the residuals of the model where the covariate is removed. This image is shown in the left image of Figure 3. One shortcoming of this sort of plot is that it masks any violations of the assumption of homogeneity of regression. (Homogeneity of regression states that the regression lines for each group are parallel). As such, I have plotted a paneled graphic in Figure 3, which shows a diﬀerent scatterplot for each level of health rating, with quadratic lines overlaying the data. I used quadratic lines to see if nonlinearity might be an issue. Once again, there are a few things worth noting: 1.These ﬁtted lines are not “parallel,” meaning that the assumption of homogeneity of regression has been violated (indicating that ANCOVA is not appropriate). This also means that the left image in Figure 3 is misleading. 2. The ﬁtted lines are not linear, indicating that linear models will not be appropriate. 3.There are very few people who report poor or excellent health, suggesting that I might combine the poor/fair groups as well as the excellent/very good. Also at this point, I shouldn’t interpret the plots shown in Figure 3; the model clearly does not ﬁt. As a result, I may have to iterate through this step (and probably Step 4) until', 'page_label: 22 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 22 01020 poorfairgood very goodexcellent Health RatingDistress | Mental Illnesspoor fair good very good excellent 0.10.50.90.10.50.90.10.50.90.10.50.90.10.50.90510152025 MIDistress Figure 3 . Visual displays of the relationship between mental illness, health, and distress. The left image is an added variable plot of the relationship between health rating and distress (controlling for mental illness). The right image shows the relationship between mental illness and distress for each level of health rating. The lines are quadratic lines mapping the relationship between mental illness and distress, conditional on self-reported health rating. I ﬁnd a model that appropriately ﬁt the data. It is also clear that any attempt at strictly conﬁrmatory analysis must take a backseat as I have some decisions to make that were not anticipated a priori. I have a few options: 1.I can transform the dependent variable and attempt to “linearize” the relationships. This generally fails when the DV is zero-inﬂated (as it is in this case). 2.I can attempt to use non-parametric procedures, such as rank transformations of the dependent variable. Unfortunately, rank transformations fail to preserve interaction eﬀects (which are clearly happening, as seen in Figure 3). 3.I can perform more “modern” robust methods (Erceg-Hurn & Mirosevich, 2008). These methods essentially replace mean-based estimates (including conditional means) with trimmed means, standard variances with winsorized variances, and standard conﬁdence intervals (CIs) with bootstrapped CIs that are computed from the trimmed/winsorized estimates. Unfortunately, these methods would not work since more than 10% (the “traditional” degree of trimming from either tail) of the tail of the distribution is contained at zero. (In general, modern robust methods do not work well for zero- inﬂated data). 4.I can attempt to ﬁt a nonlinear model. Unfortunately, these sorts of models can be more diﬃcult to interpret (because coeﬃcients may not have intuitive interpretations). Unfortunately, allof these choices are complicated, and require some sophisticated modeling. I did not choose a sophisticated model to show oﬀ my statistics skills. (In fact, it', 'page_label: 23 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 23 took me an embarrassing amount of time to ﬁnd something that would actually model the data well)8. Rather, I chose a particularly illustrative example that, unfortunately, required a relatively complex model. (In full disclosure, I intentionally “ﬁshed” for an example that would be particularly illustrative). Of the strategies listed above, I favor the fourth strategy. Once again, I prefer a visual approach to modeling these data, so I will overlay the ﬁt of the model atop the raw datapoints. Before I do so, however, I will aggregate the poor and fair health categories, as well as the very good and excellent categories, otherwise, the poor and excellent categories may be unduly inﬂuenced by outlying datapoints.9 I attempted to use multiple nonlinear models, including a gamma generalized linear model (GLM), a gamma GLM with a polynomial term, a random forest model (Breiman, 2001), and an ordered logistic regression. Most failed to model the data adequately, as the ﬁt of the model failed to pass through the more concentrated parts of the dataset. Finally, I settled on a splined model, as well as a nonlinear model that utilizes the Michaelis-Menten equation (MME). The MME was designed to model enzyme reactions and has the following form: Y=Vmax·X kM+X whereVmaxrepresent the maximum ﬁtted Y value (distress in this case) over the entire range of X (mental illness) and kMis loosely interpreted as the rate of increase in distress. (Technically, kMis the point at which Vmaxreaches its halfway point, which increases if it has a steeper slope). To allow the model to generate predictions for each health rating, I modiﬁed the model as follows: Distress =MI·(Vmax+β1Good Health +β2Fair Health ) kM+β3Good Health +β4Fair Health +MI whereβ1/β2indicate how Vmaxdiﬀers for those in good/fair health relative to the “very good” health individuals, and β3/β4indicate deviations in slopes. The model’s parameters were estimated using a Bayesian approach with diﬀuse priors. A Bayesian approach will make it more seamless when I replicate these ﬁndings in Step 8 (because the posterior estimates can serve as priors for the replication). Why did I use the Michaelis-Menten equation? Well, because it ﬁt the data, as shown in Figure 4. The top rows are for the model where I combined fair/poor and very good/excellent, and the bottom plot is for the non-combined data. Both models ﬁt fairly well, except for the spline model under the excellent condition. For that reason, I am going 8To see a YouTube video explaining my thought process during my statistical modeling, visit https: //youtu.be/5BpmktmvgIA. 9I did perform a sensitivity analysis on this decision as well (i.e., performing the analysis both before and after aggregating those categories). Combining the categories makes the picture more clear. Whether that clarity is spurious, I leave it to the reader to decide. Occasionally, I will present the results from both approaches.', 'page_label: 24 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 24 fair good very good 0.1 0.5 0.90.1 0.5 0.90.1 0.5 0.90510152025 MIDistress poor fair good very good excellent 0.10.50.90.10.50.90.10.50.90.10.50.90.10.50.90510152025 MIDistress Figure 4 . Predictions for the spline (blue lines) and Michaelis-Menten equation (MME; red lines) models of the NSDUH data. The top rows combine the poor/fair and very good/excellent conditions, while the bottom plots are for the uncombined analysis. to choose the MME model for the remainder of my analyses. I am also inclined to choose the model that combines fair/poor and very good/excellent; the extreme categories have so few people I don’t trust the predictions. At this point, I am also going to temporarily refrain from interpreting the graphics until I have assessed the viability of the assumptions, which I will do in the following section. 5. Study the Residuals To ensure that the MME model adequately ﬁts the data, I generated residual plots (Figure 5), including a histogram of the residuals, as well as residual dependence and scale location (SL) plots. Again, these are the results for the combined categories (though the uncombined looked similar as well). There are potential deviations from homoskedasticity; the residuals have a slight “megaphone” shape. However, this is likely because the DV is a likert scale, which limits variability at the upper and lower ranges of distress. However, inferences tend to be fairly robust to modest deviations (Maxwell & Delaney, 2004). As such, the model appears to at least approximately meet assumptions.', 'page_label: 25 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 25 0510152025 −10010 residualscount −15−10−5051015 51015 fittedresiduals 051015 51015 fitted| residuals | Figure 5 . Residual plots of the MME analysis: a histogram of the residuals, a residual dependence plot, and an SL plot. Plotted lines are loess lines. 6. Study Parameter Estimates/Eﬀect sizes As mentioned previously, β1andβ2indicate how the maximum predicted value in the good/fair health individuals (respectively) diﬀer from the maximum of those in very good health. The diﬀerence between those in good/very good is 0.18 distress points, with a 95% credible interval ranging from -2.83 to 3.29, while the diﬀerence between those in fair versus very good health is 4.31 distress points, with a 95% credible interval of 1.37 to 7.25. 7. Determine Clinical Signiﬁcance Based on a Decision Criteria As I mentioned previously, this analysis was not a replication of a previous study. As such, there was little statistical information from which I could derive a decision criteria. However, in the next step, replication, I now have empirical information from which to generate a decision criteria. In the mean time, however, I will spend some time summarizing the insights I have gained from my ﬁrst analysis. Both mental illness and distress were skewed and the ﬁt of the model was not linear. Interestingly, as individuals in this sample increased in mental illness, there are very dramatic increases in distress, though only to a point (approximately 0.10). From that point, it did not seem to matter whether an individual had a really high probability of mental illness (e.g., 0.9) or relatively low (e.g., 0.1), their distress level is approximately the same. Additionally, those in fair/poor health do seem to experience more distress than those in very good health, or at least the estimated maximum distress ( Vmax) for the two groups are diﬀerent by about 4.31. In other words, having even moderately good health limits the maximum distress one might experience. On the other hand, increasing', 'page_label: 26 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 26 one’s health from good to very good/excellent doesn’t seem to make much of a diﬀerence. Revisiting my original hypothesis, I sought to estimate the eﬀect of health after controlling for mental illness. However, I’m not entirely sure it makes sense to control for mental illness; the relationship between mental illness and distress is both complicated and nuanced and it doesn’t really make sense to me to remove its eﬀect from my interpretation. Put diﬀerently, the eﬀect of health on distress depends highlyon one’s mental illness, and the eﬀect of health can only be interpreted in that context. 8. Repeat With a New Dataset Fortunately, the NSDUH routinely reports results of their survey of drug and alcohol use every year. As such, I decided to do a strictly conﬁrmatory test of my model on the 2018 survey. To do so, I used the posterior distribution from the previous MME model as the prior for the Bayesian model. Aside from inputting the priors, I used identical syntax to run the analysis. For my decision criteria, I chose the lower limit of my 95% credible interval for β2, 1.37, as my decision-criteria. In other words, if the diﬀerence between those who self-report as fair versus very good is more than 1.37 points, I consider that diﬀerence practically signiﬁcant. Figure 6 shows the results of the replication. As before, each health category is displayed as a separate panel and the ﬁts of the MME model are shown as red lines. The black lines are “ghost lines” (Fife, 2019b), which simply repeat the pattern from one panel (“good” in this case) across the other panels to make it easier to compare ﬁts across panels. For the top graphic, the results are remarkably similar to those in Figure 4. Also, the average diﬀerence in distress between those of fair versus very good health is 5.42, which is well above the decision criteria. Further, the 95% credible interval has narrowed from the initial study (1.37, 3.29) to the replication (4, 6.87). Fortunately, the uncombined results also suggest that improved health mitigates distress. Both the poor and fair conditions max out at much higher levels of distress than those in good/very good/excellent distress. Although the excellent condition does not follow the same pattern, likely because there is so much noise. Results Section Earlier I stated that, at a minimum, a researcher should report (1) a graphical depiction of the analysis, (2) a comment on the appropriateness of statistical assumptions, (3) parameter/eﬀect size estimates with conﬁdence intervals, and (4) a supplemental section or link where the reader can view all graphics/sensitivity analyses. For this example, the results section may read as follows: Upon visual inspection of the residuals, it was determined that linear models were not appropriate. This discovery forced a change from conﬁrmatory to exploratory analysis. Consequently, we utilized a nonlinear model, using the Michaelis-Menten equation*:', 'page_label: 27 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 27 fair good very good 0.000.250.500.75 0.000.250.500.75 0.000.250.500.750510152025 MIDistress poor fair good very good excellent 0.000.250.500.750.000.250.500.750.000.250.500.750.000.250.500.750.000.250.500.750510152025 MIDistress Figure 6 . Results of a strict conﬁrmatory replication of the MME model, which models the relationship between mental illness/health and distress. The top plots show the combined results while the bottom plots show the uncombined results. The posterior of the parameter estimates from the original study served as priors in a Bayesian analysis. This graphic shows the ﬁt. Black lines are ghost lines (Fife, 2019b), which repeat the pattern from the good health condition to the other conditions. distress =MI·(Vmax+β1Good Health +β2Fair Health ) kM+β3Good Health +β4Fair Health +MI We also aggregated the poor/fair, as well as the excellent/very good groups because data were quite sparse at the extremes. Sensitivity analyses revealed that the ﬁnal conclusions were relatively insensitive to whether the data were combined or not. Further, we utilized a Bayesian analysis with diﬀuse priors. This model was ﬁt to the 2014 NSDUH dataset, the hypotheses preregistered, then replicated on the 2018 dataset, but using the posterior of the original analysis as the priors in the replication. The results of the replication are presented in Figure 6. The predicted diﬀerence between those with fair versus very good health was 5.42, with a 95% credible interval of 4 to 6.87. More details (such as graphics of distributions and sensitivity analyses) can be viewed at', 'page_label: 28 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 28 http://www.examplesite.com/article * We also analyzed the data using a various other models, including a spline model. These other models ﬁt the data poorly. For full details and source code, see the supplemental section. Summary My ﬁrst naive NHST analysis revealed a signiﬁcant eﬀect of health on psychological distress among heroin users, with a respectable eﬀect size. However, by following the eight steps of data analysis we have learned many things we otherwise would have missed, such as: •Mental illness is highly skewed, and distress is moderately skewed •The relationship between distress and mental illness is highly curvilinear; as the probability of mental illness increases, one’s distress increases rapidly, then levels oﬀ with higher levels of mental illness. •Those with a 10% probability of mental illness have about equivalent distress as those with a probability of 90% or higher. •Regardless of one’s health, the predicted rate at which distress increases is the same, though the predicted maximum distress is diﬀerent. •Relative to those of poor/fair health, having good health (or very good) reduces the maximal distress someone might experience by approximately 5.42 points. •Improving one’s health from good to very good makes little diﬀerence in distress. It is important to note that the issues noted above led to substantially deeper insights about the data which were entirely missed by the standard approach. Discussion The recent “replication crisis” suggests there are statistical practices within the ﬁeld of psychology that inﬂate false positives and negatives. These practices include “ p-hacking,” failing to meet statistical assumptions, and a narrow focus on statistical signiﬁcance rather than interpreting what the data are actually telling us. In this paper, I have suggested a framework under which researchers might perform data analysis that easily ﬁts within current data analysis practices, while inviting a greater focus on estimation and data visualization. In addition, this framework provides step-by-step guidance for researchers that aims to empower analysts to focus on what the data are actually saying. I have also highlighted these principals and practices with the use of an actual dataset. My analysis revealed that performing the NHST ritual, even when eﬀect sizes were reported, yielded a misleading and incomplete picture. My re-investigation of the same hypothesis revealed patterns more nuanced than a single NHST p-value (or eﬀect size) captured. It is my hope the example I provided was illustrative. I suspect most researchers do not have the interest to utilize advanced nonlinear models. Allow me to oﬀer some hope. First, I intentionally chose a dataset I knew would severely violate the assumption of linearity. I would hope most researchers would not encounter such “zero-inﬂated” models where', 'page_label: 29 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 29 non-linear relationships are almost inevitable. On other hand, some constructs psychologists investigate (e.g., frequency of rape occurrences, number of times one has attempted suicide) should not be investigated with standard linear models. Those researchers who study these types of data perhaps ought to learn more sophisticated models to best interpret what their data are saying. However, it may be optimistic to think researchers will learn these complex modeling techniques themselves. In these situations, it might be best to collaborate with those who are familiar with these techniques. On the other hand, visuals are both intuitive to interpret and easy to produce. As such, these ought to alwaysbe employed. Finally, despite my best eﬀorts to emphasize this framework easily ﬁts within current statistical practices, I suspect there may be some resistance. For example, editors may lament that performing these eight steps will double the length of the average article. I agree, though this is less of a concern as journals become more digital. However, I do not think it necessary every plot and sensitivity analysis make it to the ﬁnal version of the paper. Researchers already frequently omit details about data cleanup and how models were decided. However, I do strongly suggest this information be publicly available, either through supplemental material or through the author’s website. Doing so will allow future consumers of the research to understand what decisions were made, why they were made, and how these decisions may (or may not) have aﬀected the analysis. In addition, it provides additional tools to consumers that allows them to judge the verisimilitude of the research themselves. Another obstacle to incorporating these suggestions may be a lack of training. Many researchers may not know how to graph loess lines, jitter categorical variables, or create paneled plots. Because of this, I have created a step-by-step tutorial that shows researchers how to visually represent the most common analyses, including regression, multiple regression, factorial ANOVAs, and t-tests. This tutorial demonstrates how to perform these in both the point-and-click software Jamovi. This tutorial can be found at http://rpubs.com/ dustinﬁfe/528244. Additional resources can be found at https://yt.vu/p/PL8F480DgtpW_ v1fmBauNMPF9Gqdoaa8zJ. I also invite ﬂexibility among reviewers and editors. I recognize the approach I introduce is diﬀerent than what is traditionally taught in textbooks and what is traditionally performed in applied settings. While traditional statistics is taught as a mechanical sequence that yields unambiguous answers to research questions, my approach encourages ﬂexibility in reporting results. The natural reaction among editors and reviewers might be to reject anything unfamiliar. However, as illustrated in the example, this would be unwise. This approach to data analysis invites ambiguity and forces analysts, reviewers, and editors to confront the uncertainty inherent in data analysis. This is a very good thing. Under such ﬂexibility, I hope editors and reviewers might be open to very diﬀerent approaches to analyzing data and interpreting results. These might include: •Analysis sections that do not report p-values.Analystsutilizingtheeightstepsapproach may feel that a p-value is misleading and/or inadequate at summarizing one’s results. Rather, they may choose to determine statistical signiﬁcance using other values (e.g., Bayes factors, mean diﬀerences, slopes).', 'page_label: 30 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 30 •Reports that are purely exploratory. As users practice this approach to data analysis, they will quickly learn the richness intrinsic in data and recognize a largely untapped resource. Indeed, this is what I hopewill happen. This will inevitably lead to an inﬂux in EDA. While some have suggested allpublished analyses ought to be conﬁrmatory (Lindsay, 2015), I feel otherwise. Granted, EDA oﬀers weaker evidence than CDA, and all EDA ought to be followed by CDA. However, if the choice is between failing to publish EDA because one does not have the resources to CDA and publishing only the EDA, I choose the later. Published EDA results can then be replicated by someone who does have the resources. •Substantive conclusions that are based on non-traditional criteria (e.g., Bayes fac- tors, slopes, reaction time diﬀerences, correlation coeﬃcients, AICs). The ﬁrst step encourages researchers to set their own criteria for what is deemed “signiﬁcant” or important. The metrics of choice will vary from discipline to discipline and editors and reviewers ought to evaluate the choice of criteria in light of the substantive questions being asked. •Substantive conclusions that are based on graphical interpretation alone. This approach to data analysis recognizes the critical role graphics play in encoding important information, evaluating model ﬁt, conveying uncertainty, etc. If researchers utilize this approach, I hopethey will use graphical depictions to aid decisions of scientiﬁc relevance. If they do, they ought not to be penalized simply because it is diﬀerent. •Results expositions that seem disorganized. If these eight steps are followed, analyses will rarely be linear; rather, analysts may have to stumble through various modeling strategies quite iteratively as they search for the best representation of the data. Although some might be inclined to relegate the stops and starts to a supplemental section, others might feel ethically inclined to convey how they arrived at their conclusions in a way that reﬂects the messiness inherent in their analysis. Granted, the exposition should be clear, but I invite editors and reviewers to be forgiving of interpretations that take some time to unfold. •Conclusions that are hedged with uncertainty and/or ambiguity. Whilep-values invite a false sense of certainty, the approach I advocate often invites uncertainty. This is a good thing. In the past, authors writing eye-catching titles and conclusions have damaged the ﬁeld’s reputation and perpetuated fallacious beliefs in the media. Cautious language, though less ﬂashy than bold claims, is probably a better way of conveying results. •Extremely lengthy supplemental sections located on outside repositories. Once again, this eight step approach is all but guaranteed to increase the number of starts and stumbles during the data analysis process. In line with the second guiding principle of data analysis (utilizing sensitivity analyses for ad-hoc decisions), these starts and stumbles should be communicated to the audience. This may require a rather long supplemental section and editors and reviewers should be accepting about these sorts of submissions. •Detailed analyses reported via blogs, YouTube videos, podcasts, etc. Sometimes the', 'page_label: 31 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 31 optimal way to convey statistical results may not be in journal format, or even a traditional supplemental section. Writing for journals often invokes a need for precision and clarity, and writers may anticipate they’ll have to spend a great deal of time preemptively defending every statement and decision. This need for clarity and precision is a good thing, but may be a handicap during the more ambiguous stages of data analysis. Additionally, some results are best interpreted interactively (e.g., through rotating three dimensional plots or multiple static plots), while others may be best understood during stream-of-consciousness narration. Other avenues may be more amenable to expounding on the process whereby the researcher arrived at their conclusions, and editors and reviewers may need to be ﬂexible in allowing this. In this paper, for example, I uploaded a screencast on my YouTube channel that more fully explains my thought-process (see https://youtu.be/5BpmktmvgIA), which allowed me to give more detailed information than I might have if I were required to write about it. •Alternative modeling strategies (random forest, SVM, Bayesian methods). The eight steps inevitably will shift an analyst’s perspective away from testing hypotheses and toward building a statistical model that adequately represents the data. Sometimes the best model to use will not be a t-test, ANOVA, regression, etc. I anticipate that as more people adopt this approach, diverse modeling approaches will become more common. •Become comfortable with subjectivity . As researchers begin to make personalized decision criteria, they may choose to set their decision criteria based more on their subjective judgment of a meaningful eﬀect. For example, a researcher might decide that a smoking cessation program is eﬀective if greater than 20% of participants quit without remission. In this situation, it may be tempting to reject an author’s decision criteria for being arbitrary (why not 30%?). However, there is nothing inherently wrong with a subjective decision criteria. A threshold of 0.05 is an arbitrary threshold forp-values. A researcher’s decision criteria should be judged not on how objective it is, but by its stringency. In summary, my recommendation for editors and reviewers is to never reject an article simply because it utilizes a non-standard approach for arriving at substantive conclusions. Rather, the approach should be evaluated in terms of how well it ﬁts the needs of the situation and whether the assumptions of the model are reasonably met. If the model is appropriate and reasonable, there’s no reason an author’s attempt at ingenuity should count against them. In conclusion, the discipline of psychology is at a crossroads. We can continue to participate in NHST-based psychology and the problems we have recently encountered will persist. Or we can revolutionize the way we think about analysis, listen to the messages the data are trying to tell us, and uncover truths previously buried behind ANOVA summary tables andp-values.', 'page_label: 32 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 32 References Baguley, T. (2009). Standardized or simple eﬀect size: What should be reported? British Journal of Psychology ,100(3), 603–617. doi:10.1348/000712608X377117 Berkson, J. (1942). Tests of signiﬁcance considered as evidence. Journal of the American Statistical Association ,37(219), 325–335. doi:10.1080/01621459.1942.10501760 Bond, C. F., Wiitala, W. L., & Richard, F. D. (2003). Meta-analysis of raw mean diﬀerences. Psychological Methods ,8(4), 406–18. doi:10.1037/1082-989X.8.4.406 Bonneau, G. P., Hege, H. C., Johnson, C. R., Oliveira, M. M., Potter, K., Rheingans, P., & Schultz, T. (2014). Overview and state-of-the-art of uncertainty visualization. Mathematics and Visualization ,37, 3–27. doi:10.1007/978-1-4471-6497-5_1 Breiman, L. (2001). Random forests. Machine Learning ,45(1), 5–32. doi:10.1023/A:1010933404324 Chawla, D. S. (2016, October). Oh, well - \"love hormone\" doesn’t reduce psychiatric symptoms, say researchers in request to re- tract. Retrieved from http://retractionwatch.com/2016/10/04/ oh-well-love-hormone-doesnt-reduce-psychiatric-symptoms-says-researchers-in-request-to-retract/ Cohen, J. (1992). A power primer. Psychological Bulletin ,112(1), 155–159. doi:http://dx.doi.org/10.1037/0033-2909.112.1.155 Cohen, J. (1994). The earth is round (p < .05). American Psychologist ,49(12), 997–1003. doi:10.1037/0003-066X.49.12.997 Cook, T. D., & Campbell, D. T. (1976). The design and conduct of quasi-experiments and true experiments in ﬁeld settings. Research in Organizations: Issues and Controversies , 223–326. Cortina, J. M., & Landis, R. S. (2011). The earth is not round (p = .00). Organizational Research Methods ,14(2), 332–349. doi:10.1177/1094428110391542 Counsell, A., & Harlow, L. L. (2017). Reporting practices and use of quantitative meth- ods in canadian journal articles in psychology. Canadian Psychology/Psychologie Canadienne ,58(2), 140–147. doi:10.1037/cap0000074 Coyne, J. C. (2016). Replication initiatives will not salvage the trustworthiness of psychology. BMC Psychology . doi:10.1186/s40359-016-0134-3 Cumming, G. (2014). The new statistics: Why and how. Psychological Science ,25(1), 7–29. doi:10.1177/0956797613504966 Cumming, G., Fidler, F., Leonard, M., Kalinowski, P., Christiansen, A., Kleinig, A., ... Wilson, S. (2007). Statistical reform in psychology is anything changing ? Psychological Science ,18(3), 1–4. doi:10.1111/j.1467-9280.2007.01881.x', 'page_label: 33 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 33 Cumming, G., Fidler, F., & Thomason, N. (2001). The statistical re-education of psychology. InThe 6th international conference on teaching statistics. Hawthorn, Victorial: Swinburne Press. Edwards, J. R., & Berry, J. W. (2010). The presence of something or the absence of nothing: Increasing theoretical precision in management research. Organizational Research Methods,13(4), 668–689. doi:10.1177/1094428110380467 Erceg-Hurn, D. M., & Mirosevich, V. M. (2008). Modern robust statistical methods: An easy way to maximize the accuracy and power of your research. The American Psychologist ,63(7), 591–601. doi:10.1037/0003-066X.63.7.591 Fife, D. A. (2019a). A graphic is worth a thousand test statistics: Mapping visuals onto common analyses. Retrieved from http://rpubs.com/dustinﬁfe/528244 Fife, D. A. (2019b). Flexplot: Graphical-based data analysis [r and jamovi]. Available at www.Jamovi.com; www.github.com/dustinﬁfe/ﬂexplot. doi:10.31234/osf.io/kh9c3 Fife, D. A., & Rodgers, J. L. (2019). Exonerating eda: Addressing the replication crisis by expanding the eda/cda continuum. Unpublished Manuscript . Retrieved from http://quantpsych.net/ﬁfe-exonerating-eda-draft-oct2019-df-edits/ Furr, R. M. (2014). Scale construction and psychometrics for social and personality psychol- ogy. Thousand Oaks, CA: SAGE. doi:10.4135/9781446287866 Harlow, L. L., Mulaik, S. A., & Steiger, J. H. (2016). What if there were no signiﬁcance tests?(2nd ed.). New York, NY: Routledge. Healy, K., & Moody, J. (2014). Data visualization in sociology. Annual Review of Sociology , 40(1), 105–128. doi:10.1146/ANNUREV-SOC-071312-145551 Hoekstra, R., Kiers, H., & Johnson, A. (2012). Are assumptions of well-known sta- tistical techniques checked, and why not? Frontiers in Psychology ,3(137). doi:10.3389/fpsyg.2012.00137 Hofmann, S. G., Fang, A., & Brager, D. N. (2015). Eﬀect of intranasal oxytocin admin- istration on psychiatric symptoms: A meta-analysis of placebo-controlled studies. Psychiatry Research ,228(3), 708. doi:10.1016/j.psychres.2015.05.039 JASP Team. (2019). JASP (version 0.10.2)[Computer software]. Retrieved from https: //jasp-stats.org/ Jones, L. V. (1952). Test of hypotheses: One-sided vs. Two-sided alternatives. Psychological Bulletin,49(1), 43. Kessler, R. C., Barker, P. R., Colpe, L. J., Epstein, J. F., Gfroerer, J. C., Hiripi, E., ... Zaslavsky, A. M. (2003). Screening for serious mental illness in the general population. Archives of General Psychiatry ,60(2), 184–189. doi:10.1001/archpsyc.60.2.184 Kruschke, J. K., & Liddell, T. M. (2018). The bayesian new statistics: Hypothesis test- ing, estimation, meta-analysis, and power analysis from a bayesian perspective. Psychonomic Bulletin and Review ,25(1). doi:10.3758/s13423-016-1221-4', 'page_label: 34 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 34 Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2004). Applied linear statistical models. In Applied linear statistical models . New York, NY: McGraw-Hill/Irwin. Levine, S. S. (2018). Show us your data: Connect the dots, improve science. Management and Organization Review ,14(2), 433–437. doi:10.1017/mor.2018.19 Lindsay, D. S. (2015). Replication in psychological science. Psychological Science ,26(12), 1827–1832. doi:10.1177/0956797615616374 Loken, E., & Gelman, A. (2017). Measurement error and the replication crisis. Science, 355(6325). doi:10.1126/science.aal3618 Maxwell, S. E., & Delaney, H. D. (2004). Designing experiments and analyzing data: A model comparison perspective . New York, NY: Taylor & Francis. Micceri, T. (1989). The unicorn, the normal curve, and other improbable creatures. Psycho- logical Bulletin ,105(1), 156–166. Retrieved from https://pdfs.semanticscholar.org/ 2903/180261ee0d99a27cfe85cde9cf4af74923c6.pdf Morey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The fallacy of placing conﬁdence in conﬁdence intervals. Psychonomic Bulletin and Review,23(1). doi:10.3758/s13423-015-0947-8 National Survey on Drug Use and Health. (2014). National survey on drug use and health 2014 . Substance Abuse; Mental Health Services Administration, Center for Behavioral Health Statistics; Quality. Retrieved from https://www.dataﬁles.samhsa. gov/study/national-survey-drug-use-and-health-nsduh-2014-nid13618 Nelson, L. D., Simmons, J. P., & Simonsohn, U. (2018). Psychology’s renaissance. Annual Review of Psychology ,69, 511–545. doi:10.1146/annurev-psych-122216 Osborne, J. W. (2013). Is data cleaning and the testing of assumptions relevant in the 21st century? Frontiers in Psychology ,4. doi:10.3389/fpsyg.2013.00370 Pashler, H., & Wagenmakers, E.-J. (2012). Editors’ introduction to the special section on replicability in psychological science: A crisis of conﬁdence? Perspectives on Psychological Science ,7(6), 528–530. doi:10.1177/1745691612465253 project, T. jamovi. (2019). Jamovi (version 0.9) [computer software]. Retrieved from https://www.jamovi.org Rodgers, J. L. (2010). The epistemology of mathematical and statistical modeling: A quiet methodological revolution. The American Psychologist ,65(1), 1–12. doi:10.1037/a0018326 Rothman, K. J. (2010). Curbing type i and type ii errors. European Journal of Epidemiology , 25(4), 223–224. doi:10.1007/s10654-010-9437-5 Schmidt, F.L.(1996). Statisticalsigniﬁcancetestingandcumulativeknowledgeinpsychology: Implications for training of researchers. Psychological Methods ,1(2), 115–129. doi:10.1037/1082-989X.1.2.115', 'page_label: 35 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 35 Shadish, W., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference . Boston, MA: Houghton Miﬄin. Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology. Psycho- logical Science ,22(11), 1359–1366. doi:10.1177/0956797611417632 Simonsohn, U. (2014). Posterior-hacking: Selective reporting invalidates bayesian results also.SSRN Electronic Journal . doi:10.2139/ssrn.2374040 Steegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science ,11(5), 702–712. doi:10.1177/1745691616658637 Szollosi, A., Kellen, D., Navarro, D., Shiﬀrin, R., Rooij, I. van, Zandt, T. V., & Donkin, C. (2019). Preregistration is redundant, at best. doi:10.31234/OSF.IO/X36PZ Traﬁmow, D. (2017). Using the coeﬃcient of conﬁdence to make the philosophical switch from a posteriori to a priori inferential statistics. Educational and Psychological Measurement ,77(5), 831–854. doi:10.1177/0013164416667977 Tukey, J. W. (1986). Analyzing data: Sanctiﬁcation or detective work? The Collected Works of John W. Tukey , 721–737. Valentine, J. C., Aloe, A. M., & Lau, T. S. (2015). Life after nhst: How to describe your data without p-ing everywhere. Basic and Applied Social Psychology ,37(5), 260–273. doi:10.1080/01973533.2015.1060240 Wagenmakers, E.-J., Wetzels, R., Borsboom, D., Maas, H. L. J. van der, & Kievit, R. A. (2012). An agenda for purely conﬁrmatory research. Perspectives on Psychological Science,7(6), 632–638. doi:10.1177/1745691612463078 Wicherts, J. M., Veldkamp, C. L. S., Augusteijn, H. E. M., Bakker, M., Aert, R. C. M. van, & Assen, M. A. L. M. van. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in Psychology ,7(NOV). doi:10.3389/fpsyg.2016.01832 Wilkinson, L., & Task Force on Statistical Inference. (1999). Statistical methods in psychology journals: Guidelines and explanations. American Psychologist ,54(8), 594–601.', 'file_path: /home/cuphead/Projects/llama-index/data/maroctelecom.txt  Maroc Telecom Company type\\tPublic Traded as \\tEuronext Paris: IAM Industry\\tTelecommunications Founded\\tFebruary 3, 1998; 26 years ago Headquarters\\tRabat, Morocco Key people \\tAbdeslam Ahizoune, Chairman & CEO Laurent Mairot, CFO Products\\tLandline phones, Mobile phone lines, Fiber-optic Internet, ADSL, 4G+ Revenue\\tIncrease US$ 3,6 billion (2018) Net income \\tIncrease US$ 610 million (2018) Owners\\t      Etisalat by e& (53%)     Moroccan government (22%)  Number of employees \\t10,609 (2018) Website\\thttp://www.iam.ma  Maroc Telecom (Acronym: IAM, Arabic: اتصالات المغرب) is the main telecommunications company in Morocco.[1][2] Currently employing around 11,178 employees, it is the largest telecommunications network in the country with 8 regional delegations and 220 offices present across Morocco. The company is listed on both the Casablanca Stock Exchange and Euronext Paris. History See also: Postal history of Morocco  The origin of a Moroccan telecommunications project dates back to 1891, when Sultan Hassan I created the first Moroccan postal service. In 1913, the Moroccan Postal Telephone and Telegraph was established before a Dahir (King\\'s decree) related to the monopoly of the state of Telegraphy and Telephony was published.  In 1967, Morocco placed the first underwater cable between Tetouan, Morocco, and Perpignan, France, through the Mediterranean. A few years later, in 1970, a transmission via INTELSAT was introduced. The Telex service was then automated in 1971 just before installing a digital center in Fes.  Due to the advancement of telecommunications around the globe, Morocco decided to create a new entity called the Office National des Postes et Télécommunications (ONPT) to manage the industry. ONPT was responsible of the introduction of Analog Mobile Radiotelephony in 1987. Later on, in 1992, Morocco set up the first underwater optical fiber cable. Two years later, a GSM service was operational. The Internet was introduced in Morocco by ONPT in 1995.  After the publication of a telecommunications\\' decree, Maroc Telecom (IAM) was eventually founded in 1998. The acronym IAM comes from its original Arabic name Ittisalat Al Maghrib.[3] The name \"Maroc Telecom\" was adopted later for better international recognizability. Privatization  On 20 February 2001, the Moroccan government sold 35% of Maroc Telecom\\'s shares to French mass media company Vivendi. The transaction amounted to 23 Billion dirhams.[4] On 4 January 2005 Vivendi acquired an additional 16% for 12.4 billion dirhams raising its participation to 51%.[5] In October 2007, the CDG ceded, via its subsidiary Filpar Holding, 2% of Maroc Telecom to Vivendi in exchange of 0.6% of Vivendi\\'s shares, putting the total shares owned by Vivendi to 53%.[6]  In 2006, the company reported a turnover of $2.67 bn. The custom base was established at 1.27m lines for the landline and at 391,000 lines for the ADSL.[7]  In July 2013, it was announced that the firm’s majority owner, Vivendi, would sell its 53% stake in the firm to Etisalat for around $4.2 billion.[8]  In 2016, Maroc Telecom introduced fiber optics to the country with speeds up to 200 Mbits/s. Activities Land lines  It consists of the provision of public phones throughout Morocco. The fixed park reaches 1.6 million lines. Mobile phodocuments in the data folder (which in this case just consists of the essay text, but could contain many documents).nes  Mobile services are provided via a GSM network. Maroc Telecom counted 33 million customers at the end of October 2012. Its network covers 97% of the Moroccan population. It also has 12.5 million customers in Mali, Gabon, Burkina Faso and Mauritania. It is one of the most profitable phone operators in Africa with a revenue of 2.2 billion euros during the first 9 months of 2012.[9]  Maroc Telecom launched 4G+ in Morocco on July 13, 2015.', 'file_path: /home/cuphead/Projects/llama-index/data/maroctelecom.txt  Activities Land lines  It consists of the provision of public phones throughout Morocco. The fixed park reaches 1.6 million lines. Mobile phodocuments in the data folder (which in this case just consists of the essay text, but could contain many documents).nes  Mobile services are provided via a GSM network. Maroc Telecom counted 33 million customers at the end of October 2012. Its network covers 97% of the Moroccan population. It also has 12.5 million customers in Mali, Gabon, Burkina Faso and Mauritania. It is one of the most profitable phone operators in Africa with a revenue of 2.2 billion euros during the first 9 months of 2012.[9]  Maroc Telecom launched 4G+ in Morocco on July 13, 2015.[10] Customer support  Customer support does only propose contacting by phone to a special number and not via a contact form like most of telecommunication companies.[11] Projects and investments  On June 1, 2006, IAM launched the IPTV package deployed by Huawei Technology[12] via the ADSL line. The service was the first of its kind in Africa and the Middle East.  In July 2006, Maroc Telecom signed with the French telecommunications equipment company Alcatel a contract for a submarine communications cable connection between Morocco and France. Maroc Telecom\\'s aim is to upgrade the capacity of its services (i.e. broadband services, call centers). The project cost €26 million and was named \"Atlas Offshore\".[13]  In December 2006, IAM invested in Burkina Faso’s ONATEL, acquiring 51% of its capital.[14]', \"file_path: /home/cuphead/Projects/llama-index/data/story.txt  Once upon a time, in a town nestled between two mountains that whispered secrets to the wind, there lived a peculiar man named Ephraim. Ephraim was known throughout the village for his uncanny ability to communicate with animals—not just the usual dogs and cats, but birds, insects, and even the occasional elusive fox.  One fateful morning, when the sun rose in hues of apricot and lavender, Ephraim set off on his daily trek into the forest. He was in search of an ancient oak rumored to hold the key to understanding the language of the forest creatures. Legends whispered that whoever could decipher their tongue would gain unimaginable wisdom and power.  As Ephraim ventured deeper into the woods, the trees seemed to murmur with anticipation. Squirrels chattered excitedly overhead, and deer watched him with curious eyes. It was as if the whole forest knew of his quest.   Hours turned into days as Ephraim navigated through thick underbrush and crossed babbling brooks. He survived on berries and nuts, sharing his meals with the woodland critters who guided him along hidden paths.  Finally, on the seventh day of his journey, Ephraim stumbled upon the ancient oak. Its gnarled branches reached toward the heavens like a thousand crooked fingers, and its trunk was marked with symbols carved by creatures long forgotten. With trembling hands, Ephraim touched the rough bark, feeling the pulse of ancient magic beneath his fingertips.  Suddenly, a voice whispered in his ear—not with words, but with images and emotions. It was the language of the forest, a symphony of nature's secrets woven into the fabric of existence. Ephraim closed his eyes and listened, his heart pounding with wonder.  From that day forward, Ephraim became the bridge between humanity and the natural world. He helped settle disputes between rival wolf packs, taught young owls the art of silent flight, and even brokered peace between the mischievous sprites of the glade and the stern guardians of the river.  But Ephraim's newfound gift came with a price. The more he understood the language of the forest, the less he could communicate with his fellow villagers. They whispered that he had gone mad, talking to trees and laughing with the wind. Some feared him, while others revered him as a sage touched by the gods.  As the years passed, Ephraim grew old and frail, his body bent like the ancient oak that had revealed the secrets of the forest to him. On his last day, surrounded by friends both human and animal, Ephraim whispered his final words to the wind—a promise to protect the delicate balance between the realms he had come to love.  And so, when Ephraim passed from this world, the animals of the forest gathered to mourn him. Birds sang melancholy melodies, and flowers bloomed in hues of mourning purple. But in the hearts of those who remembered him, Ephraim lived on as a legend—a strange and wondrous tale of a man who spoke the language of the wild.\"], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f20fbc1cb80>, 'json_data': {'input': ['page_label: 1 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  The Eight Steps of Data Analysis: A Graphical Framework to Promote Sound Statistical Analysis Dustin Fife1 1Rowan University Abstract Data analysis is a risky endeavor, particularly among those unaware of its dangers. In the words of Cook and Campbell (1976; see also Shadish, Cook, & Campbell, 2002), “Statistical Conclusions Validity” threatens all research subjected to the dark arts of statistical magic. Although traditional statistics classes may advise against certain practices (e.g., multiple comparisons, small sample sizes, violating normality), they may fail to cover others (e.g., outlier detection and violating linearity). More common, perhaps, is that researchers may fail to remember them. In this paper, rather than rehashing old warnings and diatribes against this practice or that, I instead advocate a general statistical analysis strategy. This graphically-based eight step strategy promises to resolve the majority of statistical traps researchers may fall in without having to remember large lists of problematic statistical practices. These steps will assist in preventing both false positives and negatives and yield critical insights about the data that would have otherwise been missed. I conclude with an applied example that shows how the eight steps reveal interesting insights that would not be detected with standard statistical practices. Keywords: statistical assumptions, NHST, conﬁrmatory data analysis, graph- ical data analysis, ﬁshing, p-hacking The ﬁeld of psychology has been forced to participate in methodological introspection, of sorts. This introspection began late in the 20th century as methodologists vehemently protested the knee-jerk focus on p-values Null Hypothesis Signiﬁcance Testing (NHST) encourages (Cohen, 1994; Harlow, Mulaik, & Steiger, 2016; Jones, 1952). The American I wish to thank those who assisted in reviewing this manuscript, including Tom Dinzeo, Polly Tremoulet, Jeﬀrey Greeson, Yoav Zeevi, Christine Simmons, and Joseph Rodgers, as well as the anonymous reviewers and editor. Correspondence concerning this article should be addressed to Dustin Fife, 201 Mullica Hill Road Glassboro, NJ 08028. E-mail: ﬁfe.dustin@gmail.com', 'page_label: 2 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 2 Psychological Association (APA) suggested several alternatives, including a stronger focus on estimation (i.e., identifying the strength of the eﬀect, as well as the size/direction of the parameters of interest; Wilkinson & Task Force on Statistical Inference, 1999), rather than the probability of the data in the face of no eﬀect (a strange hypothesis indeed!). This forced introspection not only continues today, but the replication crisis has heightened its necessity. A recent attempt to replicate 100 diﬀerent studies (from three top journals in psychology) resulted in poor replication metrics (Open Science Collaboration, 2015); estimates of eﬀect sizes were half as strong in the replications than in the original studies and 61% of the attempted replications failed to produce statistical signiﬁcance. (For an alternative perspective on the replication crisis, see Shrout & Rodgers, 2018 and Maxwell, Lau, & Howard, 2015). Some have suggested this replication crisis was caused (at least partially) by researchers’ over-reliance on NHST (and other statistical practices; Cumming, 2014; Pashler & Wagenmakers, 2012). In this paper, however, I will not rant and rave about NHST; Others have already highlighted its problems (Cohen, 1994; Cumming, Fidler, & Thomason, 2001; Schmidt, 1996; Traﬁmow, 2017). Rather, my purpose is primarily to bridge the gap between known best practices in data analysis and actual statistical application. In so doing, I hope to provide a step-by-step strategy targeted at applied researchers for developing a deeper understanding of one’s data. A Potential Misunderstanding Arguably, what spawned the ﬁrst methodological “crisis” in the 1990s was Jacob Cohen’s thorough and acerbic critique of signiﬁcance testing (Cohen, 1994). Near the conclusion of his article, he said, “don’t look for a magic alternative to NHST, some other objective mechanical ritual to replace it. It doesn’t exist” (p. 1001). I wonder if this statement is too easy to misunderstand. I believe what Cohen was trying to suggest was there is no onealternative to NHST; rather statistical analysis requires a rather large toolbox, where each tool is adapted to the circumstances under which it is most appropriate. The tool might be, for example, Bayes factor, conﬁdence intervals, eﬀect sizes, single-subjects designs, preregistration, and/or graphical data analysis. I fear that when some read Cohen’s words, they interpret it to mean “NHST is wrong and there is nothing else researchers can use.” This is a rather discouraging interpretation, yet this interpretation is easy to make. In the past, methodologists have been quick to critique, but much slower to oﬀer alternative recommendations. What exacerbates the problem is the fact that few agree about what ought to replace NHST (e.g., Cumming, 2014; Kruschke & Liddell, 2018). Again, the reason clear alternatives have failed to emerge is likely because the alternative is, more than likely, a toolbox rather than a speciﬁc tool. I am inclined to think few methodologists believe anyonealternative is ideally suited to all circumstances, and, as such, have shied away from oﬀering step-by-step instructions for best-practices because it feels too mechanical. On the other hand, applied researchers generally need some sort of structure. What I will introduce provides the structure without the mechanics. My framework is designed to train a researcher to look for cues in data that the toolbox needs to be adjusted. While NHST seems to allow one to follow a sequence of steps and arrive at an unambiguous conclusion, my approach allows one to follow a sequence of steps designed to gather evidence in order', 'page_label: 3 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 3 to arrive at a conclusion that may be ambiguous. NHST permits one to “turn oﬀ” their brain. My approach re-activates it. Although the approach and many of the graphical tools I present are new, most of the suggestions are not; for decades statisticians have advocated for non-mechanistic judgments, assessing model assumptions, graphical data analysis, and sensitivity analyses.1Despite this fact, practices have changed little over the years. Researchers rarely evaluate statistical assumptions (Hoekstra, Kiers, & Johnson, 2012) or plot their data (Healy & Moody, 2014; Levine, 2018), and researchers still rely heavily on signiﬁcance testing (Counsell & Harlow, 2017; Coyne, 2016). Perhaps part of the problem is that these suggestions are relatively de-centralized. In this paper, I oﬀer a centralized set of guidelines that have been cobbled from various sources that synthesize decades of suggestions into one uniﬁed step-by-step framework. This framework will not only protect against false conclusions, but will also free researcher’s minds from rigid NHST thinking that is endemic in psychology. In the following sections, I ﬁrst review several reasons NHST practices are pervasive in psychology, then discuss potential causes for the replication crisis. Next, I review how the eight steps of data analysis encourages a greater focus on estimation and “listening” to one’s data. Finally, I conclude with an example where I show how the eight steps deepened understanding of data. Should Psychology Abandon p-values? For decades, some methodologists have suggested signiﬁcance tests ought to have no place in psychological journals (Cohen, 1994; Harlow et al., 2016; Schmidt, 1996; Valentine, Aloe, & Lau, 2015). Yet there seems to be little evidence researchers aren’t using p-values to make decisions, nor does there seem to be much of a visible shift in statistical practices (Counsell & Harlow, 2017; Coyne, 2016; Cumming et al., 2007). Despite passionate and cogent arguments against NHST, several obstacles remain and willremain, no matter how red-faced methodologists get. These include: •Social pressures . The entire ﬁeld of psychology understands p-value speak and a researcher may decide not to venture outside NHST practices for fear of having an otherwise publishable paper relegated to the ﬁle drawer. •Habit. For many researchers, they have been doing NHST for decades. For these people, shifting away from such rote practices is counter-intuitive (and diﬃcult). •Learning . Abandoning p-values in favor of some other statistical practice may require considerable time and eﬀort that most researchers do not have. •p-values reduce ambiguity . Withoutp-values, it would open the ﬁeld to disagree- ment about what constitutes a scientiﬁcally signiﬁcant ﬁnding. A rigid cutoﬀ of 0.05 acts as an operational deﬁnition for a relationship that ought to be noticed (and published). 1Anecdotally, I’ve shared this manuscript with both statisticians and applied researchers. The statisticians tend to say, “Of course that’s how data analysis ought to be done. We’ve been advocating for that for years!” On the other hand, applied researchers say, “I never thought to do data analysis this way.”', 'page_label: 4 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 4 •p-valuesprovidea“translationalmechanism”fromtheorytodata . Asargued by Cortina and Landis (2011), NHST bridges theoretical language into data analysis, and back again into theoretical language. This translation, they argue, is ill-deﬁned, at best, and non-existent, at worst, in other statistical frameworks. The guidelines I introduce help side-step (and sometimes address head-on) all of these concerns. No researcher needs to learn additional statistical techniques (except when the eight steps reveal that traditional methods fail to model the data), or remove p-values from their method section . Rather, I advocate a simple shift in focus that will add richness to one’s statistical practices. Doing so will, over time, shift the culture away from p-values and toward a greater focus on estimation and attending to messages our data have long been trying to tell us. Potential Causes of Replication Crisis To diagnose the cause of the replication crisis, it is advantageous to think of the current predicament as a systematic, discipline-wide habit of committing Type I errors. Some authors (e.g.; Rothman, 2010) have suggested abandoning statistical NHST will ﬁx all Type I (and Type II) errors. This may be technically correct, but eliminating NHST will not necessarily alter the number of false positives/negatives when researchers utilize other metrics for decision-making. Regardless of whether p-values or some other metric are used, the sources of false positives (and false negatives) are the same. Shadish, Cook, and Campbell (2002), noted two characteristics of data in particular that may inﬂate Type I error rates (and/or false positives): (1) Violated assumptions of statistical tests (e.g., normality, homogeneity, linearity, independence), and (2) Fishing/multiple testing. Various authors have commented on both of these issues and I will brieﬂy review each in turn. Violated Assumptions of Statistical Tests Linear models (e.g., regression, ANOVA, t-tests, structural equation models) are the most common models in psychology. For these models to behave appropriately (i.e., for p-values to actually reﬂect the probability of a Type I error under the null), the data must meet four key assumptions: independence2, normality, homoskedasticity, and linearity.3 When decisions of signiﬁcance are determined based on p-values, some of these assumptions are more critical than others (e.g., if normality is violated, the probability of committing a Type I error under a statistical signiﬁcance decision criteria, given the null, tend to stay close to 0.05; while violations of independence will lead to signiﬁcant departures from 0.05). When violated, Type I error rates may remain fairly close 0.05, or may deviate substantially in either direction. Likewise, when violated, Type II errors may also be inﬂated. 2Independence is a serious assumption that, when violated, will result in substantial bias in estimat- ing standard errors. However, independence is more of a design issue than a characteristic of the data. Consequently, I will not address how to evaluate independence. 3Linearity is actually not an assumption of ANOVAs/ t-tests. Or, rather, linearity is an assumption, though it is guaranteed to be met with categorical predictors.', 'page_label: 5 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 5 The previous paragraph is how most textbooks write of statistical assumptions. I tend to think of them diﬀerently, particularly since I rarely use p-values for decision-making. If these assumptions are violated, it means the researcher has simply chosen the wrong statistical model, a condition easily ﬁxed by choosing another. Yes, p-values will not remain at 0.05 if assumptions are violated, but it also means the researcher has simply chosen a model that is not appropriate. It would be like choosing to compute the mean on highly skewed data; one can do it but the information gleaned may be misleading. If the wrong model is chosen, one might have a false positive or negative. The sensitivity of linear models to these assumptions have been well documented (e.g., Maxwell & Delaney, 2004; Osborne, 2013) as well as the consequences of violating these assumptions (Micceri, 1989). Yet rarely do researchers mention whether they checked for the appropriateness of linear models (Hoekstra et al., 2012). And because most researchers do not provide the datasets used for analysis4, it is unknown the degree to which psychological research has been corrupted by violated assumptions. Thesolutiontotheproblem, asImentionindetaillater, isagreaterfocusonestimation and graphical data analysis. Graphics allow the researcher to determine at a glance whether assumptions have been violated. Although violated assumptions may inﬂate false positives, I suspect a far more common cause is multiple testing. I will address this issue in the following section. Fishing/Multiple Testing/ p-hacking Most researchers are likely familiar with the problem of multiple testing: when there are four groups, for example, it would be unwise to perform a t-test comparing each and every group (group 1 vs. group 2, group 2 vs group 3, etc.). Likewise, it would be unwise to compute dozens of Cohen’s dvalues and only interpret those that are larger than 0.5. Though the probability of one test being signiﬁcant under the null is 0.05 (or the probability of one large dis small), the probability of rejecting one among several is much higher (much like the probability of rolling at least one six over the course of 10 rolls is much higher than the probability of getting it on the ﬁrst roll). Indeed, this problem is suﬃciently well-known that if any researcher were to submit a paper and report they performed 107 t-tests, the paper would likely be rejected. Yet multiple testing likely happens all the time in psychology, but in more nuanced ways (Simmons, Nelson, & Simonsohn, 2011). Suppose, for example, a researcher collected 10 covariates that could potentially muck up the relationship between the IV and the DV. Said researcher might include a covariate, then run the analysis. If the treatment eﬀect is non-signiﬁcant, the researcher may decide another covariate is more appropriate to use as a control. This practice may continue until one of the covariates yields statistical signiﬁcance on the treatment eﬀect. This is another form of multiple testing. Likewise, if the researcher measured several dependent variables, then performed statistical analysis on each DV until 4This statement is based on extensive (and frustrating) personal experience.', 'page_label: 6 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 6 signiﬁcance was achieved, this too constitutes multiple testing, yet it is not so explicitly and universally condemned as running multiple t-tests. The term that encapsulates the new ways in which some researchers participate in multiple testing is called “ p-hacking,” and p-hacking includes not only the practices I have mentioned (measuring multiple DVs and covariates and running several models until signiﬁcance is obtained), but also several others, including adding more observations until signiﬁcance is reached, and dropping an experimental condition. With a discipline so focused onp-values, it is simple to see why so many researchers exercise “researcher degrees of freedom” (knowingly or unknowingly) to achieve the “gold standard” of p<0.05. Various authors (e.g., Nelson, Simmons, & Simonsohn, 2018; Wagenmakers, Wetzels, Borsboom, Maas, & Kievit, 2012) suggest researchers voluntarily report their planned statistical analysis a priori (e.g., through the “as predicted” platform: https://aspredicted. org/), then be explicit in the paper about any modiﬁcations to the original plan. This is a great idea, when possible. However, it is extremely rare that researchers are prepared to make such detailed plans in advance; Also preregistration won’t address the statistical issues mentioned previously5(i.e, violating of statistical assumptions) or invite researchers to consider the uncertainty associated with statistical analysis. Few researchers are ready to propose such risky hypotheses; few analyses go exactly as planned and preregistration best works when modiﬁcations to the original plan are unlikely. In these situations, preregistration is well-equipped to prevent p-hacking. Indeed, that is exactly what it was designed to do. It was not designed, however, to deepen one’s understanding of data. One could, presumably, preregister a hypothesis, test the hypothesis, and replicate it, and still be misled if that person is not attending to the messages the data are trying to voice. The eight steps of data analysis I propose, on the other hand, were designed to give voice to data. These steps were not designed to prevent p-hacking. In other words, neither the eight steps nor preregistration alone will ﬁx the replication crisis. Together, however, they will shift the focus away from simply publishing a (potentially spurious) ﬁnding toward building sound scientiﬁc and mathematical models of reality. Type II Errors and the File-Drawer Problem The replication crisis highlights the fact that psychology may be inundated with false positives. Unfortunately, it is more diﬃcult to estimate the prevalence of false negatives. Researchers may spend months collecting data only to have a p-value not reach statistical signiﬁcance. Some may abandon the project, while others might participate in p-hacking until signiﬁcance is achieved. The framework I propose will alleviate the problem of both false positives and negatives. For example, a pattern may not reach statistical signiﬁcance for several reasons that would be detected under this framework, including outliers that pull means to be more similar, strong 5Preregistration will certainly invite authors to consider how they will handle violations of assumptions (Wicherts et al., 2016), but preregistration alone will not tell researchers how to detect and handle such violations. The eight steps will guide researchers making these decisions. As such, the two (preregistration and the eight steps) work hand in hand.', 'page_label: 7 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 7 non-linear patterns that are poorly represented by a straight line, and violated statistical assumptions that render traditional tests overly conservative. In either case, whether we publish spurious ﬁndings or abandon non-signiﬁcant results, spending more time with our data via the eight steps of data analysis will aid in shifting our focus to what the data are actually trying to tell us. Guiding Principles of Data Analysis Before I explain the eight steps, let me ﬁrst introduce three guiding principles of data analysis: 1.Plot raw data whenever possible . One can easily be deceived when a graphic displays only summaries of the data (e.g., means as dots and standard errors as lines). For example, suppose Figure 1 came from an experiment where subjects were randomly assigned to watch a neutral video or a violent video. Further, suppose the subjects were subsequently measured on aggression. If one were to simply interpret the left plot, they might believe the type of video had a large eﬀect on aggression. Yet when we overlay the “jittered”6raw datapoints (right panel), we see that the aggression scores are bimodal in the violent group. Perhaps that bimodality is caused by gender (e.g., males report higher aggression after watching the video, while females do not). This would be an important discovery that would be masked if one simply graphed the summaries rather than the raw data. Furthermore, the right panel shows the mean for the violent video group is quite misleading; the mean falls at a place where data are quite sparse. 2.Utilize sensitivity analyses whenever needed. Often times our data throw us curve balls that require making an ad hoc decision. For example, our data may require a transformation to render the residuals more normal, an outlier may require deleting, or a missing value may require imputing. One would hope the conclusions gleaned from data remain largely unaﬀected by whatever decision we make. The only way to determine whether our results are sensitive to our decisions is to run the analysis both ways. Others have called this a “multiverse” analysis (Steegen, Tuerlinckx, Gelman, & Vanpaemel, 2016). For example, suppose a researcher decides an outlier ought to be deleted. The researcher should then run the analysis both with and without the outlier deleted and determine the degree to which the results change. The researcher might also investigate other strategies for dealing with the outlier (e.g., treating it as missing then imputing that value). Under this situation, I strongly recommend the researcher report the results of all three analyses (at least in a supplemental section) and comment on whether the results are sensitive to the decision made. Note that this sensitivity analysis is comparing two models that test the samehypoth- esis, rather than two models that test diﬀerent hypotheses (i.e., a model comparison; 6Jittering means to add random noise to a categorical variable (in this case, Neutral and Violent, which may be coded as 1 and 2). Jittering categorical values prevents overlap of datapoints and makes it easier to see the distribution of the datapoints. See Fife (2019a) for examples of how to produce jittered plots in the point-and-click software called Jamovi.', 'page_label: 8 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 8 246 Neutral ViolentAggression 0246 Neutral ViolentAggression Figure 1. Two plots of the same data. In the left panel, only means and standard deviations are reported. In the right panel, the raw data points overlay the standard deviation bars. When possible, graphical displays of raw data are always preferred over graphical summaries. Rodgers, 2010). Model comparisons are excellent tools for teasing out competing explanations of the data, but these sorts of model comparisons are outside the scope of this paper. 3.Explicitly state where analyses fall on the exploratory/conﬁrmatory continuum. Ex- ploratory data analysis (EDA) is the process whereby researchers analyze data without preconceived notions of what patterns they might ﬁnd. While conﬁrmatory data analysis (CDA) has been likened to placing a hypothesis on trial, EDA is like detective- work; data are searched for interesting patterns that might be worth pursuing for future investigation (including preregistered replications). Researchers might engage in either activity, or anything in between, such as “rough” CDA (Fife & Rodgers, 2019). Unfortunately, researchers have blamed EDA for the replication crisis . This is unfortunate and comes from misunderstanding the role, purpose, and tools associated with EDA. Prior to the replication crisis, researchers were notdoing EDA. Rather, they had exploratory intentions that utilized conﬁrmatory tools. Additionally, results discovered through exploration were presented as if they were conﬁrmatory, which is a critical violation of one of EDA’s most important rules: users of EDA must be explicit about which results were obtained through exploration versus conﬁrmation. For a more thorough and complete treatment of EDA, CDA, and everything in between, see Fife and Rodgers (2019). These eight steps are designed to assist researchers participating in conﬁrmatory or “rough” conﬁrmatory research (Fife & Rodgers, 2019), or even exploratory data analysis. In conﬁrmatory studies, researchers have a preconceived hypothesis for which', 'page_label: 9 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 9 they seek to evaluate the evidence. However, along the way of evaluating the evidence, data analysis plans often do not pan out as intended; interesting patterns present themselves which a researcher may want to investigate, unexpected complications arise that necessitate modifying the data analysis plan. In alignment with the rules and ethics of EDA, it is necessary that researchers be explicit about which analyses were uncovered through exploratory analysis, and which were obtained through conﬁrmatory analysis. One might choose to blend the two into one paper, provided the researcher is again explicit about which results were conﬁrmatory and which were exploratory. Returning to Figure 1, for example, the researcher may have decided before collecting data that the video would make participants more aggressive, yet did not anticipate the bimodality of scores in the treatment group. In the report, the researcher might say, “Upon graphing the results, a bimodal distribution was discovered among the participants in the treatment group. These results were unanticipated a priori so we decided to explore this relationship further...” As mentioned previously, the eight steps can be used to guide conﬁrmatory, rough conﬁrmatory, or even exploratory data analysis. In the following section I will outline the eight steps and hope to illustrate how they might be used to direct the analyst’s focus in such a way that improves understanding of data. The Eight Steps to Data Analysis In this section I will describe each of the eight steps. For simplicity, I have included a table (Table 1) that shows the eight steps and the function each step serves. For each of the steps that follow, I will address why I recommend performing said operation and what weaknesses it aims to overcome. When evaluating the recommendations in Table 1, the reader might be inclined to say, “Why thesesteps instead of others?” For example, a Bayesian might insist that Bayes factors should accompany all analyses. Another might complain there’s no mention of missing data or correcting for unreliability. Still others might say these steps fail to include preregistration. To this question, I would oﬀer a few reasons why I am advocating for these steps rather than others. First, these eights steps are not about tools/techniques, but about the approach one takes toward data analysis. This approach is designed to direct the analyst’s attention toward the evidence in favor (or against) the chosen hypothesis. Additionally, these steps are fairly universal, regardless of what type of data one chooses to analyze. Techniques, on the other hand, are situation-speciﬁc. Bayes factors are excellent for deciding between two hypotheses, but not so excellent when one is concerned about estimation. Increasing reliability is necessary when one has unreliable measures, but unnecessary when one has no measurement error. Preregistration is critical for strong conﬁrmatory studies, but unnecessary when one is doing exploratory research. The eight steps, on the other hand, are always appropriate and are critical in helping the analyst decide which tools are appropriate for a given situation. For example, Bayes factors could be used for making a', 'page_label: 10 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 10 Table 1 A Summary of the eight Steps of Data Analysis Step Purpose 1. State the theoretical hypothesis Helps to minimize “ﬁshing” for statistical signiﬁ- cance Provides a translational map from theory to data Allows users to specify their own decision criteria Invites researchers to consider preregistering hy- potheses 2. Assess psychometric properties of variablesInvites researchers to think about the impact of measurement 3. Plot univariate distributions Helps identify outliers Helps identify issues with non-normality Assists in identifying coding errors 4. Plot a graphic to match the Directs focus toward the size of eﬀects theoretical hypothesis Helps identify potential problems with non- linearity/heteroskedasticity Improves cognitive encoding of results Highlights uncertainty 5. Study residuals Helps identify problems with normality (e.g., through histogram of residuals) Helps identify problems with non- linearity/homoskedasticity (e.g., through a residual dependence or SL plot) 6. Interpret parameter esti- mates/eﬀect sizesEncourages the researcher to focus on estimation before signiﬁcance Put graphical information into concrete numbers 7. Set a decision criteria (if appro- priate)Assists in making a decision about signiﬁcance 8. Replicate on a new dataset Encourages cumulative and reproducible science', 'page_label: 11 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 11 decision criteria that is also pre-registered (Step 1). Likewise, unreliability and missing data will reveal themselves in Steps 2/3. A second reason I advocate for these steps is because they are relatively uncontroversial. There’s a great deal of disagreement about the merits of Bayesian estimation (Simonsohn, 2014), conﬁdence intervals (Morey, Hoekstra, Rouder, Lee, & Wagenmakers, 2016), preregis- tration (Szollosi et al., 2019), etc. Too often, I think methodologists are quick to argue over diﬀerences between approaches and give less air time to similarities. No methodologists that I know of would argue that graphics are bad, evaluating model assumptions is problematic, or interpreting parameter estimates is dangerous. Rather, they argue this is obvious, yet people are rarely doing these things; to me, that seems to indicate that maybe we ought to spend less time arguing and more time going back to the basics. Third, I believe these steps are beneﬁcial because they emphasize graphics. The visual processing system has enormous bandwidth and is able to encode large amounts of information with little eﬀort (Bonneau et al., 2014). Graphics have a way of simultaneously highlighting uncertainty while also revealing problems with statistical models. Additionally, humans are really good at detecting visual patterns. If it is something we are good at, should we not be leveraging that when doing scientiﬁc research? Finally, I advocate for these eight steps because I ﬁnd them useful. I am not so dogmatic as to think these eight steps are theeight steps of data analysis. One might conjure a diﬀerent set of eight, or ten, or twenty. Each step seems to provide a diﬀerent “view” of the research question, each contributing a unique piece of evidence for the ﬁnal evaluation. When I use them, and when I teach my students to use them, we gain a lot of information about the data. Additionally, when I have, through laziness, bypassed a step, I have been guilty of making short-sighted conclusions. 1. State the Theoretical Hypothesis and (Optionally) State a Decision Criteria The ﬁrst step to data analysis ought to be to state the theoretical hypothesis. Ideally, this would take place long before the researcher actually collects data (i.e., the hypothesis is preregistered). Doing so in advance will assist in preventing researchers from “bending” their original hypothesis to ﬁt the actual analysis performed (e.g., “Oh yeah. I, uh, meant to include that as a covariate originally. I just forgot”). This step is entirely voluntary and preregistration strengthens the weight of the evidence in favor of the hypothesis. (It is, after all, much less impressive to “predict” an eﬀect you already discovered than to predict an eﬀect before you actually collect data). If the analyst does decide a decision is needed, I recommend three strategies that will increase the utility of their decision criteria: (1) mapping hypotheses to speciﬁc statistical parameters, (2) stating strong hypotheses, and (3) developing decision criteria for clinical “signiﬁcance.” Mapping hypotheses to parameters. In an ideal world, one’s statistical hypothe- sis is closely tied to the theoretical hypothesis, such that support for the statistical hypothesis provides support for the theoretical hypothesis. However, there is always some simpliﬁcation', 'page_label: 12 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 12 that occurs when one translates from theory to statistical inference. This is inevitable, yet it is critical to ensure that the two (theory and model) are as closely aligned as possible. To do so, the theoretically-derived hypotheses ought to be tied to speciﬁc parameters in a model. Too often researchers write well-crafted introduction sections, providing strong theoretical rationale for their chosen verbal hypothesis, yet, there is a disconnect between the well-crafted introduction and the results section; the hypothesis points to a particular parameter (e.g., the interaction term in a model or the main eﬀect of a predictor after controlling for another), yet the results report gobs of results and corresponding tests of signiﬁcance. On other occasions, the statistical hypothesis is very weakly tied to the theoret- ical hypothesis. Not only does this dilute the message (because the parameter of interest is buried between other tests), but this constitutes ﬁshing. Each reported p-value is the result of a tested hypothesis and, as such, each reported p-value ought to be clearly supported by strong theoretical rationale. If the introduction section only develops arguments for testing one parameter, then only one parameter ought to be interpreted (though all parameters ought to be reported). Granted, some analyses require entering other parameters in the model. For example, if the researcher’s hypothesis concerns an interaction eﬀect between two independent variables, the main eﬀects must be included in the model as well. However, these main eﬀects need not be tested (or rather, interpreted since software packages tend to report signiﬁcance for all parameters) for signiﬁcance because, again, the researcher’s hypothesis is not concerned with these parameters. Stating Strong Hypotheses. Years ago, Meehl (1967) criticized the use of zero as a tested hypothesis. That’s a rather low bar to pass. Instead, he advocated for “strong” hypotheses, where researchers specify numeric values for the hypothesized parameter. For example, rather than testing whether a correlation is diﬀerent from zero, a researcher can test whether the correlation is diﬀerent from +0.4. This amounts to reversing the role of the null and the alternative and can lead to some logistic problems (e.g., researchers might be inclined to collect small samples so they don’t have power to reject their cherished hypotheses). With some modiﬁcation, we might instead hypothesize the parameter of interest falls within a particular range (e.g., from r= 0.2 to 0.4), or at least that its direction is positive (or negative). Better yet, researchers might use the values of the parameters themselves to set their own decision criteria, which I will discuss next. For a more detailed treatment about various approaches for developing precise hypotheses see Edwards and Berry (2010). Developing decision criteria. As mentioned previously, one of the purported advantages of NHST is that it provides a bridge from theory to conclusion via a p-value (Cortina & Landis, 2011). However, not all decisions require one to make a decision. Clearly, certain situations call for such judgments (e.g., Does this ﬁnding have scientiﬁc merit? Should this treatment be used? Are side-eﬀects of medication small enough to merit implementation?) and alternative frameworks have no clear route from theory to judgment. Unfortunately, using a universal criteria ( p< 0.05) has, in a way, “hijacked” decision making from the scientiﬁc community. A p-value is a function of both the sample size and the eﬀect size. In certain domains (e.g., neuroscience) a large Nis simply not feasible, and yet the culture of NHST does not permit ﬂexibility in considering other decision criteria', 'page_label: 13 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 13 that allow for more lenient p-values. On the other hand, alternatives to NHST (e.g., eﬀect sizes) have been criticized because they do not provide simple rules for deciding whether a ﬁnding has (or has not) scientiﬁc merit (Cortina & Landis, 2011). I advocate, instead, the decision criteria be left in the hands of the researcher. Re- searchers may choose, if they wish, that a signiﬁcant ﬁnding is one that reaches p< 0.05. Other researchers, on the other hand, may decide a clinically signiﬁcant ﬁnding is one where d> 0.83, or a mean diﬀerence between treatment and control group is greater than 10 points, or expenditures are reduced by $10,000. In short, any metric may serve as the basis for making decisions; one is not limited to p-values. One might criticize this approach by asking what is to stop researchers from setting more lenient criteria than p< 0.05. Researchers have a vested interest in a paper reaching “signiﬁcance” (however it is deﬁned) and if they can lower the threshold for reaching signiﬁcance to even less than what is currently acceptable, they will abuse that. They might, for example, state in advance that any correlation greater than 0.0000001 is clinically signiﬁcant. Fortunately, researchers eventually have to defend their decision criteria when submit- ting their paper for review. If they set a low bar for their decision criteria at preregistration, they will have to answer to a skeptical community of reviewers at a later date. I suspect this knowledge will severely limit the degree to which researchers seek to abuse the practice of setting their own decision criteria. Rather, I suspect researchers will actually impose more stringent criteria on their hypotheses. In addition, by placing these sorts of decisions back in the hands of the researcher and the scientiﬁc community, the signiﬁcance of results will not have to be qualiﬁed as “statistically but not clinically signiﬁcant.” 2. Assess psychometric properties. Manyhavesuggestedthe“replicationcrisis”isaresultofattemptingtomakeconclusive answers on noisy data (Loken & Gelman, 2017). The obvious solution to the noise is simply to increase the sample size. In some situations, however, this is not feasible, nor is it always the most practical approach. Gelman (2018) noted that doubling the reliability of a test will yield equivalent gains in precision as quadrupling one’s sample size. In other words, we might get more “bang for our buck” by spending a bit more time with measurement. By assessing the psychometric properties of our measures, it will invite deeper thinking on measurement issues and how they might aﬀect data analysis. If our measures fail psychometrically, no amount of sophisticated modeling will yield any insights that have scientiﬁc merit. For more details about assessing psychometrics, see Furr (2014). 3. Plot the Univariate Distributions The third step of data analysis is to plot the univariate distributions of the variables of interest. For quantitative variables, histograms are good candidates. (Quantile-quantile plots may also be beneﬁcial, but they are less common and thus less interpretable). For', 'page_label: 14 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 14 categorical variables, bar charts are appropriate. A visual of the distribution will inform the researcher of several potential pitfalls in the coming analysis, such as: •Incorrectly coded values (e.g., with a second wave of data collection, the researcher accidentally changes the treatment group designation from “Treatment” to “TRT,” then later aggregates the two data waves and accidentally treats those labeled as “Treatment” and “TRT” as separate groups) •Improperly coded missing values (e.g., a -999 is treated as a value, rather than a missing variable) •Non-normality (e.g., if there are excessive zeroes in the sample) •Outliers •Role reversals (e.g., if a bar chart shows many more women than men ﬁreﬁghters, the labels were likely ﬂipped) Beginning with such plots may prevent embarrassing retractions later. For example, Hofmann, Fang, and Brager (2015) wrote an article that suggested Oxytocin reduced psychiatric symptoms, but later had to retract the article. When entering the eﬀect sizes for a meta analysis program, they assumed all eﬀect sizes were positive. Because the program they used required specifying which eﬀects were negative (and because they improperly assumed they were all positive), the aggregated eﬀect size was inﬂated. This could be avoided by simply plotting the univariate distribution of eﬀect sizes. (Read more at Chawla, 2016). At this point it may not be necessary to address the outliers and/or non-normality of the data. Remember the assumption of linear models (e.g., regression, ANOVA, t-tests) are that the residuals of the model are normally distributed. The outcome variable itself need not be normal (though it usually helps). Often including one’s predictors in a model will render the residuals normal, even if the variable itself was not normal. Likewise, an outlier in univariate space may not be an outlier in multivariate space. However, plotting the univariate distributions in advance will inform the researcher of potential problems that may occur later in the analysis. For a video playlist on evaluating univariate distributions, see https://yt.vu/p/PL8F480DgtpW-T_ySqIurOMIaChNlOr3Ka. 4. Plot a Graphic to Match the Theoretical Hypothesis Once the univariate distributions are plotted (and any coding errors handled), the next step is to plot a graphic to match the theoretical hypothesis. If one were to perform simple linear regression, for example, a scatterplot would help the reader visualize the results. For ANOVAs/ t-test, violin plots or bee swarm plots would be appropriate. Although some of the strategies used (and types of plots) may be new, all of them are easy to produce in either R or even point and click software, such as Jamovi (project, 2019) or JASP (JASP Team, 2019).7For a tutorial on determining which graphic is most appropriate and for instructions on creating each of these types of plots, see Fife (2019a), as well as a video playlist at https://yt.vu/p/PL8F480DgtpW8WFhHFRzos7iUK2r-MhmKw. 7SPSS can perform most of these plots, but unfortunately (as far as I know) does not allow the user to overlay the raw datapoints over boxplots, mean plots, multi-way dot plots, etc.', 'page_label: 15 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 15 The advantage of utilizing goodgraphics is that they make it nearly impossible to deceive one’s readers (or one’s self, for that matter, especially if raw data are displayed). Sound graphics are essential for identifying two problems in particular: (1) outliers, and (2) non-linearity. If one or more outliers are present, it is possible they are driving statistical signiﬁcance. If so, this will be easy to determine from a graphic. Likewise, if the researcher attempts to ﬁt a straight line to data that are clearly non-linear, usually a graphic will show the error of one’s ways. If this is the case, one need not hang their head in defeat. It simply means the researcher has chosen the wrong model and must select another (such as polynomial regression, a transformed dependent variable, non-parametric procedures, or generalized linear models). To better detect departures from linearity, I recommend adding loess lines to a graphic; the package Flexplot (Fife, 2019b), which is available in R, the point-and-click platform Jamovi, and will shortly be released in JASP, defaults to show loess lines in scatterplots. Loess lines are non-parametric curves that are allowed to “bend” with the data. They can assist in detecting non-linearities that a standard model (which forces a straight line) would not detect. Recall our original goal: we are trying not to make a false positive/negative. At this point, it is impossible to make a false conclusion because we haven’t made a conclusion at all. We have not yet even computed an estimate, let alone made a decision about statistical signiﬁcance. In addition, the researcher may decide that computing the p-value on a dataset is not necessary. If the visual analysis shows a whopping eﬀect, who really cares about whether it is statistically signiﬁcant? In the words of Joseph Berkson, these sorts of images pass the “intraocular trauma test” (Berkson, 1942). (Though the converse is also true. I once had a graduate student produce scatterplots and ﬁnd the predicted relationship was non-existent. She immediately quit the analysis and concluded, “If there is an eﬀect, it’s so small I don’t even care about it.” I promptly gave her an A on her assignment). In short, plotting the data before computing the analysis will prevent researchers from deceiving themselves, render tests of statistical signiﬁcance less necessary, and force the reader to think in terms of the size of the eﬀect, rather than its existence. Unfortunately, not all statistical analyses lend themselves nicely to graphical display. Structural equation models, factor analysis models, and hierarchical linear models, for example, are more diﬃcult to map onto a single plot. These may require multiple plots and, unfortunately, the fragmented nature of these graphics may detach the visuals slightly from the actual analysis. Future research ought to attempt to bridge that gap and ﬁnd intuitive, graphical representations of these more complex models. 5. Study the Residuals After plotting the data, the researcher may not know if the chosen analysis (e.g., linear regression) is appropriate. The data may violate the assumption of linearity, normality of residuals, or homoskedasticity. Yet in order to properly diagnose the problem, we ought', 'page_label: 16 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 16 to compute the residuals (and thus have to actually perform the analysis). Unfortunately, before extracting the residuals, the model has to actually be ﬁt to the data. I would advise the reader to close one’s eyes (metaphorically or otherwise) until after the residuals have been studied before studying the results of a statistic. With residuals in hand, the researcher is now ready to properly assess the assumptions of the model. It is common (at least in the bio-medical literature) to perform statistical tests of normality or homoskedasticity, or independence. I would advise against it. Like other tests of signiﬁcance, these tests of assumptions are sensitive to sample size. With a small enough N, even large departures from statistical assumptions are not detected, while with largeN, even trivial diﬀerences are ﬂagged. These statistical tests tell us whether our distributions depart from what is expected. They do not tell us whether they are diﬀerent enough to muck up our analysis. The latter is better done through visual interpretation of results (as well as through a sensitivity analysis). Histograms will inform the researcher whether the normality assumption has been approximately met. Residual dependence plots assist in determining whether linear- ity and homoskedasticity have been met. Either of these plots will assist in ﬂagging outliers. For instructions in how to diagnose problems using these plots, see Kutner, Nachtsheim, Neter, and Li (2004), as well as the following YouTube playlist: https: //yt.vu/p/PL8F480DgtpW8v-h-7s9Ih826Qi7aa3rBS If the visual inspection of the residuals signals problems, one may have to iterate through steps 2-4 until the assumptions have been met, each time making a modiﬁcation to the model (such as transforming the DV, removing outliers, utilizing weighted least squares, or using generalized linear models). Furthermore, problems at this early stage demonstrate the researcher is not yet ready for conﬁrmatory data analysis. Again, there is nothing wrong with migrating to rough conﬁrmatory or exploratory data analysis and if a researcher ﬁnds the intended model is not appropriate, the researcher ought to explicitly state their analysis has turned from conﬁrmatory to something else (Fife & Rodgers, 2019). Regardless, the researcher may proceed to Step 6 once the assumptions have been met. 6. Interpret Eﬀect Sizes/Parameter Estimates At this point the researcher has far more information about the data than what is typically reported in psychological journals; the researcher knows outliers are not driving the analysis, knows the model chosen is appropriate, and has a visual that illustrates the strength of the relationship between the variables of interest. After Step 5, the researcher ought to be conﬁdent the model chosen is appropriate (i.e., the assumptions of the model have been met). Once again, I emphasize that it is impossible to commit a Type I (or Type II) error because statistical signiﬁcance has not yet been evaluated. Likewise, no conclusions have been made, so it’s impossible to make a false positive. Rather, I recommend the researcher study and interpret eﬀect sizes and parameter estimates. We all have been cautioned against making mountains out of molehills, or emphasizing statistical signiﬁcance at the expense of practical signiﬁcance. This is why the APA recommended researchers report eﬀect sizes in addition to statistical signiﬁcance. I would argue practical signiﬁcance is far more important than', 'page_label: 17 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 17 statistical signiﬁcance. Studying the eﬀect size (and parameter estimates) before statistical signiﬁcance is a conscious choice aimed at reminding the researcher of this preference for estimation rather than signiﬁcance. Most statistical packages oﬀer readily available estimates of eﬀect sizes, including f2, part and partial correlations, r2, and Cohen’s d. To determine which measure of eﬀect is appropriate, I recommend the concise and eﬀective article by Cohen (1992). For software dedicated to eﬀect size calculations and graphical data analysis, see Fife (2019b), as well as a tutorial on estimates at https://yt.vu/p/PL8F480DgtpW8jshu9vTyCf4HqSHYx05Fw. Where possible, eﬀect sizes in the original (unstandardized) metric should be inter- preted (Baguley, 2009; Bond, Wiitala, & Richard, 2003; Tukey, 1986). In a regression, the parameters of interest are the slopes (and occasionally the intercept). For ANOVAs/ t-test, the parameters of interest are the mean diﬀerences between groups. For structural equation modeling, the parameters of interest are the path coeﬃcients. For logistic regression and other generalized linear models, the researchers may have to perform mental gymnastics as they attempt to interpret things in terms of log odds (or in terms of odds ratios). Studying these parameters adds another layer of depth at which the researcher can make sense of the data. Not only will it inform the researcher about the direction of the eﬀect (e.g., males scored higher in aggression than females, anxiety is positively predictive of depression, performance is inversely related to mood), but also oﬀers a mathematical equation that maps predictors onto outcomes. For example, suppose a researcher performs a regression that assesses weight loss from experimental condition, controlling for motivation. Further suppose the regression equation is as follows: weight change = 1.2−0.8×motivation −4.5×treatment −1.2×motivation ×treatment This regression equation would be an interesting result indeed. This suggests the following: •Those in the control group who have no motivation will actually gain an average of 1.2 pounds (because control group is the reference group) •Every time an individual increases their motivation by a point, they can expect to lose 0.8 pounds •The treatment group averages 4.5 pounds more weight loss than the control group •The relationship between motivation and weight loss is stronger for the treatment group than for the control group, such that for the treatment group, for every point increase in motivation, they lose an additional 2 pounds (i.e., 1.2+0.8) Granted, much of this information could be gleaned from a graphic, but the estimates put the visual interpretation into concrete mathematical terms that are interesting in their own right. Furthermore, the eﬀect sizes and parameter estimates reduce ambiguity inherent in visual interpretation. To further reduce ambiguity (and marry the ideas of signiﬁcance testing with estimation), a researcher should pair conﬁdence (or credible) intervals with these estimates. Doing so will further reduce ambiguity, while explicitly recognizing the degree of uncertainty.', 'page_label: 18 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 18 7. Make a Decision (If Applicable) Recall we have plotted univariate distributions to ﬂag potential data recording errors, assess normality, and identify potential outliers. We have also created a visual representation of our analysis that shows both the size of the eﬀects and the direction. We have also thoroughly assessed whether our model is appropriate through residual analysis, estimated eﬀect sizes, and interpreted parameter estimates. In short, we have much more thoroughly familiarized ourselves with our own data (Tukey, 1986). If, at this stage, the reader feels it rather pointless to assess statistical signiﬁcance, I have successfully made my point. This second to last step is entirely optional and ideally makes it clear that our data have long been trying to tell us much more than we have allowed them. Simply computing statistical signiﬁcance without doing the previous steps is akin to eating a single sprinkle oﬀ a large birthday cake. With so much richness remaining, it is a shame that we limit ourselves to a single test that is largely uninformative. Earlier, I advocated that researchers instead set their own decision criteria. At this point, making a decision of signiﬁcance is easy; one has already pre-speciﬁed what is clinically signiﬁcant and now they simply compute the numbers and identify whether signiﬁcance was reached. 8. Replicate With New Data The decision made in the previous step is always provisional. Few single studies have the power (statistically or otherwise) to make conclusive statements about the truthfulness of a hypothesis. Rather, these ﬁndings are tentative and ought to invite closer scrutiny and replication. The discovery of the Higgs Boson, for example, was not considered settled until after hundreds of trillions of replications. The estimates obtained for the parameter of interest may serve as a prior in a Bayesian analysis, or could to be aggregated into the next study (and others like it) via meta-analysis. Such cumulative methods will invite a greater sense of humility about one’s own role on the scientiﬁc process and consequently invite deeper attention to development of theory. Reporting Results I recommend every researcher perform the eight steps when doing data analysis. However, it may not be necessary to report every step in a journal article. Not only would this increase the length of most articles (a trivial problem as journals become more digitized), but it may detract from the purpose of the article (to evaluate the original hypothesis). However, at minimum, I strongly recommend a researcher’s ﬁnal report contain: 1. One or more graphical depictions of the analysis of interest (Step 4). 2.A comment on how the researcher determined the appropriateness of statistical as- sumptions. 3. Parameter and eﬀect size estimates, with conﬁdence or credible intervals.', 'page_label: 19 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 19 4.A supplemental section containing all graphics and sensitivity analyses ora link to a website where these can be viewed. In other words, I am not advocating for a complete revamping and replacing of how statistics are reported in journal articles. Rather, I suggest we add these few pieces of information so the richness of our data becomes more visible. Example In the section that follows, I decided notto re-analyze existing datasets of previously published papers for two reasons. First, it can be diﬃcult to ﬁnd studies where researchers have actually uploaded their data for public scrutiny (although the Open Science Framework is making data far more accessible). Second, I would hate to pick on researchers conscientious enough to actually oﬀer their dataset by highlighting their data analysis mistakes. I want to encourage openness, and becoming a public data vigilante would be counterproductive. Consequently, I will analyze a publicly available dataset, the National Survey of Drug Use and Health (2014) and oﬀer my own hypothesis that, when analyzed using traditional NHST methods, yields misleading results. 1. State the Theoretical Hypothesis of Interest and (Optionally) Set a Decision Criteria. Suppose I am a drug counselor that has had only marginal success in assisting heroin addicts overcome their addictions; those who use heroin experience more psychological distress, which in turn motivates them to escape their distress via heroin. Now let us suppose I believe promoting healthy behaviors (e.g., exercise, nutritious eating) will reduce psychological distress and help break that negative feedback loop. Ideally, one would perform an experiment, but perhaps as a preliminary study, I decide to use an existing dataset to perform an observational analysis to assess the potential eﬃcacy of promoting healthy behaviors in a full experiment. However, I may consider controlling for mental illness. One may be experiencing psychological distress because of their mental illness, which may make them more likely to escape such distress through drug use. Stated diﬀerently: Among heroin users, those who report having more healthy behaviors will report less psychological distress, after controlling for mental illness Further suppose that this is the ﬁrst attempt the researcher has made at evaluating this hypothesis. In other words, the researcher is in “rough” conﬁrmatory mode, at best, and may even be leaning toward exploratory research. Given that, it makes little sense to set a decision criteria since any attempt to do so will be somewhat arbitrary. For illustrative purposes, I will test this hypothesis using standard NHST methodology using an ANCOVA model. Based on these data, I could conclude:', 'page_label: 20 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 20 Self-reported health rating was signiﬁcantly associated with psychological distress, after controlling for mental illness, F(4,189) = 10.84,MSE = 28.86,p<.001, ˆη2 G=.187. As I will show, the above statement is both misleading as well as incomplete. 2. Psychometrics The NSDUH utilizes the Kessler-6 distress scale to measure psychological distress (Kessler et al., 2003). This is comprised of six items that asks the degree to which participants experienced the following symptoms within the last thirty days: nervousness, hopelessness, restlessness, depression, feeling everything required eﬀort, and worthlessness. Reliability estimates suggested that within this sample, the measure was quite consistent ( α=0.94). Because of the high reliability estimates, I simply summed the scores to create my distress scale. As for the other two variables, mental illness was derived using a logistic regression model that utilized various indicators of mental health (e.g., suicidal thoughts, suicidal attempts, major depression, psychological impairment). Health is a one-item, self-reported question that asks them to rate their overall health. Unfortunately, I cannot assess the reliability of either health or mental illness, since both were assessed using only one item. 3. Plot Univariate Distributions I plotted the univariate distributions of the three variables of interest: probability of mental illness, health rating, and psychological distress. These distributions are shown in Figure 2. The plots reveal potential issues with the data. The probability of mental illness is far from normally distributed. The mode of the distribution is near zero (i.e., the data are zero-inﬂated). Note that linear models make no assumptions of normality for the independent variables (or the dependent variables for that matter, rather the assumption is about the residuals of the dependent variable). However, in my experience, if both the IV and the DV are skewed, the assumption of linearity will almost certainly be violated. Given that the distress variable is also skewed, this could certainly be problematic for linear models (such as an ANCOVA). At this point I am primed to look for serious issues with normality, linearity, and likely heteroskedasticity. As I mentioned earlier, if these assumptions are violated, it simply means we have chosen the wrong model to ﬁt the data. 4. Plot a Graphic to Match the Analysis of Interest Recall that the ﬁctitious researcher has decided to perform an ANCOVA. An AN- COVA essentially performs a standard linear regression between the covariate and the DV, extracts the residuals, then performs an ANOVA on the residuals (though this is all done', 'page_label: 21 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 21 020406080 0.00 0.25 0.50 0.75 Probability of Mental Illnesscount poorfairgoodvery goodexcellent 0204060 countHealth Rating 0102030 0 10 20 Psychological Distresscount Figure 2 . Univariate distributions of psychological distress, health rating, and probability of mental illness. simultaneously with an ANCOVA). An “added variable plot” would be an appropriate graphic to match the ANCOVA, where the grouping variable is plotted against the residuals of the model where the covariate is removed. This image is shown in the left image of Figure 3. One shortcoming of this sort of plot is that it masks any violations of the assumption of homogeneity of regression. (Homogeneity of regression states that the regression lines for each group are parallel). As such, I have plotted a paneled graphic in Figure 3, which shows a diﬀerent scatterplot for each level of health rating, with quadratic lines overlaying the data. I used quadratic lines to see if nonlinearity might be an issue. Once again, there are a few things worth noting: 1.These ﬁtted lines are not “parallel,” meaning that the assumption of homogeneity of regression has been violated (indicating that ANCOVA is not appropriate). This also means that the left image in Figure 3 is misleading. 2. The ﬁtted lines are not linear, indicating that linear models will not be appropriate. 3.There are very few people who report poor or excellent health, suggesting that I might combine the poor/fair groups as well as the excellent/very good. Also at this point, I shouldn’t interpret the plots shown in Figure 3; the model clearly does not ﬁt. As a result, I may have to iterate through this step (and probably Step 4) until', 'page_label: 22 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 22 01020 poorfairgood very goodexcellent Health RatingDistress | Mental Illnesspoor fair good very good excellent 0.10.50.90.10.50.90.10.50.90.10.50.90.10.50.90510152025 MIDistress Figure 3 . Visual displays of the relationship between mental illness, health, and distress. The left image is an added variable plot of the relationship between health rating and distress (controlling for mental illness). The right image shows the relationship between mental illness and distress for each level of health rating. The lines are quadratic lines mapping the relationship between mental illness and distress, conditional on self-reported health rating. I ﬁnd a model that appropriately ﬁt the data. It is also clear that any attempt at strictly conﬁrmatory analysis must take a backseat as I have some decisions to make that were not anticipated a priori. I have a few options: 1.I can transform the dependent variable and attempt to “linearize” the relationships. This generally fails when the DV is zero-inﬂated (as it is in this case). 2.I can attempt to use non-parametric procedures, such as rank transformations of the dependent variable. Unfortunately, rank transformations fail to preserve interaction eﬀects (which are clearly happening, as seen in Figure 3). 3.I can perform more “modern” robust methods (Erceg-Hurn & Mirosevich, 2008). These methods essentially replace mean-based estimates (including conditional means) with trimmed means, standard variances with winsorized variances, and standard conﬁdence intervals (CIs) with bootstrapped CIs that are computed from the trimmed/winsorized estimates. Unfortunately, these methods would not work since more than 10% (the “traditional” degree of trimming from either tail) of the tail of the distribution is contained at zero. (In general, modern robust methods do not work well for zero- inﬂated data). 4.I can attempt to ﬁt a nonlinear model. Unfortunately, these sorts of models can be more diﬃcult to interpret (because coeﬃcients may not have intuitive interpretations). Unfortunately, allof these choices are complicated, and require some sophisticated modeling. I did not choose a sophisticated model to show oﬀ my statistics skills. (In fact, it', 'page_label: 23 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 23 took me an embarrassing amount of time to ﬁnd something that would actually model the data well)8. Rather, I chose a particularly illustrative example that, unfortunately, required a relatively complex model. (In full disclosure, I intentionally “ﬁshed” for an example that would be particularly illustrative). Of the strategies listed above, I favor the fourth strategy. Once again, I prefer a visual approach to modeling these data, so I will overlay the ﬁt of the model atop the raw datapoints. Before I do so, however, I will aggregate the poor and fair health categories, as well as the very good and excellent categories, otherwise, the poor and excellent categories may be unduly inﬂuenced by outlying datapoints.9 I attempted to use multiple nonlinear models, including a gamma generalized linear model (GLM), a gamma GLM with a polynomial term, a random forest model (Breiman, 2001), and an ordered logistic regression. Most failed to model the data adequately, as the ﬁt of the model failed to pass through the more concentrated parts of the dataset. Finally, I settled on a splined model, as well as a nonlinear model that utilizes the Michaelis-Menten equation (MME). The MME was designed to model enzyme reactions and has the following form: Y=Vmax·X kM+X whereVmaxrepresent the maximum ﬁtted Y value (distress in this case) over the entire range of X (mental illness) and kMis loosely interpreted as the rate of increase in distress. (Technically, kMis the point at which Vmaxreaches its halfway point, which increases if it has a steeper slope). To allow the model to generate predictions for each health rating, I modiﬁed the model as follows: Distress =MI·(Vmax+β1Good Health +β2Fair Health ) kM+β3Good Health +β4Fair Health +MI whereβ1/β2indicate how Vmaxdiﬀers for those in good/fair health relative to the “very good” health individuals, and β3/β4indicate deviations in slopes. The model’s parameters were estimated using a Bayesian approach with diﬀuse priors. A Bayesian approach will make it more seamless when I replicate these ﬁndings in Step 8 (because the posterior estimates can serve as priors for the replication). Why did I use the Michaelis-Menten equation? Well, because it ﬁt the data, as shown in Figure 4. The top rows are for the model where I combined fair/poor and very good/excellent, and the bottom plot is for the non-combined data. Both models ﬁt fairly well, except for the spline model under the excellent condition. For that reason, I am going 8To see a YouTube video explaining my thought process during my statistical modeling, visit https: //youtu.be/5BpmktmvgIA. 9I did perform a sensitivity analysis on this decision as well (i.e., performing the analysis both before and after aggregating those categories). Combining the categories makes the picture more clear. Whether that clarity is spurious, I leave it to the reader to decide. Occasionally, I will present the results from both approaches.', 'page_label: 24 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 24 fair good very good 0.1 0.5 0.90.1 0.5 0.90.1 0.5 0.90510152025 MIDistress poor fair good very good excellent 0.10.50.90.10.50.90.10.50.90.10.50.90.10.50.90510152025 MIDistress Figure 4 . Predictions for the spline (blue lines) and Michaelis-Menten equation (MME; red lines) models of the NSDUH data. The top rows combine the poor/fair and very good/excellent conditions, while the bottom plots are for the uncombined analysis. to choose the MME model for the remainder of my analyses. I am also inclined to choose the model that combines fair/poor and very good/excellent; the extreme categories have so few people I don’t trust the predictions. At this point, I am also going to temporarily refrain from interpreting the graphics until I have assessed the viability of the assumptions, which I will do in the following section. 5. Study the Residuals To ensure that the MME model adequately ﬁts the data, I generated residual plots (Figure 5), including a histogram of the residuals, as well as residual dependence and scale location (SL) plots. Again, these are the results for the combined categories (though the uncombined looked similar as well). There are potential deviations from homoskedasticity; the residuals have a slight “megaphone” shape. However, this is likely because the DV is a likert scale, which limits variability at the upper and lower ranges of distress. However, inferences tend to be fairly robust to modest deviations (Maxwell & Delaney, 2004). As such, the model appears to at least approximately meet assumptions.', 'page_label: 25 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 25 0510152025 −10010 residualscount −15−10−5051015 51015 fittedresiduals 051015 51015 fitted| residuals | Figure 5 . Residual plots of the MME analysis: a histogram of the residuals, a residual dependence plot, and an SL plot. Plotted lines are loess lines. 6. Study Parameter Estimates/Eﬀect sizes As mentioned previously, β1andβ2indicate how the maximum predicted value in the good/fair health individuals (respectively) diﬀer from the maximum of those in very good health. The diﬀerence between those in good/very good is 0.18 distress points, with a 95% credible interval ranging from -2.83 to 3.29, while the diﬀerence between those in fair versus very good health is 4.31 distress points, with a 95% credible interval of 1.37 to 7.25. 7. Determine Clinical Signiﬁcance Based on a Decision Criteria As I mentioned previously, this analysis was not a replication of a previous study. As such, there was little statistical information from which I could derive a decision criteria. However, in the next step, replication, I now have empirical information from which to generate a decision criteria. In the mean time, however, I will spend some time summarizing the insights I have gained from my ﬁrst analysis. Both mental illness and distress were skewed and the ﬁt of the model was not linear. Interestingly, as individuals in this sample increased in mental illness, there are very dramatic increases in distress, though only to a point (approximately 0.10). From that point, it did not seem to matter whether an individual had a really high probability of mental illness (e.g., 0.9) or relatively low (e.g., 0.1), their distress level is approximately the same. Additionally, those in fair/poor health do seem to experience more distress than those in very good health, or at least the estimated maximum distress ( Vmax) for the two groups are diﬀerent by about 4.31. In other words, having even moderately good health limits the maximum distress one might experience. On the other hand, increasing', 'page_label: 26 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 26 one’s health from good to very good/excellent doesn’t seem to make much of a diﬀerence. Revisiting my original hypothesis, I sought to estimate the eﬀect of health after controlling for mental illness. However, I’m not entirely sure it makes sense to control for mental illness; the relationship between mental illness and distress is both complicated and nuanced and it doesn’t really make sense to me to remove its eﬀect from my interpretation. Put diﬀerently, the eﬀect of health on distress depends highlyon one’s mental illness, and the eﬀect of health can only be interpreted in that context. 8. Repeat With a New Dataset Fortunately, the NSDUH routinely reports results of their survey of drug and alcohol use every year. As such, I decided to do a strictly conﬁrmatory test of my model on the 2018 survey. To do so, I used the posterior distribution from the previous MME model as the prior for the Bayesian model. Aside from inputting the priors, I used identical syntax to run the analysis. For my decision criteria, I chose the lower limit of my 95% credible interval for β2, 1.37, as my decision-criteria. In other words, if the diﬀerence between those who self-report as fair versus very good is more than 1.37 points, I consider that diﬀerence practically signiﬁcant. Figure 6 shows the results of the replication. As before, each health category is displayed as a separate panel and the ﬁts of the MME model are shown as red lines. The black lines are “ghost lines” (Fife, 2019b), which simply repeat the pattern from one panel (“good” in this case) across the other panels to make it easier to compare ﬁts across panels. For the top graphic, the results are remarkably similar to those in Figure 4. Also, the average diﬀerence in distress between those of fair versus very good health is 5.42, which is well above the decision criteria. Further, the 95% credible interval has narrowed from the initial study (1.37, 3.29) to the replication (4, 6.87). Fortunately, the uncombined results also suggest that improved health mitigates distress. Both the poor and fair conditions max out at much higher levels of distress than those in good/very good/excellent distress. Although the excellent condition does not follow the same pattern, likely because there is so much noise. Results Section Earlier I stated that, at a minimum, a researcher should report (1) a graphical depiction of the analysis, (2) a comment on the appropriateness of statistical assumptions, (3) parameter/eﬀect size estimates with conﬁdence intervals, and (4) a supplemental section or link where the reader can view all graphics/sensitivity analyses. For this example, the results section may read as follows: Upon visual inspection of the residuals, it was determined that linear models were not appropriate. This discovery forced a change from conﬁrmatory to exploratory analysis. Consequently, we utilized a nonlinear model, using the Michaelis-Menten equation*:', 'page_label: 27 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 27 fair good very good 0.000.250.500.75 0.000.250.500.75 0.000.250.500.750510152025 MIDistress poor fair good very good excellent 0.000.250.500.750.000.250.500.750.000.250.500.750.000.250.500.750.000.250.500.750510152025 MIDistress Figure 6 . Results of a strict conﬁrmatory replication of the MME model, which models the relationship between mental illness/health and distress. The top plots show the combined results while the bottom plots show the uncombined results. The posterior of the parameter estimates from the original study served as priors in a Bayesian analysis. This graphic shows the ﬁt. Black lines are ghost lines (Fife, 2019b), which repeat the pattern from the good health condition to the other conditions. distress =MI·(Vmax+β1Good Health +β2Fair Health ) kM+β3Good Health +β4Fair Health +MI We also aggregated the poor/fair, as well as the excellent/very good groups because data were quite sparse at the extremes. Sensitivity analyses revealed that the ﬁnal conclusions were relatively insensitive to whether the data were combined or not. Further, we utilized a Bayesian analysis with diﬀuse priors. This model was ﬁt to the 2014 NSDUH dataset, the hypotheses preregistered, then replicated on the 2018 dataset, but using the posterior of the original analysis as the priors in the replication. The results of the replication are presented in Figure 6. The predicted diﬀerence between those with fair versus very good health was 5.42, with a 95% credible interval of 4 to 6.87. More details (such as graphics of distributions and sensitivity analyses) can be viewed at', 'page_label: 28 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 28 http://www.examplesite.com/article * We also analyzed the data using a various other models, including a spline model. These other models ﬁt the data poorly. For full details and source code, see the supplemental section. Summary My ﬁrst naive NHST analysis revealed a signiﬁcant eﬀect of health on psychological distress among heroin users, with a respectable eﬀect size. However, by following the eight steps of data analysis we have learned many things we otherwise would have missed, such as: •Mental illness is highly skewed, and distress is moderately skewed •The relationship between distress and mental illness is highly curvilinear; as the probability of mental illness increases, one’s distress increases rapidly, then levels oﬀ with higher levels of mental illness. •Those with a 10% probability of mental illness have about equivalent distress as those with a probability of 90% or higher. •Regardless of one’s health, the predicted rate at which distress increases is the same, though the predicted maximum distress is diﬀerent. •Relative to those of poor/fair health, having good health (or very good) reduces the maximal distress someone might experience by approximately 5.42 points. •Improving one’s health from good to very good makes little diﬀerence in distress. It is important to note that the issues noted above led to substantially deeper insights about the data which were entirely missed by the standard approach. Discussion The recent “replication crisis” suggests there are statistical practices within the ﬁeld of psychology that inﬂate false positives and negatives. These practices include “ p-hacking,” failing to meet statistical assumptions, and a narrow focus on statistical signiﬁcance rather than interpreting what the data are actually telling us. In this paper, I have suggested a framework under which researchers might perform data analysis that easily ﬁts within current data analysis practices, while inviting a greater focus on estimation and data visualization. In addition, this framework provides step-by-step guidance for researchers that aims to empower analysts to focus on what the data are actually saying. I have also highlighted these principals and practices with the use of an actual dataset. My analysis revealed that performing the NHST ritual, even when eﬀect sizes were reported, yielded a misleading and incomplete picture. My re-investigation of the same hypothesis revealed patterns more nuanced than a single NHST p-value (or eﬀect size) captured. It is my hope the example I provided was illustrative. I suspect most researchers do not have the interest to utilize advanced nonlinear models. Allow me to oﬀer some hope. First, I intentionally chose a dataset I knew would severely violate the assumption of linearity. I would hope most researchers would not encounter such “zero-inﬂated” models where', 'page_label: 29 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 29 non-linear relationships are almost inevitable. On other hand, some constructs psychologists investigate (e.g., frequency of rape occurrences, number of times one has attempted suicide) should not be investigated with standard linear models. Those researchers who study these types of data perhaps ought to learn more sophisticated models to best interpret what their data are saying. However, it may be optimistic to think researchers will learn these complex modeling techniques themselves. In these situations, it might be best to collaborate with those who are familiar with these techniques. On the other hand, visuals are both intuitive to interpret and easy to produce. As such, these ought to alwaysbe employed. Finally, despite my best eﬀorts to emphasize this framework easily ﬁts within current statistical practices, I suspect there may be some resistance. For example, editors may lament that performing these eight steps will double the length of the average article. I agree, though this is less of a concern as journals become more digital. However, I do not think it necessary every plot and sensitivity analysis make it to the ﬁnal version of the paper. Researchers already frequently omit details about data cleanup and how models were decided. However, I do strongly suggest this information be publicly available, either through supplemental material or through the author’s website. Doing so will allow future consumers of the research to understand what decisions were made, why they were made, and how these decisions may (or may not) have aﬀected the analysis. In addition, it provides additional tools to consumers that allows them to judge the verisimilitude of the research themselves. Another obstacle to incorporating these suggestions may be a lack of training. Many researchers may not know how to graph loess lines, jitter categorical variables, or create paneled plots. Because of this, I have created a step-by-step tutorial that shows researchers how to visually represent the most common analyses, including regression, multiple regression, factorial ANOVAs, and t-tests. This tutorial demonstrates how to perform these in both the point-and-click software Jamovi. This tutorial can be found at http://rpubs.com/ dustinﬁfe/528244. Additional resources can be found at https://yt.vu/p/PL8F480DgtpW_ v1fmBauNMPF9Gqdoaa8zJ. I also invite ﬂexibility among reviewers and editors. I recognize the approach I introduce is diﬀerent than what is traditionally taught in textbooks and what is traditionally performed in applied settings. While traditional statistics is taught as a mechanical sequence that yields unambiguous answers to research questions, my approach encourages ﬂexibility in reporting results. The natural reaction among editors and reviewers might be to reject anything unfamiliar. However, as illustrated in the example, this would be unwise. This approach to data analysis invites ambiguity and forces analysts, reviewers, and editors to confront the uncertainty inherent in data analysis. This is a very good thing. Under such ﬂexibility, I hope editors and reviewers might be open to very diﬀerent approaches to analyzing data and interpreting results. These might include: •Analysis sections that do not report p-values.Analystsutilizingtheeightstepsapproach may feel that a p-value is misleading and/or inadequate at summarizing one’s results. Rather, they may choose to determine statistical signiﬁcance using other values (e.g., Bayes factors, mean diﬀerences, slopes).', 'page_label: 30 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 30 •Reports that are purely exploratory. As users practice this approach to data analysis, they will quickly learn the richness intrinsic in data and recognize a largely untapped resource. Indeed, this is what I hopewill happen. This will inevitably lead to an inﬂux in EDA. While some have suggested allpublished analyses ought to be conﬁrmatory (Lindsay, 2015), I feel otherwise. Granted, EDA oﬀers weaker evidence than CDA, and all EDA ought to be followed by CDA. However, if the choice is between failing to publish EDA because one does not have the resources to CDA and publishing only the EDA, I choose the later. Published EDA results can then be replicated by someone who does have the resources. •Substantive conclusions that are based on non-traditional criteria (e.g., Bayes fac- tors, slopes, reaction time diﬀerences, correlation coeﬃcients, AICs). The ﬁrst step encourages researchers to set their own criteria for what is deemed “signiﬁcant” or important. The metrics of choice will vary from discipline to discipline and editors and reviewers ought to evaluate the choice of criteria in light of the substantive questions being asked. •Substantive conclusions that are based on graphical interpretation alone. This approach to data analysis recognizes the critical role graphics play in encoding important information, evaluating model ﬁt, conveying uncertainty, etc. If researchers utilize this approach, I hopethey will use graphical depictions to aid decisions of scientiﬁc relevance. If they do, they ought not to be penalized simply because it is diﬀerent. •Results expositions that seem disorganized. If these eight steps are followed, analyses will rarely be linear; rather, analysts may have to stumble through various modeling strategies quite iteratively as they search for the best representation of the data. Although some might be inclined to relegate the stops and starts to a supplemental section, others might feel ethically inclined to convey how they arrived at their conclusions in a way that reﬂects the messiness inherent in their analysis. Granted, the exposition should be clear, but I invite editors and reviewers to be forgiving of interpretations that take some time to unfold. •Conclusions that are hedged with uncertainty and/or ambiguity. Whilep-values invite a false sense of certainty, the approach I advocate often invites uncertainty. This is a good thing. In the past, authors writing eye-catching titles and conclusions have damaged the ﬁeld’s reputation and perpetuated fallacious beliefs in the media. Cautious language, though less ﬂashy than bold claims, is probably a better way of conveying results. •Extremely lengthy supplemental sections located on outside repositories. Once again, this eight step approach is all but guaranteed to increase the number of starts and stumbles during the data analysis process. In line with the second guiding principle of data analysis (utilizing sensitivity analyses for ad-hoc decisions), these starts and stumbles should be communicated to the audience. This may require a rather long supplemental section and editors and reviewers should be accepting about these sorts of submissions. •Detailed analyses reported via blogs, YouTube videos, podcasts, etc. Sometimes the', 'page_label: 31 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 31 optimal way to convey statistical results may not be in journal format, or even a traditional supplemental section. Writing for journals often invokes a need for precision and clarity, and writers may anticipate they’ll have to spend a great deal of time preemptively defending every statement and decision. This need for clarity and precision is a good thing, but may be a handicap during the more ambiguous stages of data analysis. Additionally, some results are best interpreted interactively (e.g., through rotating three dimensional plots or multiple static plots), while others may be best understood during stream-of-consciousness narration. Other avenues may be more amenable to expounding on the process whereby the researcher arrived at their conclusions, and editors and reviewers may need to be ﬂexible in allowing this. In this paper, for example, I uploaded a screencast on my YouTube channel that more fully explains my thought-process (see https://youtu.be/5BpmktmvgIA), which allowed me to give more detailed information than I might have if I were required to write about it. •Alternative modeling strategies (random forest, SVM, Bayesian methods). The eight steps inevitably will shift an analyst’s perspective away from testing hypotheses and toward building a statistical model that adequately represents the data. Sometimes the best model to use will not be a t-test, ANOVA, regression, etc. I anticipate that as more people adopt this approach, diverse modeling approaches will become more common. •Become comfortable with subjectivity . As researchers begin to make personalized decision criteria, they may choose to set their decision criteria based more on their subjective judgment of a meaningful eﬀect. For example, a researcher might decide that a smoking cessation program is eﬀective if greater than 20% of participants quit without remission. In this situation, it may be tempting to reject an author’s decision criteria for being arbitrary (why not 30%?). However, there is nothing inherently wrong with a subjective decision criteria. A threshold of 0.05 is an arbitrary threshold forp-values. A researcher’s decision criteria should be judged not on how objective it is, but by its stringency. In summary, my recommendation for editors and reviewers is to never reject an article simply because it utilizes a non-standard approach for arriving at substantive conclusions. Rather, the approach should be evaluated in terms of how well it ﬁts the needs of the situation and whether the assumptions of the model are reasonably met. If the model is appropriate and reasonable, there’s no reason an author’s attempt at ingenuity should count against them. In conclusion, the discipline of psychology is at a crossroads. We can continue to participate in NHST-based psychology and the problems we have recently encountered will persist. Or we can revolutionize the way we think about analysis, listen to the messages the data are trying to tell us, and uncover truths previously buried behind ANOVA summary tables andp-values.', 'page_label: 32 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 32 References Baguley, T. (2009). Standardized or simple eﬀect size: What should be reported? British Journal of Psychology ,100(3), 603–617. doi:10.1348/000712608X377117 Berkson, J. (1942). Tests of signiﬁcance considered as evidence. Journal of the American Statistical Association ,37(219), 325–335. doi:10.1080/01621459.1942.10501760 Bond, C. F., Wiitala, W. L., & Richard, F. D. (2003). Meta-analysis of raw mean diﬀerences. Psychological Methods ,8(4), 406–18. doi:10.1037/1082-989X.8.4.406 Bonneau, G. P., Hege, H. C., Johnson, C. R., Oliveira, M. M., Potter, K., Rheingans, P., & Schultz, T. (2014). Overview and state-of-the-art of uncertainty visualization. Mathematics and Visualization ,37, 3–27. doi:10.1007/978-1-4471-6497-5_1 Breiman, L. (2001). Random forests. Machine Learning ,45(1), 5–32. doi:10.1023/A:1010933404324 Chawla, D. S. (2016, October). Oh, well - \"love hormone\" doesn’t reduce psychiatric symptoms, say researchers in request to re- tract. Retrieved from http://retractionwatch.com/2016/10/04/ oh-well-love-hormone-doesnt-reduce-psychiatric-symptoms-says-researchers-in-request-to-retract/ Cohen, J. (1992). A power primer. Psychological Bulletin ,112(1), 155–159. doi:http://dx.doi.org/10.1037/0033-2909.112.1.155 Cohen, J. (1994). The earth is round (p < .05). American Psychologist ,49(12), 997–1003. doi:10.1037/0003-066X.49.12.997 Cook, T. D., & Campbell, D. T. (1976). The design and conduct of quasi-experiments and true experiments in ﬁeld settings. Research in Organizations: Issues and Controversies , 223–326. Cortina, J. M., & Landis, R. S. (2011). The earth is not round (p = .00). Organizational Research Methods ,14(2), 332–349. doi:10.1177/1094428110391542 Counsell, A., & Harlow, L. L. (2017). Reporting practices and use of quantitative meth- ods in canadian journal articles in psychology. Canadian Psychology/Psychologie Canadienne ,58(2), 140–147. doi:10.1037/cap0000074 Coyne, J. C. (2016). Replication initiatives will not salvage the trustworthiness of psychology. BMC Psychology . doi:10.1186/s40359-016-0134-3 Cumming, G. (2014). The new statistics: Why and how. Psychological Science ,25(1), 7–29. doi:10.1177/0956797613504966 Cumming, G., Fidler, F., Leonard, M., Kalinowski, P., Christiansen, A., Kleinig, A., ... Wilson, S. (2007). Statistical reform in psychology is anything changing ? Psychological Science ,18(3), 1–4. doi:10.1111/j.1467-9280.2007.01881.x', 'page_label: 33 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 33 Cumming, G., Fidler, F., & Thomason, N. (2001). The statistical re-education of psychology. InThe 6th international conference on teaching statistics. Hawthorn, Victorial: Swinburne Press. Edwards, J. R., & Berry, J. W. (2010). The presence of something or the absence of nothing: Increasing theoretical precision in management research. Organizational Research Methods,13(4), 668–689. doi:10.1177/1094428110380467 Erceg-Hurn, D. M., & Mirosevich, V. M. (2008). Modern robust statistical methods: An easy way to maximize the accuracy and power of your research. The American Psychologist ,63(7), 591–601. doi:10.1037/0003-066X.63.7.591 Fife, D. A. (2019a). A graphic is worth a thousand test statistics: Mapping visuals onto common analyses. Retrieved from http://rpubs.com/dustinﬁfe/528244 Fife, D. A. (2019b). Flexplot: Graphical-based data analysis [r and jamovi]. Available at www.Jamovi.com; www.github.com/dustinﬁfe/ﬂexplot. doi:10.31234/osf.io/kh9c3 Fife, D. A., & Rodgers, J. L. (2019). Exonerating eda: Addressing the replication crisis by expanding the eda/cda continuum. Unpublished Manuscript . Retrieved from http://quantpsych.net/ﬁfe-exonerating-eda-draft-oct2019-df-edits/ Furr, R. M. (2014). Scale construction and psychometrics for social and personality psychol- ogy. Thousand Oaks, CA: SAGE. doi:10.4135/9781446287866 Harlow, L. L., Mulaik, S. A., & Steiger, J. H. (2016). What if there were no signiﬁcance tests?(2nd ed.). New York, NY: Routledge. Healy, K., & Moody, J. (2014). Data visualization in sociology. Annual Review of Sociology , 40(1), 105–128. doi:10.1146/ANNUREV-SOC-071312-145551 Hoekstra, R., Kiers, H., & Johnson, A. (2012). Are assumptions of well-known sta- tistical techniques checked, and why not? Frontiers in Psychology ,3(137). doi:10.3389/fpsyg.2012.00137 Hofmann, S. G., Fang, A., & Brager, D. N. (2015). Eﬀect of intranasal oxytocin admin- istration on psychiatric symptoms: A meta-analysis of placebo-controlled studies. Psychiatry Research ,228(3), 708. doi:10.1016/j.psychres.2015.05.039 JASP Team. (2019). JASP (version 0.10.2)[Computer software]. Retrieved from https: //jasp-stats.org/ Jones, L. V. (1952). Test of hypotheses: One-sided vs. Two-sided alternatives. Psychological Bulletin,49(1), 43. Kessler, R. C., Barker, P. R., Colpe, L. J., Epstein, J. F., Gfroerer, J. C., Hiripi, E., ... Zaslavsky, A. M. (2003). Screening for serious mental illness in the general population. Archives of General Psychiatry ,60(2), 184–189. doi:10.1001/archpsyc.60.2.184 Kruschke, J. K., & Liddell, T. M. (2018). The bayesian new statistics: Hypothesis test- ing, estimation, meta-analysis, and power analysis from a bayesian perspective. Psychonomic Bulletin and Review ,25(1). doi:10.3758/s13423-016-1221-4', 'page_label: 34 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 34 Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2004). Applied linear statistical models. In Applied linear statistical models . New York, NY: McGraw-Hill/Irwin. Levine, S. S. (2018). Show us your data: Connect the dots, improve science. Management and Organization Review ,14(2), 433–437. doi:10.1017/mor.2018.19 Lindsay, D. S. (2015). Replication in psychological science. Psychological Science ,26(12), 1827–1832. doi:10.1177/0956797615616374 Loken, E., & Gelman, A. (2017). Measurement error and the replication crisis. Science, 355(6325). doi:10.1126/science.aal3618 Maxwell, S. E., & Delaney, H. D. (2004). Designing experiments and analyzing data: A model comparison perspective . New York, NY: Taylor & Francis. Micceri, T. (1989). The unicorn, the normal curve, and other improbable creatures. Psycho- logical Bulletin ,105(1), 156–166. Retrieved from https://pdfs.semanticscholar.org/ 2903/180261ee0d99a27cfe85cde9cf4af74923c6.pdf Morey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The fallacy of placing conﬁdence in conﬁdence intervals. Psychonomic Bulletin and Review,23(1). doi:10.3758/s13423-015-0947-8 National Survey on Drug Use and Health. (2014). National survey on drug use and health 2014 . Substance Abuse; Mental Health Services Administration, Center for Behavioral Health Statistics; Quality. Retrieved from https://www.dataﬁles.samhsa. gov/study/national-survey-drug-use-and-health-nsduh-2014-nid13618 Nelson, L. D., Simmons, J. P., & Simonsohn, U. (2018). Psychology’s renaissance. Annual Review of Psychology ,69, 511–545. doi:10.1146/annurev-psych-122216 Osborne, J. W. (2013). Is data cleaning and the testing of assumptions relevant in the 21st century? Frontiers in Psychology ,4. doi:10.3389/fpsyg.2013.00370 Pashler, H., & Wagenmakers, E.-J. (2012). Editors’ introduction to the special section on replicability in psychological science: A crisis of conﬁdence? Perspectives on Psychological Science ,7(6), 528–530. doi:10.1177/1745691612465253 project, T. jamovi. (2019). Jamovi (version 0.9) [computer software]. Retrieved from https://www.jamovi.org Rodgers, J. L. (2010). The epistemology of mathematical and statistical modeling: A quiet methodological revolution. The American Psychologist ,65(1), 1–12. doi:10.1037/a0018326 Rothman, K. J. (2010). Curbing type i and type ii errors. European Journal of Epidemiology , 25(4), 223–224. doi:10.1007/s10654-010-9437-5 Schmidt, F.L.(1996). Statisticalsigniﬁcancetestingandcumulativeknowledgeinpsychology: Implications for training of researchers. Psychological Methods ,1(2), 115–129. doi:10.1037/1082-989X.1.2.115', 'page_label: 35 file_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf  THE EIGHT STEPS OF DATA ANALYSIS 35 Shadish, W., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference . Boston, MA: Houghton Miﬄin. Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology. Psycho- logical Science ,22(11), 1359–1366. doi:10.1177/0956797611417632 Simonsohn, U. (2014). Posterior-hacking: Selective reporting invalidates bayesian results also.SSRN Electronic Journal . doi:10.2139/ssrn.2374040 Steegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science ,11(5), 702–712. doi:10.1177/1745691616658637 Szollosi, A., Kellen, D., Navarro, D., Shiﬀrin, R., Rooij, I. van, Zandt, T. V., & Donkin, C. (2019). Preregistration is redundant, at best. doi:10.31234/OSF.IO/X36PZ Traﬁmow, D. (2017). Using the coeﬃcient of conﬁdence to make the philosophical switch from a posteriori to a priori inferential statistics. Educational and Psychological Measurement ,77(5), 831–854. doi:10.1177/0013164416667977 Tukey, J. W. (1986). Analyzing data: Sanctiﬁcation or detective work? The Collected Works of John W. Tukey , 721–737. Valentine, J. C., Aloe, A. M., & Lau, T. S. (2015). Life after nhst: How to describe your data without p-ing everywhere. Basic and Applied Social Psychology ,37(5), 260–273. doi:10.1080/01973533.2015.1060240 Wagenmakers, E.-J., Wetzels, R., Borsboom, D., Maas, H. L. J. van der, & Kievit, R. A. (2012). An agenda for purely conﬁrmatory research. Perspectives on Psychological Science,7(6), 632–638. doi:10.1177/1745691612463078 Wicherts, J. M., Veldkamp, C. L. S., Augusteijn, H. E. M., Bakker, M., Aert, R. C. M. van, & Assen, M. A. L. M. van. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in Psychology ,7(NOV). doi:10.3389/fpsyg.2016.01832 Wilkinson, L., & Task Force on Statistical Inference. (1999). Statistical methods in psychology journals: Guidelines and explanations. American Psychologist ,54(8), 594–601.', 'file_path: /home/cuphead/Projects/llama-index/data/maroctelecom.txt  Maroc Telecom Company type\\tPublic Traded as \\tEuronext Paris: IAM Industry\\tTelecommunications Founded\\tFebruary 3, 1998; 26 years ago Headquarters\\tRabat, Morocco Key people \\tAbdeslam Ahizoune, Chairman & CEO Laurent Mairot, CFO Products\\tLandline phones, Mobile phone lines, Fiber-optic Internet, ADSL, 4G+ Revenue\\tIncrease US$ 3,6 billion (2018) Net income \\tIncrease US$ 610 million (2018) Owners\\t      Etisalat by e& (53%)     Moroccan government (22%)  Number of employees \\t10,609 (2018) Website\\thttp://www.iam.ma  Maroc Telecom (Acronym: IAM, Arabic: اتصالات المغرب) is the main telecommunications company in Morocco.[1][2] Currently employing around 11,178 employees, it is the largest telecommunications network in the country with 8 regional delegations and 220 offices present across Morocco. The company is listed on both the Casablanca Stock Exchange and Euronext Paris. History See also: Postal history of Morocco  The origin of a Moroccan telecommunications project dates back to 1891, when Sultan Hassan I created the first Moroccan postal service. In 1913, the Moroccan Postal Telephone and Telegraph was established before a Dahir (King\\'s decree) related to the monopoly of the state of Telegraphy and Telephony was published.  In 1967, Morocco placed the first underwater cable between Tetouan, Morocco, and Perpignan, France, through the Mediterranean. A few years later, in 1970, a transmission via INTELSAT was introduced. The Telex service was then automated in 1971 just before installing a digital center in Fes.  Due to the advancement of telecommunications around the globe, Morocco decided to create a new entity called the Office National des Postes et Télécommunications (ONPT) to manage the industry. ONPT was responsible of the introduction of Analog Mobile Radiotelephony in 1987. Later on, in 1992, Morocco set up the first underwater optical fiber cable. Two years later, a GSM service was operational. The Internet was introduced in Morocco by ONPT in 1995.  After the publication of a telecommunications\\' decree, Maroc Telecom (IAM) was eventually founded in 1998. The acronym IAM comes from its original Arabic name Ittisalat Al Maghrib.[3] The name \"Maroc Telecom\" was adopted later for better international recognizability. Privatization  On 20 February 2001, the Moroccan government sold 35% of Maroc Telecom\\'s shares to French mass media company Vivendi. The transaction amounted to 23 Billion dirhams.[4] On 4 January 2005 Vivendi acquired an additional 16% for 12.4 billion dirhams raising its participation to 51%.[5] In October 2007, the CDG ceded, via its subsidiary Filpar Holding, 2% of Maroc Telecom to Vivendi in exchange of 0.6% of Vivendi\\'s shares, putting the total shares owned by Vivendi to 53%.[6]  In 2006, the company reported a turnover of $2.67 bn. The custom base was established at 1.27m lines for the landline and at 391,000 lines for the ADSL.[7]  In July 2013, it was announced that the firm’s majority owner, Vivendi, would sell its 53% stake in the firm to Etisalat for around $4.2 billion.[8]  In 2016, Maroc Telecom introduced fiber optics to the country with speeds up to 200 Mbits/s. Activities Land lines  It consists of the provision of public phones throughout Morocco. The fixed park reaches 1.6 million lines. Mobile phodocuments in the data folder (which in this case just consists of the essay text, but could contain many documents).nes  Mobile services are provided via a GSM network. Maroc Telecom counted 33 million customers at the end of October 2012. Its network covers 97% of the Moroccan population. It also has 12.5 million customers in Mali, Gabon, Burkina Faso and Mauritania. It is one of the most profitable phone operators in Africa with a revenue of 2.2 billion euros during the first 9 months of 2012.[9]  Maroc Telecom launched 4G+ in Morocco on July 13, 2015.', 'file_path: /home/cuphead/Projects/llama-index/data/maroctelecom.txt  Activities Land lines  It consists of the provision of public phones throughout Morocco. The fixed park reaches 1.6 million lines. Mobile phodocuments in the data folder (which in this case just consists of the essay text, but could contain many documents).nes  Mobile services are provided via a GSM network. Maroc Telecom counted 33 million customers at the end of October 2012. Its network covers 97% of the Moroccan population. It also has 12.5 million customers in Mali, Gabon, Burkina Faso and Mauritania. It is one of the most profitable phone operators in Africa with a revenue of 2.2 billion euros during the first 9 months of 2012.[9]  Maroc Telecom launched 4G+ in Morocco on July 13, 2015.[10] Customer support  Customer support does only propose contacting by phone to a special number and not via a contact form like most of telecommunication companies.[11] Projects and investments  On June 1, 2006, IAM launched the IPTV package deployed by Huawei Technology[12] via the ADSL line. The service was the first of its kind in Africa and the Middle East.  In July 2006, Maroc Telecom signed with the French telecommunications equipment company Alcatel a contract for a submarine communications cable connection between Morocco and France. Maroc Telecom\\'s aim is to upgrade the capacity of its services (i.e. broadband services, call centers). The project cost €26 million and was named \"Atlas Offshore\".[13]  In December 2006, IAM invested in Burkina Faso’s ONATEL, acquiring 51% of its capital.[14]', \"file_path: /home/cuphead/Projects/llama-index/data/story.txt  Once upon a time, in a town nestled between two mountains that whispered secrets to the wind, there lived a peculiar man named Ephraim. Ephraim was known throughout the village for his uncanny ability to communicate with animals—not just the usual dogs and cats, but birds, insects, and even the occasional elusive fox.  One fateful morning, when the sun rose in hues of apricot and lavender, Ephraim set off on his daily trek into the forest. He was in search of an ancient oak rumored to hold the key to understanding the language of the forest creatures. Legends whispered that whoever could decipher their tongue would gain unimaginable wisdom and power.  As Ephraim ventured deeper into the woods, the trees seemed to murmur with anticipation. Squirrels chattered excitedly overhead, and deer watched him with curious eyes. It was as if the whole forest knew of his quest.   Hours turned into days as Ephraim navigated through thick underbrush and crossed babbling brooks. He survived on berries and nuts, sharing his meals with the woodland critters who guided him along hidden paths.  Finally, on the seventh day of his journey, Ephraim stumbled upon the ancient oak. Its gnarled branches reached toward the heavens like a thousand crooked fingers, and its trunk was marked with symbols carved by creatures long forgotten. With trembling hands, Ephraim touched the rough bark, feeling the pulse of ancient magic beneath his fingertips.  Suddenly, a voice whispered in his ear—not with words, but with images and emotions. It was the language of the forest, a symphony of nature's secrets woven into the fabric of existence. Ephraim closed his eyes and listened, his heart pounding with wonder.  From that day forward, Ephraim became the bridge between humanity and the natural world. He helped settle disputes between rival wolf packs, taught young owls the art of silent flight, and even brokered peace between the mischievous sprites of the glade and the stern guardians of the river.  But Ephraim's newfound gift came with a price. The more he understood the language of the forest, the less he could communicate with his fellow villagers. They whispered that he had gone mad, talking to trees and laughing with the wind. Some feared him, while others revered him as a sage touched by the gods.  As the years passed, Ephraim grew old and frail, his body bent like the ancient oak that had revealed the secrets of the forest to him. On his last day, surrounded by friends both human and animal, Ephraim whispered his final words to the wind—a promise to protect the delicate balance between the realms he had come to love.  And so, when Ephraim passed from this world, the animals of the forest gathered to mourn him. Birds sang melancholy melodies, and flowers bloomed in hues of mourning purple. But in the hearts of those who remembered him, Ephraim lived on as a legend—a strange and wondrous tale of a man who spoke the language of the wild.\"], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fc5ef520>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fc5ef520>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f20fc5734c0> server_hostname='api.openai.com' timeout=60.0\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x7f20fc5734c0> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fc5ef280>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fc5ef280>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 26 Jul 2024 09:35:36 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-vgkhxp5wmyi2hwi8lvfpt8cu'), (b'openai-processing-ms', b'177'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'971859'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'1.688s'), (b'x-request-id', b'req_bf6dadf73401270fce940af8c7d91576'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=rL6ioCQgedmW6AIanYf7uviHnTDCVMAml.h54dDR_eg-1721986536-1.0.1.1-F_qBhgqM1na8GvhmhvBzPTQ.sHFr8F5QphQKXLNoCNIgYRek1LswKWOhEvVHakx4HMQUfy5WJJmNE4ELzXkZwQ; path=/; expires=Fri, 26-Jul-24 10:05:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=JQnXsd8witKKAySn0QTs0rpmrW3PVWoB_xa2M566Xvg-1721986536097-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a9366857f3603ee-LIS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 26 Jul 2024 09:35:36 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-vgkhxp5wmyi2hwi8lvfpt8cu'), (b'openai-processing-ms', b'177'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'971859'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'1.688s'), (b'x-request-id', b'req_bf6dadf73401270fce940af8c7d91576'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=rL6ioCQgedmW6AIanYf7uviHnTDCVMAml.h54dDR_eg-1721986536-1.0.1.1-F_qBhgqM1na8GvhmhvBzPTQ.sHFr8F5QphQKXLNoCNIgYRek1LswKWOhEvVHakx4HMQUfy5WJJmNE4ELzXkZwQ; path=/; expires=Fri, 26-Jul-24 10:05:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=JQnXsd8witKKAySn0QTs0rpmrW3PVWoB_xa2M566Xvg-1721986536097-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a9366857f3603ee-LIS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Fri, 26 Jul 2024 09:35:36 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('openai-model', 'text-embedding-ada-002'), ('openai-organization', 'user-vgkhxp5wmyi2hwi8lvfpt8cu'), ('openai-processing-ms', '177'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=15552000; includeSubDomains; preload'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '971859'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '1.688s'), ('x-request-id', 'req_bf6dadf73401270fce940af8c7d91576'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=rL6ioCQgedmW6AIanYf7uviHnTDCVMAml.h54dDR_eg-1721986536-1.0.1.1-F_qBhgqM1na8GvhmhvBzPTQ.sHFr8F5QphQKXLNoCNIgYRek1LswKWOhEvVHakx4HMQUfy5WJJmNE4ELzXkZwQ; path=/; expires=Fri, 26-Jul-24 10:05:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=JQnXsd8witKKAySn0QTs0rpmrW3PVWoB_xa2M566Xvg-1721986536097-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8a9366857f3603ee-LIS'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Fri, 26 Jul 2024 09:35:36 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('openai-model', 'text-embedding-ada-002'), ('openai-organization', 'user-vgkhxp5wmyi2hwi8lvfpt8cu'), ('openai-processing-ms', '177'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=15552000; includeSubDomains; preload'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '971859'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '1.688s'), ('x-request-id', 'req_bf6dadf73401270fce940af8c7d91576'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=rL6ioCQgedmW6AIanYf7uviHnTDCVMAml.h54dDR_eg-1721986536-1.0.1.1-F_qBhgqM1na8GvhmhvBzPTQ.sHFr8F5QphQKXLNoCNIgYRek1LswKWOhEvVHakx4HMQUfy5WJJmNE4ELzXkZwQ; path=/; expires=Fri, 26-Jul-24 10:05:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=JQnXsd8witKKAySn0QTs0rpmrW3PVWoB_xa2M566Xvg-1721986536097-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8a9366857f3603ee-LIS'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_bf6dadf73401270fce940af8c7d91576\n",
      "request_id: req_bf6dadf73401270fce940af8c7d91576\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f20fc295c60>, 'json_data': {'input': ['summary of the Eight Steps of Data Analysis: A Graphical Framework to promote Sound Statistical Analysis, Dustin Fife'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f20fc295c60>, 'json_data': {'input': ['summary of the Eight Steps of Data Analysis: A Graphical Framework to promote Sound Statistical Analysis, Dustin Fife'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:httpcore.connection:close.started\n",
      "close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fab22500>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fab22500>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f20fc5734c0> server_hostname='api.openai.com' timeout=60.0\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x7f20fc5734c0> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fab3fa90>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fab3fa90>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 26 Jul 2024 09:45:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-vgkhxp5wmyi2hwi8lvfpt8cu'), (b'openai-processing-ms', b'26'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999971'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'req_6ad674ce4c6d962ce66cb4ec580ad5b9'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=D2Zk_TXYzX1rTxKYzMNv9pLwGruJWQB.TjpBAxWjzyY-1721987107-1.0.1.1-UXm0uUFZo_5FDFhq9Yueroi_LMbgvrtxddbST_UNKQBfVwjoVbNHjmiIrdMP6YQaHeLh8bn8xnbX4Ctb9FdxJg; path=/; expires=Fri, 26-Jul-24 10:15:07 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=FOG.6X_Nv05q_qdvPZCsu3gJgRdda75zqLPEkmCRPM8-1721987107646-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a93747d7aaa0d75-MRS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 26 Jul 2024 09:45:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-vgkhxp5wmyi2hwi8lvfpt8cu'), (b'openai-processing-ms', b'26'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999971'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'req_6ad674ce4c6d962ce66cb4ec580ad5b9'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=D2Zk_TXYzX1rTxKYzMNv9pLwGruJWQB.TjpBAxWjzyY-1721987107-1.0.1.1-UXm0uUFZo_5FDFhq9Yueroi_LMbgvrtxddbST_UNKQBfVwjoVbNHjmiIrdMP6YQaHeLh8bn8xnbX4Ctb9FdxJg; path=/; expires=Fri, 26-Jul-24 10:15:07 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=FOG.6X_Nv05q_qdvPZCsu3gJgRdda75zqLPEkmCRPM8-1721987107646-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a93747d7aaa0d75-MRS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Fri, 26 Jul 2024 09:45:07 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('openai-model', 'text-embedding-ada-002'), ('openai-organization', 'user-vgkhxp5wmyi2hwi8lvfpt8cu'), ('openai-processing-ms', '26'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=15552000; includeSubDomains; preload'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999971'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '1ms'), ('x-request-id', 'req_6ad674ce4c6d962ce66cb4ec580ad5b9'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=D2Zk_TXYzX1rTxKYzMNv9pLwGruJWQB.TjpBAxWjzyY-1721987107-1.0.1.1-UXm0uUFZo_5FDFhq9Yueroi_LMbgvrtxddbST_UNKQBfVwjoVbNHjmiIrdMP6YQaHeLh8bn8xnbX4Ctb9FdxJg; path=/; expires=Fri, 26-Jul-24 10:15:07 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=FOG.6X_Nv05q_qdvPZCsu3gJgRdda75zqLPEkmCRPM8-1721987107646-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8a93747d7aaa0d75-MRS'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Fri, 26 Jul 2024 09:45:07 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('openai-model', 'text-embedding-ada-002'), ('openai-organization', 'user-vgkhxp5wmyi2hwi8lvfpt8cu'), ('openai-processing-ms', '26'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=15552000; includeSubDomains; preload'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999971'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '1ms'), ('x-request-id', 'req_6ad674ce4c6d962ce66cb4ec580ad5b9'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=D2Zk_TXYzX1rTxKYzMNv9pLwGruJWQB.TjpBAxWjzyY-1721987107-1.0.1.1-UXm0uUFZo_5FDFhq9Yueroi_LMbgvrtxddbST_UNKQBfVwjoVbNHjmiIrdMP6YQaHeLh8bn8xnbX4Ctb9FdxJg; path=/; expires=Fri, 26-Jul-24 10:15:07 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=FOG.6X_Nv05q_qdvPZCsu3gJgRdda75zqLPEkmCRPM8-1721987107646-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8a93747d7aaa0d75-MRS'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_6ad674ce4c6d962ce66cb4ec580ad5b9\n",
      "request_id: req_6ad674ce4c6d962ce66cb4ec580ad5b9\n",
      "DEBUG:llama_index.core.indices.utils:> Top 2 nodes:\n",
      "> [Node cd5325ec-2808-4807-8b38-7f12fe9694ec] [Similarity score:             0.930842] The Eight Steps of Data Analysis: A Graphical Framework to\n",
      "Promote Sound Statistical Analysis\n",
      "Dus...\n",
      "> [Node 73c217e5-f602-495f-9c5e-7796ecdf20d0] [Similarity score:             0.875691] THE EIGHT STEPS OF DATA ANALYSIS 33\n",
      "Cumming, G., Fidler, F., & Thomason, N. (2001). The statistic...\n",
      "> Top 2 nodes:\n",
      "> [Node cd5325ec-2808-4807-8b38-7f12fe9694ec] [Similarity score:             0.930842] The Eight Steps of Data Analysis: A Graphical Framework to\n",
      "Promote Sound Statistical Analysis\n",
      "Dus...\n",
      "> [Node 73c217e5-f602-495f-9c5e-7796ecdf20d0] [Similarity score:             0.875691] THE EIGHT STEPS OF DATA ANALYSIS 33\n",
      "Cumming, G., Fidler, F., & Thomason, N. (2001). The statistic...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\npage_label: 1\\nfile_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf\\n\\nThe Eight Steps of Data Analysis: A Graphical Framework to\\nPromote Sound Statistical Analysis\\nDustin Fife1\\n1Rowan University\\nAbstract\\nData analysis is a risky endeavor, particularly among those unaware of its\\ndangers. In the words of Cook and Campbell (1976; see also Shadish, Cook,\\n& Campbell, 2002), “Statistical Conclusions Validity” threatens all research\\nsubjected to the dark arts of statistical magic. Although traditional statistics\\nclasses may advise against certain practices (e.g., multiple comparisons,\\nsmall sample sizes, violating normality), they may fail to cover others (e.g.,\\noutlier detection and violating linearity). More common, perhaps, is that\\nresearchers may fail to remember them. In this paper, rather than rehashing\\nold warnings and diatribes against this practice or that, I instead advocate a\\ngeneral statistical analysis strategy. This graphically-based eight step strategy\\npromises to resolve the majority of statistical traps researchers may fall in\\nwithout having to remember large lists of problematic statistical practices.\\nThese steps will assist in preventing both false positives and negatives and\\nyield critical insights about the data that would have otherwise been missed.\\nI conclude with an applied example that shows how the eight steps reveal\\ninteresting insights that would not be detected with standard statistical\\npractices.\\nKeywords: statistical assumptions, NHST, conﬁrmatory data analysis, graph-\\nical data analysis, ﬁshing, p-hacking\\nThe ﬁeld of psychology has been forced to participate in methodological introspection,\\nof sorts. This introspection began late in the 20th century as methodologists vehemently\\nprotested the knee-jerk focus on p-values Null Hypothesis Signiﬁcance Testing (NHST)\\nencourages (Cohen, 1994; Harlow, Mulaik, & Steiger, 2016; Jones, 1952). The American\\nI wish to thank those who assisted in reviewing this manuscript, including Tom Dinzeo, Polly Tremoulet,\\nJeﬀrey Greeson, Yoav Zeevi, Christine Simmons, and Joseph Rodgers, as well as the anonymous reviewers\\nand editor.\\nCorrespondence concerning this article should be addressed to Dustin Fife, 201 Mullica Hill Road\\nGlassboro, NJ 08028. E-mail: ﬁfe.dustin@gmail.com\\n\\npage_label: 33\\nfile_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf\\n\\nTHE EIGHT STEPS OF DATA ANALYSIS 33\\nCumming, G., Fidler, F., & Thomason, N. (2001). The statistical re-education of psychology.\\nInThe 6th international conference on teaching statistics. Hawthorn, Victorial:\\nSwinburne Press.\\nEdwards, J. R., & Berry, J. W. (2010). The presence of something or the absence of nothing:\\nIncreasing theoretical precision in management research. Organizational Research\\nMethods,13(4), 668–689. doi:10.1177/1094428110380467\\nErceg-Hurn, D. M., & Mirosevich, V. M. (2008). Modern robust statistical methods: An\\neasy way to maximize the accuracy and power of your research. The American\\nPsychologist ,63(7), 591–601. doi:10.1037/0003-066X.63.7.591\\nFife, D. A. (2019a). A graphic is worth a thousand test statistics: Mapping visuals onto\\ncommon analyses. Retrieved from http://rpubs.com/dustinﬁfe/528244\\nFife, D. A. (2019b). Flexplot: Graphical-based data analysis [r and jamovi]. Available at\\nwww.Jamovi.com; www.github.com/dustinﬁfe/ﬂexplot. doi:10.31234/osf.io/kh9c3\\nFife, D. A., & Rodgers, J. L. (2019). Exonerating eda: Addressing the replication crisis\\nby expanding the eda/cda continuum. Unpublished Manuscript . Retrieved from\\nhttp://quantpsych.net/ﬁfe-exonerating-eda-draft-oct2019-df-edits/\\nFurr, R. M. (2014). Scale construction and psychometrics for social and personality psychol-\\nogy. Thousand Oaks, CA: SAGE. doi:10.4135/9781446287866\\nHarlow, L. L., Mulaik, S. A., & Steiger, J. H. (2016). What if there were no signiﬁcance\\ntests?(2nd ed.). New York, NY: Routledge.\\nHealy, K., & Moody, J. (2014). Data visualization in sociology. Annual Review of Sociology ,\\n40(1), 105–128. doi:10.1146/ANNUREV-SOC-071312-145551\\nHoekstra, R., Kiers, H., & Johnson, A. (2012). Are assumptions of well-known sta-\\ntistical techniques checked, and why not? Frontiers in Psychology ,3(137).\\ndoi:10.3389/fpsyg.2012.00137\\nHofmann, S. G., Fang, A., & Brager, D. N. (2015). Eﬀect of intranasal oxytocin admin-\\nistration on psychiatric symptoms: A meta-analysis of placebo-controlled studies.\\nPsychiatry Research ,228(3), 708. doi:10.1016/j.psychres.2015.05.039\\nJASP Team. (2019). JASP (version 0.10.2)[Computer software]. Retrieved from https:\\n//jasp-stats.org/\\nJones, L. V. (1952). Test of hypotheses: One-sided vs. Two-sided alternatives. Psychological\\nBulletin,49(1), 43.\\nKessler, R. C., Barker, P. R., Colpe, L. J., Epstein, J. F., Gfroerer, J. C., Hiripi, E., ...\\nZaslavsky, A. M. (2003). Screening for serious mental illness in the general population.\\nArchives of General Psychiatry ,60(2), 184–189. doi:10.1001/archpsyc.60.2.184\\nKruschke, J. K., & Liddell, T. M. (2018). The bayesian new statistics: Hypothesis test-\\ning, estimation, meta-analysis, and power analysis from a bayesian perspective.\\nPsychonomic Bulletin and Review ,25(1). doi:10.3758/s13423-016-1221-4\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: summary of the Eight Steps of Data Analysis: A Graphical Framework to promote Sound Statistical Analysis, Dustin Fife\\nAnswer: '}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.1}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\npage_label: 1\\nfile_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf\\n\\nThe Eight Steps of Data Analysis: A Graphical Framework to\\nPromote Sound Statistical Analysis\\nDustin Fife1\\n1Rowan University\\nAbstract\\nData analysis is a risky endeavor, particularly among those unaware of its\\ndangers. In the words of Cook and Campbell (1976; see also Shadish, Cook,\\n& Campbell, 2002), “Statistical Conclusions Validity” threatens all research\\nsubjected to the dark arts of statistical magic. Although traditional statistics\\nclasses may advise against certain practices (e.g., multiple comparisons,\\nsmall sample sizes, violating normality), they may fail to cover others (e.g.,\\noutlier detection and violating linearity). More common, perhaps, is that\\nresearchers may fail to remember them. In this paper, rather than rehashing\\nold warnings and diatribes against this practice or that, I instead advocate a\\ngeneral statistical analysis strategy. This graphically-based eight step strategy\\npromises to resolve the majority of statistical traps researchers may fall in\\nwithout having to remember large lists of problematic statistical practices.\\nThese steps will assist in preventing both false positives and negatives and\\nyield critical insights about the data that would have otherwise been missed.\\nI conclude with an applied example that shows how the eight steps reveal\\ninteresting insights that would not be detected with standard statistical\\npractices.\\nKeywords: statistical assumptions, NHST, conﬁrmatory data analysis, graph-\\nical data analysis, ﬁshing, p-hacking\\nThe ﬁeld of psychology has been forced to participate in methodological introspection,\\nof sorts. This introspection began late in the 20th century as methodologists vehemently\\nprotested the knee-jerk focus on p-values Null Hypothesis Signiﬁcance Testing (NHST)\\nencourages (Cohen, 1994; Harlow, Mulaik, & Steiger, 2016; Jones, 1952). The American\\nI wish to thank those who assisted in reviewing this manuscript, including Tom Dinzeo, Polly Tremoulet,\\nJeﬀrey Greeson, Yoav Zeevi, Christine Simmons, and Joseph Rodgers, as well as the anonymous reviewers\\nand editor.\\nCorrespondence concerning this article should be addressed to Dustin Fife, 201 Mullica Hill Road\\nGlassboro, NJ 08028. E-mail: ﬁfe.dustin@gmail.com\\n\\npage_label: 33\\nfile_path: /home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf\\n\\nTHE EIGHT STEPS OF DATA ANALYSIS 33\\nCumming, G., Fidler, F., & Thomason, N. (2001). The statistical re-education of psychology.\\nInThe 6th international conference on teaching statistics. Hawthorn, Victorial:\\nSwinburne Press.\\nEdwards, J. R., & Berry, J. W. (2010). The presence of something or the absence of nothing:\\nIncreasing theoretical precision in management research. Organizational Research\\nMethods,13(4), 668–689. doi:10.1177/1094428110380467\\nErceg-Hurn, D. M., & Mirosevich, V. M. (2008). Modern robust statistical methods: An\\neasy way to maximize the accuracy and power of your research. The American\\nPsychologist ,63(7), 591–601. doi:10.1037/0003-066X.63.7.591\\nFife, D. A. (2019a). A graphic is worth a thousand test statistics: Mapping visuals onto\\ncommon analyses. Retrieved from http://rpubs.com/dustinﬁfe/528244\\nFife, D. A. (2019b). Flexplot: Graphical-based data analysis [r and jamovi]. Available at\\nwww.Jamovi.com; www.github.com/dustinﬁfe/ﬂexplot. doi:10.31234/osf.io/kh9c3\\nFife, D. A., & Rodgers, J. L. (2019). Exonerating eda: Addressing the replication crisis\\nby expanding the eda/cda continuum. Unpublished Manuscript . Retrieved from\\nhttp://quantpsych.net/ﬁfe-exonerating-eda-draft-oct2019-df-edits/\\nFurr, R. M. (2014). Scale construction and psychometrics for social and personality psychol-\\nogy. Thousand Oaks, CA: SAGE. doi:10.4135/9781446287866\\nHarlow, L. L., Mulaik, S. A., & Steiger, J. H. (2016). What if there were no signiﬁcance\\ntests?(2nd ed.). New York, NY: Routledge.\\nHealy, K., & Moody, J. (2014). Data visualization in sociology. Annual Review of Sociology ,\\n40(1), 105–128. doi:10.1146/ANNUREV-SOC-071312-145551\\nHoekstra, R., Kiers, H., & Johnson, A. (2012). Are assumptions of well-known sta-\\ntistical techniques checked, and why not? Frontiers in Psychology ,3(137).\\ndoi:10.3389/fpsyg.2012.00137\\nHofmann, S. G., Fang, A., & Brager, D. N. (2015). Eﬀect of intranasal oxytocin admin-\\nistration on psychiatric symptoms: A meta-analysis of placebo-controlled studies.\\nPsychiatry Research ,228(3), 708. doi:10.1016/j.psychres.2015.05.039\\nJASP Team. (2019). JASP (version 0.10.2)[Computer software]. Retrieved from https:\\n//jasp-stats.org/\\nJones, L. V. (1952). Test of hypotheses: One-sided vs. Two-sided alternatives. Psychological\\nBulletin,49(1), 43.\\nKessler, R. C., Barker, P. R., Colpe, L. J., Epstein, J. F., Gfroerer, J. C., Hiripi, E., ...\\nZaslavsky, A. M. (2003). Screening for serious mental illness in the general population.\\nArchives of General Psychiatry ,60(2), 184–189. doi:10.1001/archpsyc.60.2.184\\nKruschke, J. K., & Liddell, T. M. (2018). The bayesian new statistics: Hypothesis test-\\ning, estimation, meta-analysis, and power analysis from a bayesian perspective.\\nPsychonomic Bulletin and Review ,25(1). doi:10.3758/s13423-016-1221-4\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: summary of the Eight Steps of Data Analysis: A Graphical Framework to promote Sound Statistical Analysis, Dustin Fife\\nAnswer: '}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:close.started\n",
      "close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fc5c18a0>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fc5c18a0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f20fbd359c0> server_hostname='api.openai.com' timeout=60.0\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x7f20fbd359c0> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fb0d8b80>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f20fb0d8b80>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 26 Jul 2024 09:45:09 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-vgkhxp5wmyi2hwi8lvfpt8cu'), (b'openai-processing-ms', b'1441'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'198518'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'444ms'), (b'x-request-id', b'req_134637cffade42c83d59831f8e9c5441'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a937481ab0694f5-LIS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 26 Jul 2024 09:45:09 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-vgkhxp5wmyi2hwi8lvfpt8cu'), (b'openai-processing-ms', b'1441'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'198518'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'444ms'), (b'x-request-id', b'req_134637cffade42c83d59831f8e9c5441'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a937481ab0694f5-LIS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 26 Jul 2024 09:45:09 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-vgkhxp5wmyi2hwi8lvfpt8cu', 'openai-processing-ms': '1441', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '198518', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '444ms', 'x-request-id': 'req_134637cffade42c83d59831f8e9c5441', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8a937481ab0694f5-LIS', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 26 Jul 2024 09:45:09 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-vgkhxp5wmyi2hwi8lvfpt8cu', 'openai-processing-ms': '1441', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '198518', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '444ms', 'x-request-id': 'req_134637cffade42c83d59831f8e9c5441', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8a937481ab0694f5-LIS', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_134637cffade42c83d59831f8e9c5441\n",
      "request_id: req_134637cffade42c83d59831f8e9c5441\n",
      "The Eight Steps of Data Analysis: A Graphical Framework to promote Sound Statistical Analysis by Dustin Fife presents a strategy consisting of eight steps that aim to help researchers avoid common statistical pitfalls. This approach is designed to provide a graphical-based method to analyze data effectively, ensuring that critical insights are not overlooked. The framework promises to address various statistical traps that researchers may encounter, ultimately leading to a more robust and insightful analysis process.\n"
     ]
    }
   ],
   "source": [
    "question = \"summary of the Eight Steps of Data Analysis: A Graphical Framework to promote Sound Statistical Analysis, Dustin Fife\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cd5325ec-2808-4807-8b38-7f12fe9694ec': {'page_label': '1',\n",
       "  'file_name': 'Eight-Steps-of-data-analysis.pdf',\n",
       "  'file_path': '/home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 497272,\n",
       "  'creation_date': '2024-07-26',\n",
       "  'last_modified_date': '2024-07-26'},\n",
       " '73c217e5-f602-495f-9c5e-7796ecdf20d0': {'page_label': '33',\n",
       "  'file_name': 'Eight-Steps-of-data-analysis.pdf',\n",
       "  'file_path': '/home/cuphead/Projects/llama-index/data/Eight-Steps-of-data-analysis.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 497272,\n",
       "  'creation_date': '2024-07-26',\n",
       "  'last_modified_date': '2024-07-26'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f20fbd09630>, 'json_data': {'input': [\"What is Ephraim's quest in the story?\"], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f20fbd09630>, 'json_data': {'input': [\"What is Ephraim's quest in the story?\"], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 26 Jul 2024 09:35:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-vgkhxp5wmyi2hwi8lvfpt8cu'), (b'openai-processing-ms', b'44'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999990'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_be20b0c064c2505cf916ac6d880a8776'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a9366aa1f4b03ee-LIS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 26 Jul 2024 09:35:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-vgkhxp5wmyi2hwi8lvfpt8cu'), (b'openai-processing-ms', b'44'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999990'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_be20b0c064c2505cf916ac6d880a8776'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a9366aa1f4b03ee-LIS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Fri, 26 Jul 2024 09:35:41 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-embedding-ada-002', 'openai-organization': 'user-vgkhxp5wmyi2hwi8lvfpt8cu', 'openai-processing-ms': '44', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999990', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_be20b0c064c2505cf916ac6d880a8776', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8a9366aa1f4b03ee-LIS', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Fri, 26 Jul 2024 09:35:41 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-embedding-ada-002', 'openai-organization': 'user-vgkhxp5wmyi2hwi8lvfpt8cu', 'openai-processing-ms': '44', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999990', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_be20b0c064c2505cf916ac6d880a8776', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8a9366aa1f4b03ee-LIS', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_be20b0c064c2505cf916ac6d880a8776\n",
      "request_id: req_be20b0c064c2505cf916ac6d880a8776\n",
      "DEBUG:llama_index.core.indices.utils:> Top 2 nodes:\n",
      "> [Node 5a95d6a1-5ac9-4b1f-b252-d6612892cd30] [Similarity score:             0.840782] Once upon a time, in a town nestled between two mountains that whispered secrets to the wind, the...\n",
      "> [Node 16aee504-52a4-4f37-9708-45f15f157828] [Similarity score:             0.71481] Activities\n",
      "Land lines\n",
      "\n",
      "It consists of the provision of public phones throughout Morocco. The fixe...\n",
      "> Top 2 nodes:\n",
      "> [Node 5a95d6a1-5ac9-4b1f-b252-d6612892cd30] [Similarity score:             0.840782] Once upon a time, in a town nestled between two mountains that whispered secrets to the wind, the...\n",
      "> [Node 16aee504-52a4-4f37-9708-45f15f157828] [Similarity score:             0.71481] Activities\n",
      "Land lines\n",
      "\n",
      "It consists of the provision of public phones throughout Morocco. The fixe...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\nfile_path: /home/cuphead/Projects/llama-index/data/story.txt\\n\\nOnce upon a time, in a town nestled between two mountains that whispered secrets to the wind, there lived a peculiar man named Ephraim. Ephraim was known throughout the village for his uncanny ability to communicate with animals—not just the usual dogs and cats, but birds, insects, and even the occasional elusive fox.\\n\\nOne fateful morning, when the sun rose in hues of apricot and lavender, Ephraim set off on his daily trek into the forest. He was in search of an ancient oak rumored to hold the key to understanding the language of the forest creatures. Legends whispered that whoever could decipher their tongue would gain unimaginable wisdom and power.\\n\\nAs Ephraim ventured deeper into the woods, the trees seemed to murmur with anticipation. Squirrels chattered excitedly overhead, and deer watched him with curious eyes. It was as if the whole forest knew of his quest. \\n\\nHours turned into days as Ephraim navigated through thick underbrush and crossed babbling brooks. He survived on berries and nuts, sharing his meals with the woodland critters who guided him along hidden paths.\\n\\nFinally, on the seventh day of his journey, Ephraim stumbled upon the ancient oak. Its gnarled branches reached toward the heavens like a thousand crooked fingers, and its trunk was marked with symbols carved by creatures long forgotten. With trembling hands, Ephraim touched the rough bark, feeling the pulse of ancient magic beneath his fingertips.\\n\\nSuddenly, a voice whispered in his ear—not with words, but with images and emotions. It was the language of the forest, a symphony of nature\\'s secrets woven into the fabric of existence. Ephraim closed his eyes and listened, his heart pounding with wonder.\\n\\nFrom that day forward, Ephraim became the bridge between humanity and the natural world. He helped settle disputes between rival wolf packs, taught young owls the art of silent flight, and even brokered peace between the mischievous sprites of the glade and the stern guardians of the river.\\n\\nBut Ephraim\\'s newfound gift came with a price. The more he understood the language of the forest, the less he could communicate with his fellow villagers. They whispered that he had gone mad, talking to trees and laughing with the wind. Some feared him, while others revered him as a sage touched by the gods.\\n\\nAs the years passed, Ephraim grew old and frail, his body bent like the ancient oak that had revealed the secrets of the forest to him. On his last day, surrounded by friends both human and animal, Ephraim whispered his final words to the wind—a promise to protect the delicate balance between the realms he had come to love.\\n\\nAnd so, when Ephraim passed from this world, the animals of the forest gathered to mourn him. Birds sang melancholy melodies, and flowers bloomed in hues of mourning purple. But in the hearts of those who remembered him, Ephraim lived on as a legend—a strange and wondrous tale of a man who spoke the language of the wild.\\n\\nfile_path: /home/cuphead/Projects/llama-index/data/maroctelecom.txt\\n\\nActivities\\nLand lines\\n\\nIt consists of the provision of public phones throughout Morocco. The fixed park reaches 1.6 million lines.\\nMobile phodocuments in the data folder (which in this case just consists of the essay text, but could contain many documents).nes\\n\\nMobile services are provided via a GSM network. Maroc Telecom counted 33 million customers at the end of October 2012. Its network covers 97% of the Moroccan population. It also has 12.5 million customers in Mali, Gabon, Burkina Faso and Mauritania. It is one of the most profitable phone operators in Africa with a revenue of 2.2 billion euros during the first 9 months of 2012.[9]\\n\\nMaroc Telecom launched 4G+ in Morocco on July 13, 2015.[10]\\nCustomer support\\n\\nCustomer support does only propose contacting by phone to a special number and not via a contact form like most of telecommunication companies.[11]\\nProjects and investments\\n\\nOn June 1, 2006, IAM launched the IPTV package deployed by Huawei Technology[12] via the ADSL line. The service was the first of its kind in Africa and the Middle East.\\n\\nIn July 2006, Maroc Telecom signed with the French telecommunications equipment company Alcatel a contract for a submarine communications cable connection between Morocco and France. Maroc Telecom\\'s aim is to upgrade the capacity of its services (i.e. broadband services, call centers). The project cost €26 million and was named \"Atlas Offshore\".[13]\\n\\nIn December 2006, IAM invested in Burkina Faso’s ONATEL, acquiring 51% of its capital.[14]\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: What is Ephraim\\'s quest in the story?\\nAnswer: '}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.1}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\nfile_path: /home/cuphead/Projects/llama-index/data/story.txt\\n\\nOnce upon a time, in a town nestled between two mountains that whispered secrets to the wind, there lived a peculiar man named Ephraim. Ephraim was known throughout the village for his uncanny ability to communicate with animals—not just the usual dogs and cats, but birds, insects, and even the occasional elusive fox.\\n\\nOne fateful morning, when the sun rose in hues of apricot and lavender, Ephraim set off on his daily trek into the forest. He was in search of an ancient oak rumored to hold the key to understanding the language of the forest creatures. Legends whispered that whoever could decipher their tongue would gain unimaginable wisdom and power.\\n\\nAs Ephraim ventured deeper into the woods, the trees seemed to murmur with anticipation. Squirrels chattered excitedly overhead, and deer watched him with curious eyes. It was as if the whole forest knew of his quest. \\n\\nHours turned into days as Ephraim navigated through thick underbrush and crossed babbling brooks. He survived on berries and nuts, sharing his meals with the woodland critters who guided him along hidden paths.\\n\\nFinally, on the seventh day of his journey, Ephraim stumbled upon the ancient oak. Its gnarled branches reached toward the heavens like a thousand crooked fingers, and its trunk was marked with symbols carved by creatures long forgotten. With trembling hands, Ephraim touched the rough bark, feeling the pulse of ancient magic beneath his fingertips.\\n\\nSuddenly, a voice whispered in his ear—not with words, but with images and emotions. It was the language of the forest, a symphony of nature\\'s secrets woven into the fabric of existence. Ephraim closed his eyes and listened, his heart pounding with wonder.\\n\\nFrom that day forward, Ephraim became the bridge between humanity and the natural world. He helped settle disputes between rival wolf packs, taught young owls the art of silent flight, and even brokered peace between the mischievous sprites of the glade and the stern guardians of the river.\\n\\nBut Ephraim\\'s newfound gift came with a price. The more he understood the language of the forest, the less he could communicate with his fellow villagers. They whispered that he had gone mad, talking to trees and laughing with the wind. Some feared him, while others revered him as a sage touched by the gods.\\n\\nAs the years passed, Ephraim grew old and frail, his body bent like the ancient oak that had revealed the secrets of the forest to him. On his last day, surrounded by friends both human and animal, Ephraim whispered his final words to the wind—a promise to protect the delicate balance between the realms he had come to love.\\n\\nAnd so, when Ephraim passed from this world, the animals of the forest gathered to mourn him. Birds sang melancholy melodies, and flowers bloomed in hues of mourning purple. But in the hearts of those who remembered him, Ephraim lived on as a legend—a strange and wondrous tale of a man who spoke the language of the wild.\\n\\nfile_path: /home/cuphead/Projects/llama-index/data/maroctelecom.txt\\n\\nActivities\\nLand lines\\n\\nIt consists of the provision of public phones throughout Morocco. The fixed park reaches 1.6 million lines.\\nMobile phodocuments in the data folder (which in this case just consists of the essay text, but could contain many documents).nes\\n\\nMobile services are provided via a GSM network. Maroc Telecom counted 33 million customers at the end of October 2012. Its network covers 97% of the Moroccan population. It also has 12.5 million customers in Mali, Gabon, Burkina Faso and Mauritania. It is one of the most profitable phone operators in Africa with a revenue of 2.2 billion euros during the first 9 months of 2012.[9]\\n\\nMaroc Telecom launched 4G+ in Morocco on July 13, 2015.[10]\\nCustomer support\\n\\nCustomer support does only propose contacting by phone to a special number and not via a contact form like most of telecommunication companies.[11]\\nProjects and investments\\n\\nOn June 1, 2006, IAM launched the IPTV package deployed by Huawei Technology[12] via the ADSL line. The service was the first of its kind in Africa and the Middle East.\\n\\nIn July 2006, Maroc Telecom signed with the French telecommunications equipment company Alcatel a contract for a submarine communications cable connection between Morocco and France. Maroc Telecom\\'s aim is to upgrade the capacity of its services (i.e. broadband services, call centers). The project cost €26 million and was named \"Atlas Offshore\".[13]\\n\\nIn December 2006, IAM invested in Burkina Faso’s ONATEL, acquiring 51% of its capital.[14]\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: What is Ephraim\\'s quest in the story?\\nAnswer: '}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 26 Jul 2024 09:35:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-vgkhxp5wmyi2hwi8lvfpt8cu'), (b'openai-processing-ms', b'726'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'198691'), (b'x-ratelimit-reset-requests', b'15.342s'), (b'x-ratelimit-reset-tokens', b'392ms'), (b'x-request-id', b'req_fc7be44ec6003247bc58cc748ad15267'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a9366aceabf41f6-MRS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 26 Jul 2024 09:35:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-vgkhxp5wmyi2hwi8lvfpt8cu'), (b'openai-processing-ms', b'726'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'198691'), (b'x-ratelimit-reset-requests', b'15.342s'), (b'x-ratelimit-reset-tokens', b'392ms'), (b'x-request-id', b'req_fc7be44ec6003247bc58cc748ad15267'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a9366aceabf41f6-MRS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 26 Jul 2024 09:35:42 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-vgkhxp5wmyi2hwi8lvfpt8cu', 'openai-processing-ms': '726', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '198691', 'x-ratelimit-reset-requests': '15.342s', 'x-ratelimit-reset-tokens': '392ms', 'x-request-id': 'req_fc7be44ec6003247bc58cc748ad15267', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8a9366aceabf41f6-MRS', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 26 Jul 2024 09:35:42 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-vgkhxp5wmyi2hwi8lvfpt8cu', 'openai-processing-ms': '726', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '198691', 'x-ratelimit-reset-requests': '15.342s', 'x-ratelimit-reset-tokens': '392ms', 'x-request-id': 'req_fc7be44ec6003247bc58cc748ad15267', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8a9366aceabf41f6-MRS', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_fc7be44ec6003247bc58cc748ad15267\n",
      "request_id: req_fc7be44ec6003247bc58cc748ad15267\n",
      "Ephraim's quest in the story is to find an ancient oak tree that is rumored to hold the key to understanding the language of the forest creatures.\n"
     ]
    }
   ],
   "source": [
    "story_question = \"What is Ephraim's quest in the story?\"\n",
    "response = query_engine.query(story_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.base.response.schema.Response"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5a95d6a1-5ac9-4b1f-b252-d6612892cd30': {'file_path': '/home/cuphead/Projects/llama-index/data/story.txt',\n",
       "  'file_name': 'story.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 2963,\n",
       "  'creation_date': '2024-07-26',\n",
       "  'last_modified_date': '2024-07-26'},\n",
       " '16aee504-52a4-4f37-9708-45f15f157828': {'file_path': '/home/cuphead/Projects/llama-index/data/maroctelecom.txt',\n",
       "  'file_name': 'maroctelecom.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 4684,\n",
       "  'creation_date': '2024-07-22',\n",
       "  'last_modified_date': '2024-07-22'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save embeddngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:fsspec.local:open file: /home/cuphead/Projects/llama-index/storage/docstore.json\n",
      "open file: /home/cuphead/Projects/llama-index/storage/docstore.json\n",
      "DEBUG:fsspec.local:open file: /home/cuphead/Projects/llama-index/storage/index_store.json\n",
      "open file: /home/cuphead/Projects/llama-index/storage/index_store.json\n",
      "DEBUG:fsspec.local:open file: /home/cuphead/Projects/llama-index/storage/graph_store.json\n",
      "open file: /home/cuphead/Projects/llama-index/storage/graph_store.json\n",
      "DEBUG:fsspec.local:open file: /home/cuphead/Projects/llama-index/storage/default__vector_store.json\n",
      "open file: /home/cuphead/Projects/llama-index/storage/default__vector_store.json\n",
      "DEBUG:fsspec.local:open file: /home/cuphead/Projects/llama-index/storage/image__vector_store.json\n",
      "open file: /home/cuphead/Projects/llama-index/storage/image__vector_store.json\n"
     ]
    }
   ],
   "source": [
    "index.storage_context.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
